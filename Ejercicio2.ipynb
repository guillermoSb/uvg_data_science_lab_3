{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diesel_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-76576.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86033.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-137814.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>114863.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-54753.87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   diesel_diff\n",
       "0    -76576.19\n",
       "1     86033.88\n",
       "2   -137814.26\n",
       "3    114863.71\n",
       "4    -54753.87"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_diesel = pd.read_csv('data/data_diesel_diff.csv', sep=',')\n",
    "data_diesel = data_diesel.drop(['anio', 'mes'], axis=1)\n",
    "data_diesel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjuntos de entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data_diesel_scaled = scaler.fit_transform(data_diesel) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    }
   ],
   "source": [
    "# calculando los indices de particionamiento\n",
    "entrenamiento = round(0.6 * len(data_diesel_scaled))\n",
    "val_prueba = round(0.2 * len(data_diesel_scaled))\n",
    "\n",
    "# Particionando los datos\n",
    "train = data_diesel_scaled[:entrenamiento]\n",
    "validation = data_diesel_scaled[entrenamiento:entrenamiento+val_prueba]\n",
    "test = data_diesel_scaled[entrenamiento+val_prueba:]\n",
    "\n",
    "train = np.insert(train, 0, 0)\n",
    "train = train.reshape(-1, 1)\n",
    "\n",
    "print(len(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervisada(serie,retrasos = 1):\n",
    "    serie_x = []\n",
    "    serie_y = []\n",
    "    for i in range(len(serie)-retrasos):\n",
    "        valor = serie[i:(i+retrasos),0]\n",
    "        valor_sig = serie[i+retrasos,0]\n",
    "        serie_x.append(valor)\n",
    "        serie_y.append(valor_sig)\n",
    "    return np.array(serie_x), np.array(serie_y)\n",
    "\n",
    "x_train,y_train = supervisada(train)\n",
    "x_val,y_val = supervisada(validation)\n",
    "x_test,y_test = supervisada(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.reshape(x_train,(x_train.shape[0],1,1))\n",
    "x_val = np.reshape(x_val, (x_val.shape[0],1,1))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0],1,1))\n",
    "len(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_28 (LSTM)              (1, 1)                    12        \n",
      "                                                                 \n",
      " dense_9 (Dense)             (1, 1)                    2         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14 (56.00 Byte)\n",
      "Trainable params: 14 (56.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelo1 = Sequential()\n",
    "lote = 1\n",
    "paso = 1\n",
    "caracteristicas = 1\n",
    "modelo1.add(LSTM(lote, batch_input_shape=(lote, paso, caracteristicas), stateful=True))\n",
    "modelo1.add(Dense(1))\n",
    "modelo1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo1.compile(loss='mean_squared_error',optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "161/161 [==============================] - 1s 2ms/step - loss: 0.9592 - val_loss: 1.1440\n",
      "Epoch 2/1000\n",
      "161/161 [==============================] - 0s 845us/step - loss: 0.9495 - val_loss: 1.1332\n",
      "Epoch 3/1000\n",
      "161/161 [==============================] - 0s 786us/step - loss: 0.9392 - val_loss: 1.1222\n",
      "Epoch 4/1000\n",
      "161/161 [==============================] - 0s 775us/step - loss: 0.9287 - val_loss: 1.1108\n",
      "Epoch 5/1000\n",
      "161/161 [==============================] - 0s 782us/step - loss: 0.9178 - val_loss: 1.0991\n",
      "Epoch 6/1000\n",
      "161/161 [==============================] - 0s 780us/step - loss: 0.9067 - val_loss: 1.0871\n",
      "Epoch 7/1000\n",
      "161/161 [==============================] - 0s 803us/step - loss: 0.8953 - val_loss: 1.0748\n",
      "Epoch 8/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.8837 - val_loss: 1.0624\n",
      "Epoch 9/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.8719 - val_loss: 1.0498\n",
      "Epoch 10/1000\n",
      "161/161 [==============================] - 0s 850us/step - loss: 0.8600 - val_loss: 1.0371\n",
      "Epoch 11/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.8479 - val_loss: 1.0244\n",
      "Epoch 12/1000\n",
      "161/161 [==============================] - 0s 787us/step - loss: 0.8359 - val_loss: 1.0117\n",
      "Epoch 13/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.8238 - val_loss: 0.9990\n",
      "Epoch 14/1000\n",
      "161/161 [==============================] - 0s 776us/step - loss: 0.8119 - val_loss: 0.9865\n",
      "Epoch 15/1000\n",
      "161/161 [==============================] - 0s 780us/step - loss: 0.8001 - val_loss: 0.9741\n",
      "Epoch 16/1000\n",
      "161/161 [==============================] - 0s 778us/step - loss: 0.7884 - val_loss: 0.9620\n",
      "Epoch 17/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.7771 - val_loss: 0.9502\n",
      "Epoch 18/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.7661 - val_loss: 0.9387\n",
      "Epoch 19/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.7554 - val_loss: 0.9277\n",
      "Epoch 20/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.7453 - val_loss: 0.9172\n",
      "Epoch 21/1000\n",
      "161/161 [==============================] - 0s 843us/step - loss: 0.7356 - val_loss: 0.9073\n",
      "Epoch 22/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.7265 - val_loss: 0.8979\n",
      "Epoch 23/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.7179 - val_loss: 0.8891\n",
      "Epoch 24/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.7100 - val_loss: 0.8809\n",
      "Epoch 25/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.7026 - val_loss: 0.8733\n",
      "Epoch 26/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6958 - val_loss: 0.8664\n",
      "Epoch 27/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6897 - val_loss: 0.8601\n",
      "Epoch 28/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.6841 - val_loss: 0.8543\n",
      "Epoch 29/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6790 - val_loss: 0.8492\n",
      "Epoch 30/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6745 - val_loss: 0.8446\n",
      "Epoch 31/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6705 - val_loss: 0.8404\n",
      "Epoch 32/1000\n",
      "161/161 [==============================] - 0s 843us/step - loss: 0.6669 - val_loss: 0.8367\n",
      "Epoch 33/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.6638 - val_loss: 0.8335\n",
      "Epoch 34/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6610 - val_loss: 0.8306\n",
      "Epoch 35/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.6585 - val_loss: 0.8281\n",
      "Epoch 36/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.6564 - val_loss: 0.8258\n",
      "Epoch 37/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.6545 - val_loss: 0.8239\n",
      "Epoch 38/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6529 - val_loss: 0.8221\n",
      "Epoch 39/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.6514 - val_loss: 0.8206\n",
      "Epoch 40/1000\n",
      "161/161 [==============================] - 0s 812us/step - loss: 0.6502 - val_loss: 0.8192\n",
      "Epoch 41/1000\n",
      "161/161 [==============================] - 0s 780us/step - loss: 0.6490 - val_loss: 0.8180\n",
      "Epoch 42/1000\n",
      "161/161 [==============================] - 0s 775us/step - loss: 0.6481 - val_loss: 0.8169\n",
      "Epoch 43/1000\n",
      "161/161 [==============================] - 0s 847us/step - loss: 0.6472 - val_loss: 0.8159\n",
      "Epoch 44/1000\n",
      "161/161 [==============================] - 0s 777us/step - loss: 0.6464 - val_loss: 0.8150\n",
      "Epoch 45/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6457 - val_loss: 0.8142\n",
      "Epoch 46/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.6450 - val_loss: 0.8134\n",
      "Epoch 47/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.6445 - val_loss: 0.8127\n",
      "Epoch 48/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6439 - val_loss: 0.8120\n",
      "Epoch 49/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.6434 - val_loss: 0.8113\n",
      "Epoch 50/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.6429 - val_loss: 0.8107\n",
      "Epoch 51/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.6425 - val_loss: 0.8101\n",
      "Epoch 52/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6420 - val_loss: 0.8095\n",
      "Epoch 53/1000\n",
      "161/161 [==============================] - 0s 851us/step - loss: 0.6416 - val_loss: 0.8090\n",
      "Epoch 54/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.6412 - val_loss: 0.8084\n",
      "Epoch 55/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6408 - val_loss: 0.8079\n",
      "Epoch 56/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6404 - val_loss: 0.8073\n",
      "Epoch 57/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.6400 - val_loss: 0.8068\n",
      "Epoch 58/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.6396 - val_loss: 0.8063\n",
      "Epoch 59/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6392 - val_loss: 0.8058\n",
      "Epoch 60/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.6389 - val_loss: 0.8053\n",
      "Epoch 61/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.6385 - val_loss: 0.8048\n",
      "Epoch 62/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6381 - val_loss: 0.8043\n",
      "Epoch 63/1000\n",
      "161/161 [==============================] - 0s 851us/step - loss: 0.6377 - val_loss: 0.8038\n",
      "Epoch 64/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6374 - val_loss: 0.8033\n",
      "Epoch 65/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.6370 - val_loss: 0.8028\n",
      "Epoch 66/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.6366 - val_loss: 0.8023\n",
      "Epoch 67/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6362 - val_loss: 0.8018\n",
      "Epoch 68/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.6359 - val_loss: 0.8013\n",
      "Epoch 69/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6355 - val_loss: 0.8008\n",
      "Epoch 70/1000\n",
      "161/161 [==============================] - 0s 775us/step - loss: 0.6351 - val_loss: 0.8003\n",
      "Epoch 71/1000\n",
      "161/161 [==============================] - 0s 780us/step - loss: 0.6347 - val_loss: 0.7999\n",
      "Epoch 72/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.6344 - val_loss: 0.7994\n",
      "Epoch 73/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6340 - val_loss: 0.7989\n",
      "Epoch 74/1000\n",
      "161/161 [==============================] - 0s 830us/step - loss: 0.6336 - val_loss: 0.7985\n",
      "Epoch 75/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.6333 - val_loss: 0.7980\n",
      "Epoch 76/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.6329 - val_loss: 0.7975\n",
      "Epoch 77/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.6325 - val_loss: 0.7971\n",
      "Epoch 78/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.6321 - val_loss: 0.7966\n",
      "Epoch 79/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.6318 - val_loss: 0.7962\n",
      "Epoch 80/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6314 - val_loss: 0.7957\n",
      "Epoch 81/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.6310 - val_loss: 0.7953\n",
      "Epoch 82/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6307 - val_loss: 0.7948\n",
      "Epoch 83/1000\n",
      "161/161 [==============================] - 0s 775us/step - loss: 0.6303 - val_loss: 0.7944\n",
      "Epoch 84/1000\n",
      "161/161 [==============================] - 0s 840us/step - loss: 0.6299 - val_loss: 0.7940\n",
      "Epoch 85/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.6296 - val_loss: 0.7936\n",
      "Epoch 86/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.6292 - val_loss: 0.7931\n",
      "Epoch 87/1000\n",
      "161/161 [==============================] - 0s 796us/step - loss: 0.6288 - val_loss: 0.7927\n",
      "Epoch 88/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6285 - val_loss: 0.7923\n",
      "Epoch 89/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.6281 - val_loss: 0.7919\n",
      "Epoch 90/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.6277 - val_loss: 0.7915\n",
      "Epoch 91/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6274 - val_loss: 0.7911\n",
      "Epoch 92/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.6270 - val_loss: 0.7907\n",
      "Epoch 93/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.6267 - val_loss: 0.7903\n",
      "Epoch 94/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.6263 - val_loss: 0.7899\n",
      "Epoch 95/1000\n",
      "161/161 [==============================] - 0s 835us/step - loss: 0.6259 - val_loss: 0.7895\n",
      "Epoch 96/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6256 - val_loss: 0.7891\n",
      "Epoch 97/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.6252 - val_loss: 0.7887\n",
      "Epoch 98/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6249 - val_loss: 0.7883\n",
      "Epoch 99/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.6245 - val_loss: 0.7879\n",
      "Epoch 100/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.6242 - val_loss: 0.7876\n",
      "Epoch 101/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.6239 - val_loss: 0.7872\n",
      "Epoch 102/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.6235 - val_loss: 0.7868\n",
      "Epoch 103/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.6232 - val_loss: 0.7865\n",
      "Epoch 104/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.6228 - val_loss: 0.7861\n",
      "Epoch 105/1000\n",
      "161/161 [==============================] - 0s 832us/step - loss: 0.6225 - val_loss: 0.7858\n",
      "Epoch 106/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.6221 - val_loss: 0.7854\n",
      "Epoch 107/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.6218 - val_loss: 0.7851\n",
      "Epoch 108/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.6215 - val_loss: 0.7847\n",
      "Epoch 109/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.6211 - val_loss: 0.7844\n",
      "Epoch 110/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.6208 - val_loss: 0.7840\n",
      "Epoch 111/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.6205 - val_loss: 0.7837\n",
      "Epoch 112/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.6202 - val_loss: 0.7834\n",
      "Epoch 113/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.6198 - val_loss: 0.7830\n",
      "Epoch 114/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.6195 - val_loss: 0.7827\n",
      "Epoch 115/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.6192 - val_loss: 0.7824\n",
      "Epoch 116/1000\n",
      "161/161 [==============================] - 0s 846us/step - loss: 0.6189 - val_loss: 0.7821\n",
      "Epoch 117/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6185 - val_loss: 0.7817\n",
      "Epoch 118/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6182 - val_loss: 0.7814\n",
      "Epoch 119/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.6179 - val_loss: 0.7811\n",
      "Epoch 120/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6176 - val_loss: 0.7808\n",
      "Epoch 121/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.6173 - val_loss: 0.7805\n",
      "Epoch 122/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6170 - val_loss: 0.7802\n",
      "Epoch 123/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.6167 - val_loss: 0.7799\n",
      "Epoch 124/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.6164 - val_loss: 0.7796\n",
      "Epoch 125/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.6160 - val_loss: 0.7793\n",
      "Epoch 126/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6157 - val_loss: 0.7790\n",
      "Epoch 127/1000\n",
      "161/161 [==============================] - 0s 830us/step - loss: 0.6154 - val_loss: 0.7787\n",
      "Epoch 128/1000\n",
      "161/161 [==============================] - 0s 795us/step - loss: 0.6151 - val_loss: 0.7784\n",
      "Epoch 129/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.6148 - val_loss: 0.7781\n",
      "Epoch 130/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6145 - val_loss: 0.7779\n",
      "Epoch 131/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.6143 - val_loss: 0.7776\n",
      "Epoch 132/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.6140 - val_loss: 0.7773\n",
      "Epoch 133/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.6137 - val_loss: 0.7770\n",
      "Epoch 134/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6134 - val_loss: 0.7767\n",
      "Epoch 135/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.6131 - val_loss: 0.7765\n",
      "Epoch 136/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.6128 - val_loss: 0.7762\n",
      "Epoch 137/1000\n",
      "161/161 [==============================] - 0s 837us/step - loss: 0.6125 - val_loss: 0.7759\n",
      "Epoch 138/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.6122 - val_loss: 0.7757\n",
      "Epoch 139/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6120 - val_loss: 0.7754\n",
      "Epoch 140/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.6117 - val_loss: 0.7752\n",
      "Epoch 141/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.6114 - val_loss: 0.7749\n",
      "Epoch 142/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.6111 - val_loss: 0.7747\n",
      "Epoch 143/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.6109 - val_loss: 0.7744\n",
      "Epoch 144/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6106 - val_loss: 0.7742\n",
      "Epoch 145/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6103 - val_loss: 0.7739\n",
      "Epoch 146/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.6100 - val_loss: 0.7737\n",
      "Epoch 147/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.6098 - val_loss: 0.7734\n",
      "Epoch 148/1000\n",
      "161/161 [==============================] - 0s 833us/step - loss: 0.6095 - val_loss: 0.7732\n",
      "Epoch 149/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.6093 - val_loss: 0.7729\n",
      "Epoch 150/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.6090 - val_loss: 0.7727\n",
      "Epoch 151/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.6087 - val_loss: 0.7725\n",
      "Epoch 152/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.6085 - val_loss: 0.7722\n",
      "Epoch 153/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.6082 - val_loss: 0.7720\n",
      "Epoch 154/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.6080 - val_loss: 0.7718\n",
      "Epoch 155/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.6077 - val_loss: 0.7715\n",
      "Epoch 156/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6075 - val_loss: 0.7713\n",
      "Epoch 157/1000\n",
      "161/161 [==============================] - 0s 828us/step - loss: 0.6072 - val_loss: 0.7711\n",
      "Epoch 158/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.6070 - val_loss: 0.7709\n",
      "Epoch 159/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.6067 - val_loss: 0.7707\n",
      "Epoch 160/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.6065 - val_loss: 0.7704\n",
      "Epoch 161/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6062 - val_loss: 0.7702\n",
      "Epoch 162/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.6060 - val_loss: 0.7700\n",
      "Epoch 163/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6058 - val_loss: 0.7698\n",
      "Epoch 164/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.6055 - val_loss: 0.7696\n",
      "Epoch 165/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6053 - val_loss: 0.7694\n",
      "Epoch 166/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.6051 - val_loss: 0.7692\n",
      "Epoch 167/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.6048 - val_loss: 0.7690\n",
      "Epoch 168/1000\n",
      "161/161 [==============================] - 0s 841us/step - loss: 0.6046 - val_loss: 0.7688\n",
      "Epoch 169/1000\n",
      "161/161 [==============================] - 0s 794us/step - loss: 0.6044 - val_loss: 0.7686\n",
      "Epoch 170/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6041 - val_loss: 0.7684\n",
      "Epoch 171/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.6039 - val_loss: 0.7682\n",
      "Epoch 172/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.6037 - val_loss: 0.7680\n",
      "Epoch 173/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.6035 - val_loss: 0.7678\n",
      "Epoch 174/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6032 - val_loss: 0.7676\n",
      "Epoch 175/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6030 - val_loss: 0.7674\n",
      "Epoch 176/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.6028 - val_loss: 0.7672\n",
      "Epoch 177/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6026 - val_loss: 0.7670\n",
      "Epoch 178/1000\n",
      "161/161 [==============================] - 0s 828us/step - loss: 0.6024 - val_loss: 0.7668\n",
      "Epoch 179/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.6021 - val_loss: 0.7666\n",
      "Epoch 180/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6019 - val_loss: 0.7664\n",
      "Epoch 181/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6017 - val_loss: 0.7663\n",
      "Epoch 182/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.6015 - val_loss: 0.7661\n",
      "Epoch 183/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6013 - val_loss: 0.7659\n",
      "Epoch 184/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.6011 - val_loss: 0.7657\n",
      "Epoch 185/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6009 - val_loss: 0.7655\n",
      "Epoch 186/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6007 - val_loss: 0.7654\n",
      "Epoch 187/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.6005 - val_loss: 0.7652\n",
      "Epoch 188/1000\n",
      "161/161 [==============================] - 0s 839us/step - loss: 0.6003 - val_loss: 0.7650\n",
      "Epoch 189/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.6001 - val_loss: 0.7648\n",
      "Epoch 190/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.5999 - val_loss: 0.7647\n",
      "Epoch 191/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5997 - val_loss: 0.7645\n",
      "Epoch 192/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5995 - val_loss: 0.7643\n",
      "Epoch 193/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5993 - val_loss: 0.7642\n",
      "Epoch 194/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5991 - val_loss: 0.7640\n",
      "Epoch 195/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.5989 - val_loss: 0.7638\n",
      "Epoch 196/1000\n",
      "161/161 [==============================] - 0s 822us/step - loss: 0.5987 - val_loss: 0.7637\n",
      "Epoch 197/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5985 - val_loss: 0.7635\n",
      "Epoch 198/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5983 - val_loss: 0.7633\n",
      "Epoch 199/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5982 - val_loss: 0.7632\n",
      "Epoch 200/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5980 - val_loss: 0.7630\n",
      "Epoch 201/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.5978 - val_loss: 0.7629\n",
      "Epoch 202/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5976 - val_loss: 0.7627\n",
      "Epoch 203/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.5974 - val_loss: 0.7626\n",
      "Epoch 204/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5972 - val_loss: 0.7624\n",
      "Epoch 205/1000\n",
      "161/161 [==============================] - 0s 793us/step - loss: 0.5971 - val_loss: 0.7622\n",
      "Epoch 206/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5969 - val_loss: 0.7621\n",
      "Epoch 207/1000\n",
      "161/161 [==============================] - 0s 826us/step - loss: 0.5967 - val_loss: 0.7619\n",
      "Epoch 208/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5965 - val_loss: 0.7618\n",
      "Epoch 209/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5964 - val_loss: 0.7616\n",
      "Epoch 210/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5962 - val_loss: 0.7615\n",
      "Epoch 211/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.5960 - val_loss: 0.7613\n",
      "Epoch 212/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5958 - val_loss: 0.7612\n",
      "Epoch 213/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5957 - val_loss: 0.7611\n",
      "Epoch 214/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5955 - val_loss: 0.7609\n",
      "Epoch 215/1000\n",
      "161/161 [==============================] - 0s 839us/step - loss: 0.5953 - val_loss: 0.7608\n",
      "Epoch 216/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5952 - val_loss: 0.7606\n",
      "Epoch 217/1000\n",
      "161/161 [==============================] - 0s 752us/step - loss: 0.5950 - val_loss: 0.7605\n",
      "Epoch 218/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5948 - val_loss: 0.7603\n",
      "Epoch 219/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.5947 - val_loss: 0.7602\n",
      "Epoch 220/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5945 - val_loss: 0.7601\n",
      "Epoch 221/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.5943 - val_loss: 0.7599\n",
      "Epoch 222/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.5942 - val_loss: 0.7598\n",
      "Epoch 223/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5940 - val_loss: 0.7597\n",
      "Epoch 224/1000\n",
      "161/161 [==============================] - 0s 832us/step - loss: 0.5939 - val_loss: 0.7595\n",
      "Epoch 225/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5937 - val_loss: 0.7594\n",
      "Epoch 226/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5935 - val_loss: 0.7593\n",
      "Epoch 227/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5934 - val_loss: 0.7591\n",
      "Epoch 228/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5932 - val_loss: 0.7590\n",
      "Epoch 229/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5931 - val_loss: 0.7589\n",
      "Epoch 230/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.5929 - val_loss: 0.7587\n",
      "Epoch 231/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5928 - val_loss: 0.7586\n",
      "Epoch 232/1000\n",
      "161/161 [==============================] - 0s 831us/step - loss: 0.5926 - val_loss: 0.7585\n",
      "Epoch 233/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5925 - val_loss: 0.7583\n",
      "Epoch 234/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.5923 - val_loss: 0.7582\n",
      "Epoch 235/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5922 - val_loss: 0.7581\n",
      "Epoch 236/1000\n",
      "161/161 [==============================] - 0s 789us/step - loss: 0.5920 - val_loss: 0.7580\n",
      "Epoch 237/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5919 - val_loss: 0.7578\n",
      "Epoch 238/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5917 - val_loss: 0.7577\n",
      "Epoch 239/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5916 - val_loss: 0.7576\n",
      "Epoch 240/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5915 - val_loss: 0.7575\n",
      "Epoch 241/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.5913 - val_loss: 0.7574\n",
      "Epoch 242/1000\n",
      "161/161 [==============================] - 0s 891us/step - loss: 0.5912 - val_loss: 0.7572\n",
      "Epoch 243/1000\n",
      "161/161 [==============================] - 0s 783us/step - loss: 0.5910 - val_loss: 0.7571\n",
      "Epoch 244/1000\n",
      "161/161 [==============================] - 0s 780us/step - loss: 0.5909 - val_loss: 0.7570\n",
      "Epoch 245/1000\n",
      "161/161 [==============================] - 0s 785us/step - loss: 0.5908 - val_loss: 0.7569\n",
      "Epoch 246/1000\n",
      "161/161 [==============================] - 0s 796us/step - loss: 0.5906 - val_loss: 0.7568\n",
      "Epoch 247/1000\n",
      "161/161 [==============================] - 0s 782us/step - loss: 0.5905 - val_loss: 0.7566\n",
      "Epoch 248/1000\n",
      "161/161 [==============================] - 0s 789us/step - loss: 0.5903 - val_loss: 0.7565\n",
      "Epoch 249/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.5902 - val_loss: 0.7564\n",
      "Epoch 250/1000\n",
      "161/161 [==============================] - 0s 844us/step - loss: 0.5901 - val_loss: 0.7563\n",
      "Epoch 251/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5899 - val_loss: 0.7562\n",
      "Epoch 252/1000\n",
      "161/161 [==============================] - 0s 856us/step - loss: 0.5898 - val_loss: 0.7561\n",
      "Epoch 253/1000\n",
      "161/161 [==============================] - 0s 777us/step - loss: 0.5897 - val_loss: 0.7560\n",
      "Epoch 254/1000\n",
      "161/161 [==============================] - 0s 782us/step - loss: 0.5895 - val_loss: 0.7558\n",
      "Epoch 255/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5894 - val_loss: 0.7557\n",
      "Epoch 256/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5893 - val_loss: 0.7556\n",
      "Epoch 257/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.5892 - val_loss: 0.7555\n",
      "Epoch 258/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.5890 - val_loss: 0.7554\n",
      "Epoch 259/1000\n",
      "161/161 [==============================] - 0s 850us/step - loss: 0.5889 - val_loss: 0.7553\n",
      "Epoch 260/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5888 - val_loss: 0.7552\n",
      "Epoch 261/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.5886 - val_loss: 0.7551\n",
      "Epoch 262/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.5885 - val_loss: 0.7550\n",
      "Epoch 263/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5884 - val_loss: 0.7549\n",
      "Epoch 264/1000\n",
      "161/161 [==============================] - 0s 805us/step - loss: 0.5883 - val_loss: 0.7548\n",
      "Epoch 265/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5881 - val_loss: 0.7546\n",
      "Epoch 266/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5880 - val_loss: 0.7545\n",
      "Epoch 267/1000\n",
      "161/161 [==============================] - 0s 836us/step - loss: 0.5879 - val_loss: 0.7544\n",
      "Epoch 268/1000\n",
      "161/161 [==============================] - 0s 777us/step - loss: 0.5878 - val_loss: 0.7543\n",
      "Epoch 269/1000\n",
      "161/161 [==============================] - 0s 775us/step - loss: 0.5877 - val_loss: 0.7542\n",
      "Epoch 270/1000\n",
      "161/161 [==============================] - 0s 811us/step - loss: 0.5875 - val_loss: 0.7541\n",
      "Epoch 271/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.5874 - val_loss: 0.7540\n",
      "Epoch 272/1000\n",
      "161/161 [==============================] - 0s 786us/step - loss: 0.5873 - val_loss: 0.7539\n",
      "Epoch 273/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5872 - val_loss: 0.7538\n",
      "Epoch 274/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.5871 - val_loss: 0.7537\n",
      "Epoch 275/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5870 - val_loss: 0.7536\n",
      "Epoch 276/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5868 - val_loss: 0.7535\n",
      "Epoch 277/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5867 - val_loss: 0.7534\n",
      "Epoch 278/1000\n",
      "161/161 [==============================] - 0s 849us/step - loss: 0.5866 - val_loss: 0.7533\n",
      "Epoch 279/1000\n",
      "161/161 [==============================] - 0s 808us/step - loss: 0.5865 - val_loss: 0.7532\n",
      "Epoch 280/1000\n",
      "161/161 [==============================] - 0s 783us/step - loss: 0.5864 - val_loss: 0.7531\n",
      "Epoch 281/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5863 - val_loss: 0.7530\n",
      "Epoch 282/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5862 - val_loss: 0.7529\n",
      "Epoch 283/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.5861 - val_loss: 0.7529\n",
      "Epoch 284/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5860 - val_loss: 0.7528\n",
      "Epoch 285/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.5858 - val_loss: 0.7527\n",
      "Epoch 286/1000\n",
      "161/161 [==============================] - 0s 847us/step - loss: 0.5857 - val_loss: 0.7526\n",
      "Epoch 287/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5856 - val_loss: 0.7525\n",
      "Epoch 288/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.5855 - val_loss: 0.7524\n",
      "Epoch 289/1000\n",
      "161/161 [==============================] - 0s 795us/step - loss: 0.5854 - val_loss: 0.7523\n",
      "Epoch 290/1000\n",
      "161/161 [==============================] - 0s 785us/step - loss: 0.5853 - val_loss: 0.7522\n",
      "Epoch 291/1000\n",
      "161/161 [==============================] - 0s 802us/step - loss: 0.5852 - val_loss: 0.7521\n",
      "Epoch 292/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5851 - val_loss: 0.7520\n",
      "Epoch 293/1000\n",
      "161/161 [==============================] - 0s 775us/step - loss: 0.5850 - val_loss: 0.7519\n",
      "Epoch 294/1000\n",
      "161/161 [==============================] - 0s 838us/step - loss: 0.5849 - val_loss: 0.7518\n",
      "Epoch 295/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5848 - val_loss: 0.7518\n",
      "Epoch 296/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5847 - val_loss: 0.7517\n",
      "Epoch 297/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.5846 - val_loss: 0.7516\n",
      "Epoch 298/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5845 - val_loss: 0.7515\n",
      "Epoch 299/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.5844 - val_loss: 0.7514\n",
      "Epoch 300/1000\n",
      "161/161 [==============================] - 0s 800us/step - loss: 0.5843 - val_loss: 0.7513\n",
      "Epoch 301/1000\n",
      "161/161 [==============================] - 0s 783us/step - loss: 0.5842 - val_loss: 0.7512\n",
      "Epoch 302/1000\n",
      "161/161 [==============================] - 0s 847us/step - loss: 0.5841 - val_loss: 0.7511\n",
      "Epoch 303/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5840 - val_loss: 0.7511\n",
      "Epoch 304/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5839 - val_loss: 0.7510\n",
      "Epoch 305/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5838 - val_loss: 0.7509\n",
      "Epoch 306/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5837 - val_loss: 0.7508\n",
      "Epoch 307/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5836 - val_loss: 0.7507\n",
      "Epoch 308/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.5835 - val_loss: 0.7506\n",
      "Epoch 309/1000\n",
      "161/161 [==============================] - 0s 781us/step - loss: 0.5834 - val_loss: 0.7505\n",
      "Epoch 310/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.5833 - val_loss: 0.7505\n",
      "Epoch 311/1000\n",
      "161/161 [==============================] - 0s 828us/step - loss: 0.5832 - val_loss: 0.7504\n",
      "Epoch 312/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.5831 - val_loss: 0.7503\n",
      "Epoch 313/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5830 - val_loss: 0.7502\n",
      "Epoch 314/1000\n",
      "161/161 [==============================] - 0s 791us/step - loss: 0.5830 - val_loss: 0.7501\n",
      "Epoch 315/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5829 - val_loss: 0.7501\n",
      "Epoch 316/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.5828 - val_loss: 0.7500\n",
      "Epoch 317/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5827 - val_loss: 0.7499\n",
      "Epoch 318/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5826 - val_loss: 0.7498\n",
      "Epoch 319/1000\n",
      "161/161 [==============================] - 0s 825us/step - loss: 0.5825 - val_loss: 0.7497\n",
      "Epoch 320/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.5824 - val_loss: 0.7497\n",
      "Epoch 321/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5823 - val_loss: 0.7496\n",
      "Epoch 322/1000\n",
      "161/161 [==============================] - 0s 775us/step - loss: 0.5822 - val_loss: 0.7495\n",
      "Epoch 323/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.5822 - val_loss: 0.7494\n",
      "Epoch 324/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5821 - val_loss: 0.7494\n",
      "Epoch 325/1000\n",
      "161/161 [==============================] - 0s 794us/step - loss: 0.5820 - val_loss: 0.7493\n",
      "Epoch 326/1000\n",
      "161/161 [==============================] - 0s 802us/step - loss: 0.5819 - val_loss: 0.7492\n",
      "Epoch 327/1000\n",
      "161/161 [==============================] - 0s 854us/step - loss: 0.5818 - val_loss: 0.7491\n",
      "Epoch 328/1000\n",
      "161/161 [==============================] - 0s 782us/step - loss: 0.5817 - val_loss: 0.7491\n",
      "Epoch 329/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.5816 - val_loss: 0.7490\n",
      "Epoch 330/1000\n",
      "161/161 [==============================] - 0s 777us/step - loss: 0.5816 - val_loss: 0.7489\n",
      "Epoch 331/1000\n",
      "161/161 [==============================] - 0s 776us/step - loss: 0.5815 - val_loss: 0.7488\n",
      "Epoch 332/1000\n",
      "161/161 [==============================] - 0s 803us/step - loss: 0.5814 - val_loss: 0.7488\n",
      "Epoch 333/1000\n",
      "161/161 [==============================] - 0s 795us/step - loss: 0.5813 - val_loss: 0.7487\n",
      "Epoch 334/1000\n",
      "161/161 [==============================] - 0s 875us/step - loss: 0.5812 - val_loss: 0.7486\n",
      "Epoch 335/1000\n",
      "161/161 [==============================] - 0s 792us/step - loss: 0.5812 - val_loss: 0.7485\n",
      "Epoch 336/1000\n",
      "161/161 [==============================] - 0s 788us/step - loss: 0.5811 - val_loss: 0.7485\n",
      "Epoch 337/1000\n",
      "161/161 [==============================] - 0s 834us/step - loss: 0.5810 - val_loss: 0.7484\n",
      "Epoch 338/1000\n",
      "161/161 [==============================] - 0s 781us/step - loss: 0.5809 - val_loss: 0.7483\n",
      "Epoch 339/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.5808 - val_loss: 0.7483\n",
      "Epoch 340/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5808 - val_loss: 0.7482\n",
      "Epoch 341/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.5807 - val_loss: 0.7481\n",
      "Epoch 342/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.5806 - val_loss: 0.7480\n",
      "Epoch 343/1000\n",
      "161/161 [==============================] - 0s 839us/step - loss: 0.5805 - val_loss: 0.7480\n",
      "Epoch 344/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.5804 - val_loss: 0.7479\n",
      "Epoch 345/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5804 - val_loss: 0.7478\n",
      "Epoch 346/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.5803 - val_loss: 0.7478\n",
      "Epoch 347/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.5802 - val_loss: 0.7477\n",
      "Epoch 348/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5801 - val_loss: 0.7476\n",
      "Epoch 349/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.5801 - val_loss: 0.7476\n",
      "Epoch 350/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5800 - val_loss: 0.7475\n",
      "Epoch 351/1000\n",
      "161/161 [==============================] - 0s 833us/step - loss: 0.5799 - val_loss: 0.7474\n",
      "Epoch 352/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5798 - val_loss: 0.7474\n",
      "Epoch 353/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5798 - val_loss: 0.7473\n",
      "Epoch 354/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5797 - val_loss: 0.7472\n",
      "Epoch 355/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5796 - val_loss: 0.7472\n",
      "Epoch 356/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5796 - val_loss: 0.7471\n",
      "Epoch 357/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.5795 - val_loss: 0.7470\n",
      "Epoch 358/1000\n",
      "161/161 [==============================] - 0s 865us/step - loss: 0.5794 - val_loss: 0.7470\n",
      "Epoch 359/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5793 - val_loss: 0.7469\n",
      "Epoch 360/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.5793 - val_loss: 0.7469\n",
      "Epoch 361/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5792 - val_loss: 0.7468\n",
      "Epoch 362/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5791 - val_loss: 0.7467\n",
      "Epoch 363/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5791 - val_loss: 0.7467\n",
      "Epoch 364/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5790 - val_loss: 0.7466\n",
      "Epoch 365/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5789 - val_loss: 0.7465\n",
      "Epoch 366/1000\n",
      "161/161 [==============================] - 0s 839us/step - loss: 0.5789 - val_loss: 0.7465\n",
      "Epoch 367/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5788 - val_loss: 0.7464\n",
      "Epoch 368/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5787 - val_loss: 0.7464\n",
      "Epoch 369/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5787 - val_loss: 0.7463\n",
      "Epoch 370/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.5786 - val_loss: 0.7462\n",
      "Epoch 371/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5785 - val_loss: 0.7462\n",
      "Epoch 372/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5785 - val_loss: 0.7461\n",
      "Epoch 373/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5784 - val_loss: 0.7461\n",
      "Epoch 374/1000\n",
      "161/161 [==============================] - 0s 855us/step - loss: 0.5783 - val_loss: 0.7460\n",
      "Epoch 375/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5783 - val_loss: 0.7459\n",
      "Epoch 376/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5782 - val_loss: 0.7459\n",
      "Epoch 377/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5781 - val_loss: 0.7458\n",
      "Epoch 378/1000\n",
      "161/161 [==============================] - 0s 797us/step - loss: 0.5781 - val_loss: 0.7458\n",
      "Epoch 379/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5780 - val_loss: 0.7457\n",
      "Epoch 380/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5779 - val_loss: 0.7457\n",
      "Epoch 381/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5779 - val_loss: 0.7456\n",
      "Epoch 382/1000\n",
      "161/161 [==============================] - 0s 826us/step - loss: 0.5778 - val_loss: 0.7455\n",
      "Epoch 383/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5778 - val_loss: 0.7455\n",
      "Epoch 384/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5777 - val_loss: 0.7454\n",
      "Epoch 385/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5776 - val_loss: 0.7454\n",
      "Epoch 386/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5776 - val_loss: 0.7453\n",
      "Epoch 387/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.5775 - val_loss: 0.7453\n",
      "Epoch 388/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5775 - val_loss: 0.7452\n",
      "Epoch 389/1000\n",
      "161/161 [==============================] - 0s 833us/step - loss: 0.5774 - val_loss: 0.7452\n",
      "Epoch 390/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5773 - val_loss: 0.7451\n",
      "Epoch 391/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5773 - val_loss: 0.7450\n",
      "Epoch 392/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.5772 - val_loss: 0.7450\n",
      "Epoch 393/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5772 - val_loss: 0.7449\n",
      "Epoch 394/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5771 - val_loss: 0.7449\n",
      "Epoch 395/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5770 - val_loss: 0.7448\n",
      "Epoch 396/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5770 - val_loss: 0.7448\n",
      "Epoch 397/1000\n",
      "161/161 [==============================] - 0s 870us/step - loss: 0.5769 - val_loss: 0.7447\n",
      "Epoch 398/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5769 - val_loss: 0.7447\n",
      "Epoch 399/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5768 - val_loss: 0.7446\n",
      "Epoch 400/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5768 - val_loss: 0.7446\n",
      "Epoch 401/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5767 - val_loss: 0.7445\n",
      "Epoch 402/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5766 - val_loss: 0.7445\n",
      "Epoch 403/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5766 - val_loss: 0.7444\n",
      "Epoch 404/1000\n",
      "161/161 [==============================] - 0s 749us/step - loss: 0.5765 - val_loss: 0.7444\n",
      "Epoch 405/1000\n",
      "161/161 [==============================] - 0s 838us/step - loss: 0.5765 - val_loss: 0.7443\n",
      "Epoch 406/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5764 - val_loss: 0.7443\n",
      "Epoch 407/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5764 - val_loss: 0.7442\n",
      "Epoch 408/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5763 - val_loss: 0.7442\n",
      "Epoch 409/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5763 - val_loss: 0.7441\n",
      "Epoch 410/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5762 - val_loss: 0.7441\n",
      "Epoch 411/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.5762 - val_loss: 0.7440\n",
      "Epoch 412/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5761 - val_loss: 0.7440\n",
      "Epoch 413/1000\n",
      "161/161 [==============================] - 0s 840us/step - loss: 0.5761 - val_loss: 0.7439\n",
      "Epoch 414/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5760 - val_loss: 0.7439\n",
      "Epoch 415/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5759 - val_loss: 0.7438\n",
      "Epoch 416/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5759 - val_loss: 0.7438\n",
      "Epoch 417/1000\n",
      "161/161 [==============================] - 0s 801us/step - loss: 0.5758 - val_loss: 0.7437\n",
      "Epoch 418/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5758 - val_loss: 0.7437\n",
      "Epoch 419/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.5757 - val_loss: 0.7436\n",
      "Epoch 420/1000\n",
      "161/161 [==============================] - 0s 835us/step - loss: 0.5757 - val_loss: 0.7436\n",
      "Epoch 421/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5756 - val_loss: 0.7436\n",
      "Epoch 422/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5756 - val_loss: 0.7435\n",
      "Epoch 423/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.5755 - val_loss: 0.7435\n",
      "Epoch 424/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5755 - val_loss: 0.7434\n",
      "Epoch 425/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5754 - val_loss: 0.7434\n",
      "Epoch 426/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5754 - val_loss: 0.7433\n",
      "Epoch 427/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5753 - val_loss: 0.7433\n",
      "Epoch 428/1000\n",
      "161/161 [==============================] - 0s 840us/step - loss: 0.5753 - val_loss: 0.7432\n",
      "Epoch 429/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5752 - val_loss: 0.7432\n",
      "Epoch 430/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5752 - val_loss: 0.7432\n",
      "Epoch 431/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5751 - val_loss: 0.7431\n",
      "Epoch 432/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5751 - val_loss: 0.7431\n",
      "Epoch 433/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5750 - val_loss: 0.7430\n",
      "Epoch 434/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5750 - val_loss: 0.7430\n",
      "Epoch 435/1000\n",
      "161/161 [==============================] - 0s 844us/step - loss: 0.5750 - val_loss: 0.7429\n",
      "Epoch 436/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.5749 - val_loss: 0.7429\n",
      "Epoch 437/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5749 - val_loss: 0.7428\n",
      "Epoch 438/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5748 - val_loss: 0.7428\n",
      "Epoch 439/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5748 - val_loss: 0.7428\n",
      "Epoch 440/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.5747 - val_loss: 0.7427\n",
      "Epoch 441/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5747 - val_loss: 0.7427\n",
      "Epoch 442/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5746 - val_loss: 0.7426\n",
      "Epoch 443/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5746 - val_loss: 0.7426\n",
      "Epoch 444/1000\n",
      "161/161 [==============================] - 0s 826us/step - loss: 0.5745 - val_loss: 0.7426\n",
      "Epoch 445/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5745 - val_loss: 0.7425\n",
      "Epoch 446/1000\n",
      "161/161 [==============================] - 0s 751us/step - loss: 0.5744 - val_loss: 0.7425\n",
      "Epoch 447/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5744 - val_loss: 0.7424\n",
      "Epoch 448/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5744 - val_loss: 0.7424\n",
      "Epoch 449/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5743 - val_loss: 0.7424\n",
      "Epoch 450/1000\n",
      "161/161 [==============================] - 0s 751us/step - loss: 0.5743 - val_loss: 0.7423\n",
      "Epoch 451/1000\n",
      "161/161 [==============================] - 0s 832us/step - loss: 0.5742 - val_loss: 0.7423\n",
      "Epoch 452/1000\n",
      "161/161 [==============================] - 0s 801us/step - loss: 0.5742 - val_loss: 0.7422\n",
      "Epoch 453/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5741 - val_loss: 0.7422\n",
      "Epoch 454/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.5741 - val_loss: 0.7422\n",
      "Epoch 455/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.5741 - val_loss: 0.7421\n",
      "Epoch 456/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5740 - val_loss: 0.7421\n",
      "Epoch 457/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5740 - val_loss: 0.7420\n",
      "Epoch 458/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.5739 - val_loss: 0.7420\n",
      "Epoch 459/1000\n",
      "161/161 [==============================] - 0s 821us/step - loss: 0.5739 - val_loss: 0.7420\n",
      "Epoch 460/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5738 - val_loss: 0.7419\n",
      "Epoch 461/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5738 - val_loss: 0.7419\n",
      "Epoch 462/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5738 - val_loss: 0.7418\n",
      "Epoch 463/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5737 - val_loss: 0.7418\n",
      "Epoch 464/1000\n",
      "161/161 [==============================] - 0s 782us/step - loss: 0.5737 - val_loss: 0.7418\n",
      "Epoch 465/1000\n",
      "161/161 [==============================] - 0s 775us/step - loss: 0.5736 - val_loss: 0.7417\n",
      "Epoch 466/1000\n",
      "161/161 [==============================] - 0s 845us/step - loss: 0.5736 - val_loss: 0.7417\n",
      "Epoch 467/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5735 - val_loss: 0.7417\n",
      "Epoch 468/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5735 - val_loss: 0.7416\n",
      "Epoch 469/1000\n",
      "161/161 [==============================] - 0s 800us/step - loss: 0.5735 - val_loss: 0.7416\n",
      "Epoch 470/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5734 - val_loss: 0.7416\n",
      "Epoch 471/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.5734 - val_loss: 0.7415\n",
      "Epoch 472/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5733 - val_loss: 0.7415\n",
      "Epoch 473/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.5733 - val_loss: 0.7414\n",
      "Epoch 474/1000\n",
      "161/161 [==============================] - 0s 832us/step - loss: 0.5733 - val_loss: 0.7414\n",
      "Epoch 475/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5732 - val_loss: 0.7414\n",
      "Epoch 476/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5732 - val_loss: 0.7413\n",
      "Epoch 477/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5731 - val_loss: 0.7413\n",
      "Epoch 478/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5731 - val_loss: 0.7413\n",
      "Epoch 479/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5731 - val_loss: 0.7412\n",
      "Epoch 480/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5730 - val_loss: 0.7412\n",
      "Epoch 481/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.5730 - val_loss: 0.7412\n",
      "Epoch 482/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5729 - val_loss: 0.7411\n",
      "Epoch 483/1000\n",
      "161/161 [==============================] - 0s 841us/step - loss: 0.5729 - val_loss: 0.7411\n",
      "Epoch 484/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5729 - val_loss: 0.7411\n",
      "Epoch 485/1000\n",
      "161/161 [==============================] - 0s 794us/step - loss: 0.5728 - val_loss: 0.7410\n",
      "Epoch 486/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5728 - val_loss: 0.7410\n",
      "Epoch 487/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5728 - val_loss: 0.7410\n",
      "Epoch 488/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5727 - val_loss: 0.7409\n",
      "Epoch 489/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5727 - val_loss: 0.7409\n",
      "Epoch 490/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5726 - val_loss: 0.7409\n",
      "Epoch 491/1000\n",
      "161/161 [==============================] - 0s 836us/step - loss: 0.5726 - val_loss: 0.7408\n",
      "Epoch 492/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5726 - val_loss: 0.7408\n",
      "Epoch 493/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.5725 - val_loss: 0.7408\n",
      "Epoch 494/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5725 - val_loss: 0.7407\n",
      "Epoch 495/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5725 - val_loss: 0.7407\n",
      "Epoch 496/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5724 - val_loss: 0.7407\n",
      "Epoch 497/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5724 - val_loss: 0.7406\n",
      "Epoch 498/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5723 - val_loss: 0.7406\n",
      "Epoch 499/1000\n",
      "161/161 [==============================] - 0s 837us/step - loss: 0.5723 - val_loss: 0.7406\n",
      "Epoch 500/1000\n",
      "161/161 [==============================] - 0s 802us/step - loss: 0.5723 - val_loss: 0.7405\n",
      "Epoch 501/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.5722 - val_loss: 0.7405\n",
      "Epoch 502/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5722 - val_loss: 0.7405\n",
      "Epoch 503/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.5722 - val_loss: 0.7404\n",
      "Epoch 504/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5721 - val_loss: 0.7404\n",
      "Epoch 505/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5721 - val_loss: 0.7404\n",
      "Epoch 506/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5721 - val_loss: 0.7403\n",
      "Epoch 507/1000\n",
      "161/161 [==============================] - 0s 837us/step - loss: 0.5720 - val_loss: 0.7403\n",
      "Epoch 508/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.5720 - val_loss: 0.7403\n",
      "Epoch 509/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5719 - val_loss: 0.7403\n",
      "Epoch 510/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.5719 - val_loss: 0.7402\n",
      "Epoch 511/1000\n",
      "161/161 [==============================] - 0s 775us/step - loss: 0.5719 - val_loss: 0.7402\n",
      "Epoch 512/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5718 - val_loss: 0.7402\n",
      "Epoch 513/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5718 - val_loss: 0.7401\n",
      "Epoch 514/1000\n",
      "161/161 [==============================] - 0s 856us/step - loss: 0.5718 - val_loss: 0.7401\n",
      "Epoch 515/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.5717 - val_loss: 0.7401\n",
      "Epoch 516/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5717 - val_loss: 0.7400\n",
      "Epoch 517/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5717 - val_loss: 0.7400\n",
      "Epoch 518/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.5716 - val_loss: 0.7400\n",
      "Epoch 519/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.5716 - val_loss: 0.7400\n",
      "Epoch 520/1000\n",
      "161/161 [==============================] - 0s 873us/step - loss: 0.5716 - val_loss: 0.7399\n",
      "Epoch 521/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.5715 - val_loss: 0.7399\n",
      "Epoch 522/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5715 - val_loss: 0.7399\n",
      "Epoch 523/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5715 - val_loss: 0.7398\n",
      "Epoch 524/1000\n",
      "161/161 [==============================] - 0s 791us/step - loss: 0.5714 - val_loss: 0.7398\n",
      "Epoch 525/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.5714 - val_loss: 0.7398\n",
      "Epoch 526/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5714 - val_loss: 0.7398\n",
      "Epoch 527/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5713 - val_loss: 0.7397\n",
      "Epoch 528/1000\n",
      "161/161 [==============================] - 0s 837us/step - loss: 0.5713 - val_loss: 0.7397\n",
      "Epoch 529/1000\n",
      "161/161 [==============================] - 0s 794us/step - loss: 0.5713 - val_loss: 0.7397\n",
      "Epoch 530/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.5712 - val_loss: 0.7397\n",
      "Epoch 531/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5712 - val_loss: 0.7396\n",
      "Epoch 532/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5712 - val_loss: 0.7396\n",
      "Epoch 533/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5711 - val_loss: 0.7396\n",
      "Epoch 534/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5711 - val_loss: 0.7395\n",
      "Epoch 535/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5711 - val_loss: 0.7395\n",
      "Epoch 536/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5710 - val_loss: 0.7395\n",
      "Epoch 537/1000\n",
      "161/161 [==============================] - 0s 840us/step - loss: 0.5710 - val_loss: 0.7395\n",
      "Epoch 538/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.5710 - val_loss: 0.7394\n",
      "Epoch 539/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5709 - val_loss: 0.7394\n",
      "Epoch 540/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.5709 - val_loss: 0.7394\n",
      "Epoch 541/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.5709 - val_loss: 0.7394\n",
      "Epoch 542/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.5708 - val_loss: 0.7393\n",
      "Epoch 543/1000\n",
      "161/161 [==============================] - 0s 788us/step - loss: 0.5708 - val_loss: 0.7393\n",
      "Epoch 544/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5708 - val_loss: 0.7393\n",
      "Epoch 545/1000\n",
      "161/161 [==============================] - 0s 839us/step - loss: 0.5707 - val_loss: 0.7393\n",
      "Epoch 546/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5707 - val_loss: 0.7392\n",
      "Epoch 547/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.5707 - val_loss: 0.7392\n",
      "Epoch 548/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5706 - val_loss: 0.7392\n",
      "Epoch 549/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5706 - val_loss: 0.7392\n",
      "Epoch 550/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5706 - val_loss: 0.7391\n",
      "Epoch 551/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5706 - val_loss: 0.7391\n",
      "Epoch 552/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5705 - val_loss: 0.7391\n",
      "Epoch 553/1000\n",
      "161/161 [==============================] - 0s 836us/step - loss: 0.5705 - val_loss: 0.7391\n",
      "Epoch 554/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5705 - val_loss: 0.7390\n",
      "Epoch 555/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5704 - val_loss: 0.7390\n",
      "Epoch 556/1000\n",
      "161/161 [==============================] - 0s 796us/step - loss: 0.5704 - val_loss: 0.7390\n",
      "Epoch 557/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5704 - val_loss: 0.7390\n",
      "Epoch 558/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5703 - val_loss: 0.7389\n",
      "Epoch 559/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5703 - val_loss: 0.7389\n",
      "Epoch 560/1000\n",
      "161/161 [==============================] - 0s 837us/step - loss: 0.5703 - val_loss: 0.7389\n",
      "Epoch 561/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5702 - val_loss: 0.7389\n",
      "Epoch 562/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5702 - val_loss: 0.7389\n",
      "Epoch 563/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5702 - val_loss: 0.7388\n",
      "Epoch 564/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5702 - val_loss: 0.7388\n",
      "Epoch 565/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5701 - val_loss: 0.7388\n",
      "Epoch 566/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5701 - val_loss: 0.7388\n",
      "Epoch 567/1000\n",
      "161/161 [==============================] - 0s 822us/step - loss: 0.5701 - val_loss: 0.7387\n",
      "Epoch 568/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5700 - val_loss: 0.7387\n",
      "Epoch 569/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5700 - val_loss: 0.7387\n",
      "Epoch 570/1000\n",
      "161/161 [==============================] - 0s 799us/step - loss: 0.5700 - val_loss: 0.7387\n",
      "Epoch 571/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5700 - val_loss: 0.7386\n",
      "Epoch 572/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5699 - val_loss: 0.7386\n",
      "Epoch 573/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5699 - val_loss: 0.7386\n",
      "Epoch 574/1000\n",
      "161/161 [==============================] - 0s 839us/step - loss: 0.5699 - val_loss: 0.7386\n",
      "Epoch 575/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5698 - val_loss: 0.7386\n",
      "Epoch 576/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5698 - val_loss: 0.7385\n",
      "Epoch 577/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5698 - val_loss: 0.7385\n",
      "Epoch 578/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5697 - val_loss: 0.7385\n",
      "Epoch 579/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5697 - val_loss: 0.7385\n",
      "Epoch 580/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.5697 - val_loss: 0.7385\n",
      "Epoch 581/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.5697 - val_loss: 0.7384\n",
      "Epoch 582/1000\n",
      "161/161 [==============================] - 0s 880us/step - loss: 0.5696 - val_loss: 0.7384\n",
      "Epoch 583/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5696 - val_loss: 0.7384\n",
      "Epoch 584/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.5696 - val_loss: 0.7384\n",
      "Epoch 585/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.5695 - val_loss: 0.7384\n",
      "Epoch 586/1000\n",
      "161/161 [==============================] - 0s 810us/step - loss: 0.5695 - val_loss: 0.7383\n",
      "Epoch 587/1000\n",
      "161/161 [==============================] - 0s 830us/step - loss: 0.5695 - val_loss: 0.7383\n",
      "Epoch 588/1000\n",
      "161/161 [==============================] - 0s 863us/step - loss: 0.5695 - val_loss: 0.7383\n",
      "Epoch 589/1000\n",
      "161/161 [==============================] - 0s 799us/step - loss: 0.5694 - val_loss: 0.7383\n",
      "Epoch 590/1000\n",
      "161/161 [==============================] - 0s 802us/step - loss: 0.5694 - val_loss: 0.7383\n",
      "Epoch 591/1000\n",
      "161/161 [==============================] - 0s 807us/step - loss: 0.5694 - val_loss: 0.7382\n",
      "Epoch 592/1000\n",
      "161/161 [==============================] - 0s 796us/step - loss: 0.5694 - val_loss: 0.7382\n",
      "Epoch 593/1000\n",
      "161/161 [==============================] - 0s 778us/step - loss: 0.5693 - val_loss: 0.7382\n",
      "Epoch 594/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.5693 - val_loss: 0.7382\n",
      "Epoch 595/1000\n",
      "161/161 [==============================] - 0s 900us/step - loss: 0.5693 - val_loss: 0.7382\n",
      "Epoch 596/1000\n",
      "161/161 [==============================] - 0s 778us/step - loss: 0.5692 - val_loss: 0.7381\n",
      "Epoch 597/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.5692 - val_loss: 0.7381\n",
      "Epoch 598/1000\n",
      "161/161 [==============================] - 0s 780us/step - loss: 0.5692 - val_loss: 0.7381\n",
      "Epoch 599/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.5692 - val_loss: 0.7381\n",
      "Epoch 600/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.5691 - val_loss: 0.7381\n",
      "Epoch 601/1000\n",
      "161/161 [==============================] - 0s 786us/step - loss: 0.5691 - val_loss: 0.7381\n",
      "Epoch 602/1000\n",
      "161/161 [==============================] - 0s 858us/step - loss: 0.5691 - val_loss: 0.7380\n",
      "Epoch 603/1000\n",
      "161/161 [==============================] - 0s 776us/step - loss: 0.5691 - val_loss: 0.7380\n",
      "Epoch 604/1000\n",
      "161/161 [==============================] - 0s 786us/step - loss: 0.5690 - val_loss: 0.7380\n",
      "Epoch 605/1000\n",
      "161/161 [==============================] - 0s 822us/step - loss: 0.5690 - val_loss: 0.7380\n",
      "Epoch 606/1000\n",
      "161/161 [==============================] - 0s 795us/step - loss: 0.5690 - val_loss: 0.7380\n",
      "Epoch 607/1000\n",
      "161/161 [==============================] - 0s 788us/step - loss: 0.5689 - val_loss: 0.7379\n",
      "Epoch 608/1000\n",
      "161/161 [==============================] - 0s 840us/step - loss: 0.5689 - val_loss: 0.7379\n",
      "Epoch 609/1000\n",
      "161/161 [==============================] - 0s 882us/step - loss: 0.5689 - val_loss: 0.7379\n",
      "Epoch 610/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.5689 - val_loss: 0.7379\n",
      "Epoch 611/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5688 - val_loss: 0.7379\n",
      "Epoch 612/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5688 - val_loss: 0.7379\n",
      "Epoch 613/1000\n",
      "161/161 [==============================] - 0s 777us/step - loss: 0.5688 - val_loss: 0.7378\n",
      "Epoch 614/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5688 - val_loss: 0.7378\n",
      "Epoch 615/1000\n",
      "161/161 [==============================] - 0s 838us/step - loss: 0.5687 - val_loss: 0.7378\n",
      "Epoch 616/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.5687 - val_loss: 0.7378\n",
      "Epoch 617/1000\n",
      "161/161 [==============================] - 0s 775us/step - loss: 0.5687 - val_loss: 0.7378\n",
      "Epoch 618/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5687 - val_loss: 0.7378\n",
      "Epoch 619/1000\n",
      "161/161 [==============================] - 0s 797us/step - loss: 0.5686 - val_loss: 0.7378\n",
      "Epoch 620/1000\n",
      "161/161 [==============================] - 0s 775us/step - loss: 0.5686 - val_loss: 0.7377\n",
      "Epoch 621/1000\n",
      "161/161 [==============================] - 0s 831us/step - loss: 0.5686 - val_loss: 0.7377\n",
      "Epoch 622/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.5686 - val_loss: 0.7377\n",
      "Epoch 623/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5685 - val_loss: 0.7377\n",
      "Epoch 624/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5685 - val_loss: 0.7377\n",
      "Epoch 625/1000\n",
      "161/161 [==============================] - 0s 776us/step - loss: 0.5685 - val_loss: 0.7377\n",
      "Epoch 626/1000\n",
      "161/161 [==============================] - 0s 782us/step - loss: 0.5685 - val_loss: 0.7376\n",
      "Epoch 627/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.5684 - val_loss: 0.7376\n",
      "Epoch 628/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5684 - val_loss: 0.7376\n",
      "Epoch 629/1000\n",
      "161/161 [==============================] - 0s 863us/step - loss: 0.5684 - val_loss: 0.7376\n",
      "Epoch 630/1000\n",
      "161/161 [==============================] - 0s 792us/step - loss: 0.5684 - val_loss: 0.7376\n",
      "Epoch 631/1000\n",
      "161/161 [==============================] - 0s 805us/step - loss: 0.5683 - val_loss: 0.7376\n",
      "Epoch 632/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5683 - val_loss: 0.7376\n",
      "Epoch 633/1000\n",
      "161/161 [==============================] - 0s 782us/step - loss: 0.5683 - val_loss: 0.7375\n",
      "Epoch 634/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5683 - val_loss: 0.7375\n",
      "Epoch 635/1000\n",
      "161/161 [==============================] - 0s 858us/step - loss: 0.5682 - val_loss: 0.7375\n",
      "Epoch 636/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.5682 - val_loss: 0.7375\n",
      "Epoch 637/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.5682 - val_loss: 0.7375\n",
      "Epoch 638/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5682 - val_loss: 0.7375\n",
      "Epoch 639/1000\n",
      "161/161 [==============================] - 0s 784us/step - loss: 0.5681 - val_loss: 0.7375\n",
      "Epoch 640/1000\n",
      "161/161 [==============================] - 0s 784us/step - loss: 0.5681 - val_loss: 0.7375\n",
      "Epoch 641/1000\n",
      "161/161 [==============================] - 0s 793us/step - loss: 0.5681 - val_loss: 0.7374\n",
      "Epoch 642/1000\n",
      "161/161 [==============================] - 0s 860us/step - loss: 0.5681 - val_loss: 0.7374\n",
      "Epoch 643/1000\n",
      "161/161 [==============================] - 0s 816us/step - loss: 0.5680 - val_loss: 0.7374\n",
      "Epoch 644/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.5680 - val_loss: 0.7374\n",
      "Epoch 645/1000\n",
      "161/161 [==============================] - 0s 797us/step - loss: 0.5680 - val_loss: 0.7374\n",
      "Epoch 646/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5680 - val_loss: 0.7374\n",
      "Epoch 647/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5679 - val_loss: 0.7374\n",
      "Epoch 648/1000\n",
      "161/161 [==============================] - 0s 803us/step - loss: 0.5679 - val_loss: 0.7373\n",
      "Epoch 649/1000\n",
      "161/161 [==============================] - 0s 828us/step - loss: 0.5679 - val_loss: 0.7373\n",
      "Epoch 650/1000\n",
      "161/161 [==============================] - 0s 778us/step - loss: 0.5679 - val_loss: 0.7373\n",
      "Epoch 651/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5678 - val_loss: 0.7373\n",
      "Epoch 652/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.5678 - val_loss: 0.7373\n",
      "Epoch 653/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5678 - val_loss: 0.7373\n",
      "Epoch 654/1000\n",
      "161/161 [==============================] - 0s 791us/step - loss: 0.5678 - val_loss: 0.7373\n",
      "Epoch 655/1000\n",
      "161/161 [==============================] - 0s 834us/step - loss: 0.5678 - val_loss: 0.7373\n",
      "Epoch 656/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.5677 - val_loss: 0.7373\n",
      "Epoch 657/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.5677 - val_loss: 0.7372\n",
      "Epoch 658/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5677 - val_loss: 0.7372\n",
      "Epoch 659/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5677 - val_loss: 0.7372\n",
      "Epoch 660/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5676 - val_loss: 0.7372\n",
      "Epoch 661/1000\n",
      "161/161 [==============================] - 0s 846us/step - loss: 0.5676 - val_loss: 0.7372\n",
      "Epoch 662/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5676 - val_loss: 0.7372\n",
      "Epoch 663/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.5676 - val_loss: 0.7372\n",
      "Epoch 664/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5675 - val_loss: 0.7372\n",
      "Epoch 665/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5675 - val_loss: 0.7372\n",
      "Epoch 666/1000\n",
      "161/161 [==============================] - 0s 792us/step - loss: 0.5675 - val_loss: 0.7372\n",
      "Epoch 667/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5675 - val_loss: 0.7371\n",
      "Epoch 668/1000\n",
      "161/161 [==============================] - 0s 849us/step - loss: 0.5675 - val_loss: 0.7371\n",
      "Epoch 669/1000\n",
      "161/161 [==============================] - 0s 777us/step - loss: 0.5674 - val_loss: 0.7371\n",
      "Epoch 670/1000\n",
      "161/161 [==============================] - 0s 776us/step - loss: 0.5674 - val_loss: 0.7371\n",
      "Epoch 671/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.5674 - val_loss: 0.7371\n",
      "Epoch 672/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5674 - val_loss: 0.7371\n",
      "Epoch 673/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.5673 - val_loss: 0.7371\n",
      "Epoch 674/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5673 - val_loss: 0.7371\n",
      "Epoch 675/1000\n",
      "161/161 [==============================] - 0s 827us/step - loss: 0.5673 - val_loss: 0.7371\n",
      "Epoch 676/1000\n",
      "161/161 [==============================] - 0s 746us/step - loss: 0.5673 - val_loss: 0.7371\n",
      "Epoch 677/1000\n",
      "161/161 [==============================] - 0s 782us/step - loss: 0.5673 - val_loss: 0.7371\n",
      "Epoch 678/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.5672 - val_loss: 0.7370\n",
      "Epoch 679/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5672 - val_loss: 0.7370\n",
      "Epoch 680/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5672 - val_loss: 0.7370\n",
      "Epoch 681/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5672 - val_loss: 0.7370\n",
      "Epoch 682/1000\n",
      "161/161 [==============================] - 0s 792us/step - loss: 0.5671 - val_loss: 0.7370\n",
      "Epoch 683/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5671 - val_loss: 0.7370\n",
      "Epoch 684/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5671 - val_loss: 0.7370\n",
      "Epoch 685/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5671 - val_loss: 0.7370\n",
      "Epoch 686/1000\n",
      "161/161 [==============================] - 0s 862us/step - loss: 0.5671 - val_loss: 0.7370\n",
      "Epoch 687/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5670 - val_loss: 0.7370\n",
      "Epoch 688/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5670 - val_loss: 0.7370\n",
      "Epoch 689/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.5670 - val_loss: 0.7370\n",
      "Epoch 690/1000\n",
      "161/161 [==============================] - 0s 789us/step - loss: 0.5670 - val_loss: 0.7370\n",
      "Epoch 691/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.5670 - val_loss: 0.7369\n",
      "Epoch 692/1000\n",
      "161/161 [==============================] - 0s 839us/step - loss: 0.5669 - val_loss: 0.7369\n",
      "Epoch 693/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5669 - val_loss: 0.7369\n",
      "Epoch 694/1000\n",
      "161/161 [==============================] - 0s 751us/step - loss: 0.5669 - val_loss: 0.7369\n",
      "Epoch 695/1000\n",
      "161/161 [==============================] - 0s 783us/step - loss: 0.5669 - val_loss: 0.7369\n",
      "Epoch 696/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.5669 - val_loss: 0.7369\n",
      "Epoch 697/1000\n",
      "161/161 [==============================] - 0s 803us/step - loss: 0.5668 - val_loss: 0.7369\n",
      "Epoch 698/1000\n",
      "161/161 [==============================] - 0s 872us/step - loss: 0.5668 - val_loss: 0.7369\n",
      "Epoch 699/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.5668 - val_loss: 0.7369\n",
      "Epoch 700/1000\n",
      "161/161 [==============================] - 0s 785us/step - loss: 0.5668 - val_loss: 0.7369\n",
      "Epoch 701/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5668 - val_loss: 0.7369\n",
      "Epoch 702/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.5667 - val_loss: 0.7369\n",
      "Epoch 703/1000\n",
      "161/161 [==============================] - 0s 894us/step - loss: 0.5667 - val_loss: 0.7369\n",
      "Epoch 704/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5667 - val_loss: 0.7369\n",
      "Epoch 705/1000\n",
      "161/161 [==============================] - 0s 751us/step - loss: 0.5667 - val_loss: 0.7369\n",
      "Epoch 706/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5667 - val_loss: 0.7369\n",
      "Epoch 707/1000\n",
      "161/161 [==============================] - 0s 799us/step - loss: 0.5666 - val_loss: 0.7368\n",
      "Epoch 708/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.5666 - val_loss: 0.7368\n",
      "Epoch 709/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5666 - val_loss: 0.7368\n",
      "Epoch 710/1000\n",
      "161/161 [==============================] - 0s 805us/step - loss: 0.5666 - val_loss: 0.7368\n",
      "Epoch 711/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.5666 - val_loss: 0.7368\n",
      "Epoch 712/1000\n",
      "161/161 [==============================] - 0s 803us/step - loss: 0.5665 - val_loss: 0.7368\n",
      "Epoch 713/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.5665 - val_loss: 0.7368\n",
      "Epoch 714/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5665 - val_loss: 0.7368\n",
      "Epoch 715/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5665 - val_loss: 0.7368\n",
      "Epoch 716/1000\n",
      "161/161 [==============================] - 0s 790us/step - loss: 0.5665 - val_loss: 0.7368\n",
      "Epoch 717/1000\n",
      "161/161 [==============================] - 0s 904us/step - loss: 0.5664 - val_loss: 0.7368\n",
      "Epoch 718/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5664 - val_loss: 0.7368\n",
      "Epoch 719/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.5664 - val_loss: 0.7368\n",
      "Epoch 720/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5664 - val_loss: 0.7368\n",
      "Epoch 721/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5664 - val_loss: 0.7368\n",
      "Epoch 722/1000\n",
      "161/161 [==============================] - 0s 846us/step - loss: 0.5664 - val_loss: 0.7368\n",
      "Epoch 723/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.5663 - val_loss: 0.7368\n",
      "Epoch 724/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5663 - val_loss: 0.7368\n",
      "Epoch 725/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5663 - val_loss: 0.7368\n",
      "Epoch 726/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5663 - val_loss: 0.7368\n",
      "Epoch 727/1000\n",
      "161/161 [==============================] - 0s 778us/step - loss: 0.5663 - val_loss: 0.7368\n",
      "Epoch 728/1000\n",
      "161/161 [==============================] - 0s 828us/step - loss: 0.5662 - val_loss: 0.7368\n",
      "Epoch 729/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5662 - val_loss: 0.7368\n",
      "Epoch 730/1000\n",
      "161/161 [==============================] - 0s 752us/step - loss: 0.5662 - val_loss: 0.7368\n",
      "Epoch 731/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5662 - val_loss: 0.7368\n",
      "Epoch 732/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5662 - val_loss: 0.7368\n",
      "Epoch 733/1000\n",
      "161/161 [==============================] - 0s 832us/step - loss: 0.5662 - val_loss: 0.7368\n",
      "Epoch 734/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5661 - val_loss: 0.7368\n",
      "Epoch 735/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5661 - val_loss: 0.7368\n",
      "Epoch 736/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5661 - val_loss: 0.7368\n",
      "Epoch 737/1000\n",
      "161/161 [==============================] - 0s 785us/step - loss: 0.5661 - val_loss: 0.7367\n",
      "Epoch 738/1000\n",
      "161/161 [==============================] - 0s 844us/step - loss: 0.5661 - val_loss: 0.7367\n",
      "Epoch 739/1000\n",
      "161/161 [==============================] - 0s 751us/step - loss: 0.5661 - val_loss: 0.7367\n",
      "Epoch 740/1000\n",
      "161/161 [==============================] - 0s 749us/step - loss: 0.5660 - val_loss: 0.7367\n",
      "Epoch 741/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.5660 - val_loss: 0.7367\n",
      "Epoch 742/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.5660 - val_loss: 0.7367\n",
      "Epoch 743/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5660 - val_loss: 0.7367\n",
      "Epoch 744/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.5660 - val_loss: 0.7367\n",
      "Epoch 745/1000\n",
      "161/161 [==============================] - 0s 831us/step - loss: 0.5660 - val_loss: 0.7367\n",
      "Epoch 746/1000\n",
      "161/161 [==============================] - 0s 752us/step - loss: 0.5659 - val_loss: 0.7367\n",
      "Epoch 747/1000\n",
      "161/161 [==============================] - 0s 778us/step - loss: 0.5659 - val_loss: 0.7367\n",
      "Epoch 748/1000\n",
      "161/161 [==============================] - 0s 752us/step - loss: 0.5659 - val_loss: 0.7367\n",
      "Epoch 749/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.5659 - val_loss: 0.7367\n",
      "Epoch 750/1000\n",
      "161/161 [==============================] - 0s 828us/step - loss: 0.5659 - val_loss: 0.7367\n",
      "Epoch 751/1000\n",
      "161/161 [==============================] - 0s 752us/step - loss: 0.5659 - val_loss: 0.7367\n",
      "Epoch 752/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.5658 - val_loss: 0.7367\n",
      "Epoch 753/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.5658 - val_loss: 0.7367\n",
      "Epoch 754/1000\n",
      "161/161 [==============================] - 0s 751us/step - loss: 0.5658 - val_loss: 0.7367\n",
      "Epoch 755/1000\n",
      "161/161 [==============================] - 0s 826us/step - loss: 0.5658 - val_loss: 0.7367\n",
      "Epoch 756/1000\n",
      "161/161 [==============================] - 0s 749us/step - loss: 0.5658 - val_loss: 0.7367\n",
      "Epoch 757/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.5658 - val_loss: 0.7367\n",
      "Epoch 758/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5657 - val_loss: 0.7367\n",
      "Epoch 759/1000\n",
      "161/161 [==============================] - 0s 785us/step - loss: 0.5657 - val_loss: 0.7367\n",
      "Epoch 760/1000\n",
      "161/161 [==============================] - 0s 751us/step - loss: 0.5657 - val_loss: 0.7367\n",
      "Epoch 761/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.5657 - val_loss: 0.7367\n",
      "Epoch 762/1000\n",
      "161/161 [==============================] - 0s 835us/step - loss: 0.5657 - val_loss: 0.7367\n",
      "Epoch 763/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.5657 - val_loss: 0.7367\n",
      "Epoch 764/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.5656 - val_loss: 0.7367\n",
      "Epoch 765/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.5656 - val_loss: 0.7367\n",
      "Epoch 766/1000\n",
      "161/161 [==============================] - 0s 749us/step - loss: 0.5656 - val_loss: 0.7367\n",
      "Epoch 767/1000\n",
      "161/161 [==============================] - 0s 816us/step - loss: 0.5656 - val_loss: 0.7367\n",
      "Epoch 768/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.5656 - val_loss: 0.7367\n",
      "Epoch 769/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.5656 - val_loss: 0.7367\n",
      "Epoch 770/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5656 - val_loss: 0.7367\n",
      "Epoch 771/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5655 - val_loss: 0.7367\n",
      "Epoch 772/1000\n",
      "161/161 [==============================] - 0s 811us/step - loss: 0.5655 - val_loss: 0.7367\n",
      "Epoch 773/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5655 - val_loss: 0.7367\n",
      "Epoch 774/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.5655 - val_loss: 0.7367\n",
      "Epoch 775/1000\n",
      "161/161 [==============================] - 0s 749us/step - loss: 0.5655 - val_loss: 0.7367\n",
      "Epoch 776/1000\n",
      "161/161 [==============================] - 0s 749us/step - loss: 0.5655 - val_loss: 0.7367\n",
      "Epoch 777/1000\n",
      "161/161 [==============================] - 0s 782us/step - loss: 0.5655 - val_loss: 0.7367\n",
      "Epoch 778/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.5654 - val_loss: 0.7367\n",
      "Epoch 779/1000\n",
      "161/161 [==============================] - 0s 825us/step - loss: 0.5654 - val_loss: 0.7367\n",
      "Epoch 780/1000\n",
      "161/161 [==============================] - 0s 740us/step - loss: 0.5654 - val_loss: 0.7367\n",
      "Epoch 781/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.5654 - val_loss: 0.7367\n",
      "Epoch 782/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5654 - val_loss: 0.7367\n",
      "Epoch 783/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5654 - val_loss: 0.7367\n",
      "Epoch 784/1000\n",
      "161/161 [==============================] - 0s 816us/step - loss: 0.5654 - val_loss: 0.7367\n",
      "Epoch 785/1000\n",
      "161/161 [==============================] - 0s 746us/step - loss: 0.5653 - val_loss: 0.7367\n",
      "Epoch 786/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.5653 - val_loss: 0.7367\n",
      "Epoch 787/1000\n",
      "161/161 [==============================] - 0s 776us/step - loss: 0.5653 - val_loss: 0.7368\n",
      "Epoch 788/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.5653 - val_loss: 0.7368\n",
      "Epoch 789/1000\n",
      "161/161 [==============================] - 0s 809us/step - loss: 0.5653 - val_loss: 0.7368\n",
      "Epoch 790/1000\n",
      "161/161 [==============================] - 0s 749us/step - loss: 0.5653 - val_loss: 0.7368\n",
      "Epoch 791/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.5653 - val_loss: 0.7368\n",
      "Epoch 792/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.5652 - val_loss: 0.7368\n",
      "Epoch 793/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.5652 - val_loss: 0.7368\n",
      "Epoch 794/1000\n",
      "161/161 [==============================] - 0s 822us/step - loss: 0.5652 - val_loss: 0.7368\n",
      "Epoch 795/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.5652 - val_loss: 0.7368\n",
      "Epoch 796/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.5652 - val_loss: 0.7368\n",
      "Epoch 797/1000\n",
      "161/161 [==============================] - 0s 784us/step - loss: 0.5652 - val_loss: 0.7368\n",
      "Epoch 798/1000\n",
      "161/161 [==============================] - 0s 751us/step - loss: 0.5652 - val_loss: 0.7368\n",
      "Epoch 799/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.5652 - val_loss: 0.7368\n",
      "Epoch 800/1000\n",
      "161/161 [==============================] - 0s 819us/step - loss: 0.5651 - val_loss: 0.7368\n",
      "Epoch 801/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5651 - val_loss: 0.7368\n",
      "Epoch 802/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.5651 - val_loss: 0.7368\n",
      "Epoch 803/1000\n",
      "161/161 [==============================] - 0s 749us/step - loss: 0.5651 - val_loss: 0.7368\n",
      "Epoch 804/1000\n",
      "161/161 [==============================] - 0s 749us/step - loss: 0.5651 - val_loss: 0.7368\n",
      "Epoch 805/1000\n",
      "161/161 [==============================] - 0s 872us/step - loss: 0.5651 - val_loss: 0.7368\n",
      "Epoch 806/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.5651 - val_loss: 0.7368\n",
      "Epoch 807/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.5650 - val_loss: 0.7368\n",
      "Epoch 808/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5650 - val_loss: 0.7368\n",
      "Epoch 809/1000\n",
      "161/161 [==============================] - 0s 752us/step - loss: 0.5650 - val_loss: 0.7368\n",
      "Epoch 810/1000\n",
      "161/161 [==============================] - 0s 827us/step - loss: 0.5650 - val_loss: 0.7368\n",
      "Epoch 811/1000\n",
      "161/161 [==============================] - 0s 751us/step - loss: 0.5650 - val_loss: 0.7368\n",
      "Epoch 812/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.5650 - val_loss: 0.7368\n",
      "Epoch 813/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.5650 - val_loss: 0.7368\n",
      "Epoch 814/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.5650 - val_loss: 0.7368\n",
      "Epoch 815/1000\n",
      "161/161 [==============================] - 0s 873us/step - loss: 0.5649 - val_loss: 0.7368\n",
      "Epoch 816/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.5649 - val_loss: 0.7368\n",
      "Epoch 817/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5649 - val_loss: 0.7368\n",
      "Epoch 818/1000\n",
      "161/161 [==============================] - 0s 746us/step - loss: 0.5649 - val_loss: 0.7368\n",
      "Epoch 819/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5649 - val_loss: 0.7368\n",
      "Epoch 820/1000\n",
      "161/161 [==============================] - 0s 844us/step - loss: 0.5649 - val_loss: 0.7368\n",
      "Epoch 821/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.5649 - val_loss: 0.7368\n",
      "Epoch 822/1000\n",
      "161/161 [==============================] - 0s 779us/step - loss: 0.5649 - val_loss: 0.7368\n",
      "Epoch 823/1000\n",
      "161/161 [==============================] - 0s 792us/step - loss: 0.5648 - val_loss: 0.7368\n",
      "Epoch 824/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5648 - val_loss: 0.7368\n",
      "Epoch 825/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.5648 - val_loss: 0.7368\n",
      "Epoch 826/1000\n",
      "161/161 [==============================] - 0s 845us/step - loss: 0.5648 - val_loss: 0.7368\n",
      "Epoch 827/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.5648 - val_loss: 0.7368\n",
      "Epoch 828/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.5648 - val_loss: 0.7368\n",
      "Epoch 829/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5648 - val_loss: 0.7368\n",
      "Epoch 830/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5648 - val_loss: 0.7368\n",
      "Epoch 831/1000\n",
      "161/161 [==============================] - 0s 857us/step - loss: 0.5648 - val_loss: 0.7368\n",
      "Epoch 832/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5647 - val_loss: 0.7368\n",
      "Epoch 833/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.5647 - val_loss: 0.7368\n",
      "Epoch 834/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5647 - val_loss: 0.7368\n",
      "Epoch 835/1000\n",
      "161/161 [==============================] - 0s 777us/step - loss: 0.5647 - val_loss: 0.7368\n",
      "Epoch 836/1000\n",
      "161/161 [==============================] - 0s 854us/step - loss: 0.5647 - val_loss: 0.7368\n",
      "Epoch 837/1000\n",
      "161/161 [==============================] - 0s 832us/step - loss: 0.5647 - val_loss: 0.7368\n",
      "Epoch 838/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5647 - val_loss: 0.7368\n",
      "Epoch 839/1000\n",
      "161/161 [==============================] - 0s 783us/step - loss: 0.5647 - val_loss: 0.7368\n",
      "Epoch 840/1000\n",
      "161/161 [==============================] - 0s 793us/step - loss: 0.5646 - val_loss: 0.7368\n",
      "Epoch 841/1000\n",
      "161/161 [==============================] - 0s 930us/step - loss: 0.5646 - val_loss: 0.7368\n",
      "Epoch 842/1000\n",
      "161/161 [==============================] - 0s 783us/step - loss: 0.5646 - val_loss: 0.7368\n",
      "Epoch 843/1000\n",
      "161/161 [==============================] - 0s 785us/step - loss: 0.5646 - val_loss: 0.7368\n",
      "Epoch 844/1000\n",
      "161/161 [==============================] - 0s 787us/step - loss: 0.5646 - val_loss: 0.7368\n",
      "Epoch 845/1000\n",
      "161/161 [==============================] - 0s 775us/step - loss: 0.5646 - val_loss: 0.7368\n",
      "Epoch 846/1000\n",
      "161/161 [==============================] - 0s 834us/step - loss: 0.5646 - val_loss: 0.7368\n",
      "Epoch 847/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5646 - val_loss: 0.7368\n",
      "Epoch 848/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5645 - val_loss: 0.7368\n",
      "Epoch 849/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5645 - val_loss: 0.7368\n",
      "Epoch 850/1000\n",
      "161/161 [==============================] - 0s 792us/step - loss: 0.5645 - val_loss: 0.7368\n",
      "Epoch 851/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5645 - val_loss: 0.7368\n",
      "Epoch 852/1000\n",
      "161/161 [==============================] - 0s 844us/step - loss: 0.5645 - val_loss: 0.7368\n",
      "Epoch 853/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5645 - val_loss: 0.7368\n",
      "Epoch 854/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.5645 - val_loss: 0.7368\n",
      "Epoch 855/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5645 - val_loss: 0.7368\n",
      "Epoch 856/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5644 - val_loss: 0.7368\n",
      "Epoch 857/1000\n",
      "161/161 [==============================] - 0s 867us/step - loss: 0.5644 - val_loss: 0.7368\n",
      "Epoch 858/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5644 - val_loss: 0.7368\n",
      "Epoch 859/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5644 - val_loss: 0.7368\n",
      "Epoch 860/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5644 - val_loss: 0.7368\n",
      "Epoch 861/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5644 - val_loss: 0.7367\n",
      "Epoch 862/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5644 - val_loss: 0.7367\n",
      "Epoch 863/1000\n",
      "161/161 [==============================] - 0s 839us/step - loss: 0.5644 - val_loss: 0.7367\n",
      "Epoch 864/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5643 - val_loss: 0.7367\n",
      "Epoch 865/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5643 - val_loss: 0.7367\n",
      "Epoch 866/1000\n",
      "161/161 [==============================] - 0s 796us/step - loss: 0.5643 - val_loss: 0.7367\n",
      "Epoch 867/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5643 - val_loss: 0.7367\n",
      "Epoch 868/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5643 - val_loss: 0.7367\n",
      "Epoch 869/1000\n",
      "161/161 [==============================] - 0s 840us/step - loss: 0.5643 - val_loss: 0.7367\n",
      "Epoch 870/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5643 - val_loss: 0.7367\n",
      "Epoch 871/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5642 - val_loss: 0.7367\n",
      "Epoch 872/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5642 - val_loss: 0.7367\n",
      "Epoch 873/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.5642 - val_loss: 0.7367\n",
      "Epoch 874/1000\n",
      "161/161 [==============================] - 0s 901us/step - loss: 0.5642 - val_loss: 0.7367\n",
      "Epoch 875/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5642 - val_loss: 0.7367\n",
      "Epoch 876/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5642 - val_loss: 0.7367\n",
      "Epoch 877/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5642 - val_loss: 0.7367\n",
      "Epoch 878/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.5642 - val_loss: 0.7367\n",
      "Epoch 879/1000\n",
      "161/161 [==============================] - 0s 839us/step - loss: 0.5641 - val_loss: 0.7367\n",
      "Epoch 880/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5641 - val_loss: 0.7367\n",
      "Epoch 881/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5641 - val_loss: 0.7366\n",
      "Epoch 882/1000\n",
      "161/161 [==============================] - 0s 792us/step - loss: 0.5641 - val_loss: 0.7366\n",
      "Epoch 883/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5641 - val_loss: 0.7366\n",
      "Epoch 884/1000\n",
      "161/161 [==============================] - 0s 840us/step - loss: 0.5641 - val_loss: 0.7366\n",
      "Epoch 885/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.5641 - val_loss: 0.7366\n",
      "Epoch 886/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.5640 - val_loss: 0.7366\n",
      "Epoch 887/1000\n",
      "161/161 [==============================] - 0s 777us/step - loss: 0.5640 - val_loss: 0.7366\n",
      "Epoch 888/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5640 - val_loss: 0.7366\n",
      "Epoch 889/1000\n",
      "161/161 [==============================] - 0s 850us/step - loss: 0.5640 - val_loss: 0.7366\n",
      "Epoch 890/1000\n",
      "161/161 [==============================] - 0s 795us/step - loss: 0.5640 - val_loss: 0.7366\n",
      "Epoch 891/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5640 - val_loss: 0.7366\n",
      "Epoch 892/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5640 - val_loss: 0.7366\n",
      "Epoch 893/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5639 - val_loss: 0.7366\n",
      "Epoch 894/1000\n",
      "161/161 [==============================] - 0s 840us/step - loss: 0.5639 - val_loss: 0.7366\n",
      "Epoch 895/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5639 - val_loss: 0.7366\n",
      "Epoch 896/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5639 - val_loss: 0.7365\n",
      "Epoch 897/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5639 - val_loss: 0.7365\n",
      "Epoch 898/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5639 - val_loss: 0.7365\n",
      "Epoch 899/1000\n",
      "161/161 [==============================] - 0s 870us/step - loss: 0.5639 - val_loss: 0.7365\n",
      "Epoch 900/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.5638 - val_loss: 0.7365\n",
      "Epoch 901/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5638 - val_loss: 0.7365\n",
      "Epoch 902/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5638 - val_loss: 0.7365\n",
      "Epoch 903/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5638 - val_loss: 0.7365\n",
      "Epoch 904/1000\n",
      "161/161 [==============================] - 0s 835us/step - loss: 0.5638 - val_loss: 0.7365\n",
      "Epoch 905/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.5638 - val_loss: 0.7365\n",
      "Epoch 906/1000\n",
      "161/161 [==============================] - 0s 787us/step - loss: 0.5638 - val_loss: 0.7365\n",
      "Epoch 907/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5637 - val_loss: 0.7365\n",
      "Epoch 908/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5637 - val_loss: 0.7364\n",
      "Epoch 909/1000\n",
      "161/161 [==============================] - 0s 824us/step - loss: 0.5637 - val_loss: 0.7364\n",
      "Epoch 910/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5637 - val_loss: 0.7364\n",
      "Epoch 911/1000\n",
      "161/161 [==============================] - 0s 778us/step - loss: 0.5637 - val_loss: 0.7364\n",
      "Epoch 912/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.5637 - val_loss: 0.7364\n",
      "Epoch 913/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5636 - val_loss: 0.7364\n",
      "Epoch 914/1000\n",
      "161/161 [==============================] - 0s 866us/step - loss: 0.5636 - val_loss: 0.7364\n",
      "Epoch 915/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5636 - val_loss: 0.7364\n",
      "Epoch 916/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5636 - val_loss: 0.7364\n",
      "Epoch 917/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5636 - val_loss: 0.7364\n",
      "Epoch 918/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5636 - val_loss: 0.7364\n",
      "Epoch 919/1000\n",
      "161/161 [==============================] - 0s 839us/step - loss: 0.5636 - val_loss: 0.7363\n",
      "Epoch 920/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5635 - val_loss: 0.7363\n",
      "Epoch 921/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5635 - val_loss: 0.7363\n",
      "Epoch 922/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.5635 - val_loss: 0.7363\n",
      "Epoch 923/1000\n",
      "161/161 [==============================] - 0s 790us/step - loss: 0.5635 - val_loss: 0.7363\n",
      "Epoch 924/1000\n",
      "161/161 [==============================] - 0s 841us/step - loss: 0.5635 - val_loss: 0.7363\n",
      "Epoch 925/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5635 - val_loss: 0.7363\n",
      "Epoch 926/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5635 - val_loss: 0.7363\n",
      "Epoch 927/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5634 - val_loss: 0.7363\n",
      "Epoch 928/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5634 - val_loss: 0.7363\n",
      "Epoch 929/1000\n",
      "161/161 [==============================] - 0s 842us/step - loss: 0.5634 - val_loss: 0.7362\n",
      "Epoch 930/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.5634 - val_loss: 0.7362\n",
      "Epoch 931/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.5634 - val_loss: 0.7362\n",
      "Epoch 932/1000\n",
      "161/161 [==============================] - 0s 791us/step - loss: 0.5634 - val_loss: 0.7362\n",
      "Epoch 933/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5633 - val_loss: 0.7362\n",
      "Epoch 934/1000\n",
      "161/161 [==============================] - 0s 844us/step - loss: 0.5633 - val_loss: 0.7362\n",
      "Epoch 935/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5633 - val_loss: 0.7362\n",
      "Epoch 936/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5633 - val_loss: 0.7362\n",
      "Epoch 937/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5633 - val_loss: 0.7362\n",
      "Epoch 938/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5633 - val_loss: 0.7361\n",
      "Epoch 939/1000\n",
      "161/161 [==============================] - 0s 838us/step - loss: 0.5632 - val_loss: 0.7361\n",
      "Epoch 940/1000\n",
      "161/161 [==============================] - 0s 795us/step - loss: 0.5632 - val_loss: 0.7361\n",
      "Epoch 941/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.5632 - val_loss: 0.7361\n",
      "Epoch 942/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5632 - val_loss: 0.7361\n",
      "Epoch 943/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5632 - val_loss: 0.7361\n",
      "Epoch 944/1000\n",
      "161/161 [==============================] - 0s 839us/step - loss: 0.5632 - val_loss: 0.7361\n",
      "Epoch 945/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5632 - val_loss: 0.7361\n",
      "Epoch 946/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5631 - val_loss: 0.7360\n",
      "Epoch 947/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.5631 - val_loss: 0.7360\n",
      "Epoch 948/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5631 - val_loss: 0.7360\n",
      "Epoch 949/1000\n",
      "161/161 [==============================] - 0s 846us/step - loss: 0.5631 - val_loss: 0.7360\n",
      "Epoch 950/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5631 - val_loss: 0.7360\n",
      "Epoch 951/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5631 - val_loss: 0.7360\n",
      "Epoch 952/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5630 - val_loss: 0.7360\n",
      "Epoch 953/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5630 - val_loss: 0.7360\n",
      "Epoch 954/1000\n",
      "161/161 [==============================] - 0s 868us/step - loss: 0.5630 - val_loss: 0.7360\n",
      "Epoch 955/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5630 - val_loss: 0.7359\n",
      "Epoch 956/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5630 - val_loss: 0.7359\n",
      "Epoch 957/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5630 - val_loss: 0.7359\n",
      "Epoch 958/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5629 - val_loss: 0.7359\n",
      "Epoch 959/1000\n",
      "161/161 [==============================] - 0s 835us/step - loss: 0.5629 - val_loss: 0.7359\n",
      "Epoch 960/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.5629 - val_loss: 0.7359\n",
      "Epoch 961/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5629 - val_loss: 0.7359\n",
      "Epoch 962/1000\n",
      "161/161 [==============================] - 0s 796us/step - loss: 0.5629 - val_loss: 0.7358\n",
      "Epoch 963/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5629 - val_loss: 0.7358\n",
      "Epoch 964/1000\n",
      "161/161 [==============================] - 0s 841us/step - loss: 0.5628 - val_loss: 0.7358\n",
      "Epoch 965/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5628 - val_loss: 0.7358\n",
      "Epoch 966/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.5628 - val_loss: 0.7358\n",
      "Epoch 967/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5628 - val_loss: 0.7358\n",
      "Epoch 968/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5628 - val_loss: 0.7358\n",
      "Epoch 969/1000\n",
      "161/161 [==============================] - 0s 836us/step - loss: 0.5628 - val_loss: 0.7358\n",
      "Epoch 970/1000\n",
      "161/161 [==============================] - 0s 797us/step - loss: 0.5627 - val_loss: 0.7357\n",
      "Epoch 971/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5627 - val_loss: 0.7357\n",
      "Epoch 972/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5627 - val_loss: 0.7357\n",
      "Epoch 973/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.5627 - val_loss: 0.7357\n",
      "Epoch 974/1000\n",
      "161/161 [==============================] - 0s 820us/step - loss: 0.5627 - val_loss: 0.7357\n",
      "Epoch 975/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5626 - val_loss: 0.7357\n",
      "Epoch 976/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.5626 - val_loss: 0.7356\n",
      "Epoch 977/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5626 - val_loss: 0.7356\n",
      "Epoch 978/1000\n",
      "161/161 [==============================] - 0s 789us/step - loss: 0.5626 - val_loss: 0.7356\n",
      "Epoch 979/1000\n",
      "161/161 [==============================] - 0s 839us/step - loss: 0.5626 - val_loss: 0.7356\n",
      "Epoch 980/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5626 - val_loss: 0.7356\n",
      "Epoch 981/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5625 - val_loss: 0.7356\n",
      "Epoch 982/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5625 - val_loss: 0.7356\n",
      "Epoch 983/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5625 - val_loss: 0.7355\n",
      "Epoch 984/1000\n",
      "161/161 [==============================] - 0s 847us/step - loss: 0.5625 - val_loss: 0.7355\n",
      "Epoch 985/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5625 - val_loss: 0.7355\n",
      "Epoch 986/1000\n",
      "161/161 [==============================] - 0s 793us/step - loss: 0.5624 - val_loss: 0.7355\n",
      "Epoch 987/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.5624 - val_loss: 0.7355\n",
      "Epoch 988/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5624 - val_loss: 0.7355\n",
      "Epoch 989/1000\n",
      "161/161 [==============================] - 0s 853us/step - loss: 0.5624 - val_loss: 0.7354\n",
      "Epoch 990/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5624 - val_loss: 0.7354\n",
      "Epoch 991/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5624 - val_loss: 0.7354\n",
      "Epoch 992/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5623 - val_loss: 0.7354\n",
      "Epoch 993/1000\n",
      "161/161 [==============================] - 0s 804us/step - loss: 0.5623 - val_loss: 0.7354\n",
      "Epoch 994/1000\n",
      "161/161 [==============================] - 0s 836us/step - loss: 0.5623 - val_loss: 0.7354\n",
      "Epoch 995/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5623 - val_loss: 0.7353\n",
      "Epoch 996/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5623 - val_loss: 0.7353\n",
      "Epoch 997/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5622 - val_loss: 0.7353\n",
      "Epoch 998/1000\n",
      "161/161 [==============================] - 0s 818us/step - loss: 0.5622 - val_loss: 0.7353\n",
      "Epoch 999/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.5622 - val_loss: 0.7353\n",
      "Epoch 1000/1000\n",
      "161/161 [==============================] - 0s 783us/step - loss: 0.5622 - val_loss: 0.7353\n"
     ]
    }
   ],
   "source": [
    "epocas = 1000\n",
    "history= modelo1.fit(\n",
    "    x = x_train,\n",
    "    y = y_train,\n",
    "    batch_size = lote,\n",
    "    epochs = epocas,\n",
    "    shuffle = False,\n",
    "    validation_data = (x_val,y_val),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2f503d110>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pérdida en Entrenamiento\n",
      "161/161 [==============================] - 0s 418us/step - loss: 0.5513\n",
      "Pérdida en Validación\n",
      "53/53 [==============================] - 0s 428us/step - loss: 0.7351\n",
      "Pérdida en Prueba\n",
      "52/52 [==============================] - 0s 411us/step - loss: 0.5214\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5213595032691956"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Pérdida en Entrenamiento\")\n",
    "modelo1.evaluate(\n",
    "    x = x_train,\n",
    "    y = y_train,\n",
    "    batch_size=1\n",
    ")\n",
    "print(\"Pérdida en Validación\")\n",
    "modelo1.evaluate(\n",
    "    x = x_val,\n",
    "    y = y_val,\n",
    "    batch_size=1\n",
    ")\n",
    "print(\"Pérdida en Prueba\")\n",
    "modelo1.evaluate(\n",
    "    x = x_test,\n",
    "    y = y_test,\n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_29 (LSTM)              (1, 1, 1)                 12        \n",
      "                                                                 \n",
      " lstm_30 (LSTM)              (1, 1, 1)                 12        \n",
      "                                                                 \n",
      " lstm_31 (LSTM)              (1, 1, 1)                 12        \n",
      "                                                                 \n",
      " lstm_32 (LSTM)              (1, 1)                    12        \n",
      "                                                                 \n",
      " dense_10 (Dense)            (1, 1)                    2         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50 (200.00 Byte)\n",
      "Trainable params: 50 (200.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelo2 = Sequential()\n",
    "\n",
    "lote = 1\n",
    "paso = 1\n",
    "caracteristicas = 1\n",
    "\n",
    "modelo2.add(LSTM(lote, batch_input_shape=(lote, paso, caracteristicas), stateful=True, return_sequences=True))\n",
    "modelo2.add(LSTM(lote, return_sequences=True, stateful=True))\n",
    "modelo2.add(LSTM(lote, return_sequences=True, stateful=True))\n",
    "modelo2.add(LSTM(lote, stateful=True))\n",
    "modelo2.add(Dense(1))\n",
    "modelo2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo2.compile(loss='mean_squared_error',optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "161/161 [==============================] - 3s 6ms/step - loss: 1.0125 - val_loss: 1.2002\n",
      "Epoch 2/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 1.0125 - val_loss: 1.2002\n",
      "Epoch 3/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 1.0125 - val_loss: 1.2002\n",
      "Epoch 4/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 1.0124 - val_loss: 1.2002\n",
      "Epoch 5/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 1.0124 - val_loss: 1.2001\n",
      "Epoch 6/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 1.0124 - val_loss: 1.2001\n",
      "Epoch 7/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 1.0124 - val_loss: 1.2000\n",
      "Epoch 8/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 1.0123 - val_loss: 1.2000\n",
      "Epoch 9/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 1.0122 - val_loss: 1.1998\n",
      "Epoch 10/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 1.0120 - val_loss: 1.1995\n",
      "Epoch 11/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 1.0117 - val_loss: 1.1990\n",
      "Epoch 12/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 1.0111 - val_loss: 1.1979\n",
      "Epoch 13/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 1.0099 - val_loss: 1.1958\n",
      "Epoch 14/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 1.0074 - val_loss: 1.1914\n",
      "Epoch 15/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 1.0028 - val_loss: 1.1833\n",
      "Epoch 16/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.9947 - val_loss: 1.1695\n",
      "Epoch 17/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.9823 - val_loss: 1.1491\n",
      "Epoch 18/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.9649 - val_loss: 1.1218\n",
      "Epoch 19/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.9428 - val_loss: 1.0889\n",
      "Epoch 20/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.9170 - val_loss: 1.0527\n",
      "Epoch 21/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.8894 - val_loss: 1.0163\n",
      "Epoch 22/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.8622 - val_loss: 0.9827\n",
      "Epoch 23/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.8372 - val_loss: 0.9540\n",
      "Epoch 24/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.8157 - val_loss: 0.9310\n",
      "Epoch 25/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7981 - val_loss: 0.9133\n",
      "Epoch 26/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7841 - val_loss: 0.9003\n",
      "Epoch 27/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7732 - val_loss: 0.8909\n",
      "Epoch 28/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7646 - val_loss: 0.8842\n",
      "Epoch 29/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7579 - val_loss: 0.8796\n",
      "Epoch 30/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7524 - val_loss: 0.8765\n",
      "Epoch 31/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7480 - val_loss: 0.8744\n",
      "Epoch 32/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7443 - val_loss: 0.8732\n",
      "Epoch 33/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7411 - val_loss: 0.8725\n",
      "Epoch 34/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7382 - val_loss: 0.8723\n",
      "Epoch 35/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7357 - val_loss: 0.8723\n",
      "Epoch 36/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7333 - val_loss: 0.8726\n",
      "Epoch 37/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7311 - val_loss: 0.8731\n",
      "Epoch 38/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7290 - val_loss: 0.8737\n",
      "Epoch 39/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7269 - val_loss: 0.8744\n",
      "Epoch 40/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7250 - val_loss: 0.8753\n",
      "Epoch 41/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7231 - val_loss: 0.8762\n",
      "Epoch 42/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7212 - val_loss: 0.8772\n",
      "Epoch 43/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7193 - val_loss: 0.8782\n",
      "Epoch 44/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7175 - val_loss: 0.8794\n",
      "Epoch 45/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7157 - val_loss: 0.8806\n",
      "Epoch 46/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7139 - val_loss: 0.8819\n",
      "Epoch 47/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7122 - val_loss: 0.8832\n",
      "Epoch 48/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7104 - val_loss: 0.8846\n",
      "Epoch 49/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7087 - val_loss: 0.8861\n",
      "Epoch 50/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7070 - val_loss: 0.8877\n",
      "Epoch 51/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7054 - val_loss: 0.8893\n",
      "Epoch 52/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7037 - val_loss: 0.8910\n",
      "Epoch 53/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7021 - val_loss: 0.8927\n",
      "Epoch 54/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7005 - val_loss: 0.8945\n",
      "Epoch 55/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6989 - val_loss: 0.8963\n",
      "Epoch 56/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6974 - val_loss: 0.8982\n",
      "Epoch 57/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6959 - val_loss: 0.9001\n",
      "Epoch 58/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6944 - val_loss: 0.9020\n",
      "Epoch 59/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6930 - val_loss: 0.9039\n",
      "Epoch 60/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6916 - val_loss: 0.9058\n",
      "Epoch 61/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6903 - val_loss: 0.9077\n",
      "Epoch 62/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6889 - val_loss: 0.9096\n",
      "Epoch 63/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6877 - val_loss: 0.9115\n",
      "Epoch 64/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6864 - val_loss: 0.9133\n",
      "Epoch 65/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6852 - val_loss: 0.9151\n",
      "Epoch 66/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6840 - val_loss: 0.9169\n",
      "Epoch 67/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6829 - val_loss: 0.9186\n",
      "Epoch 68/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6818 - val_loss: 0.9203\n",
      "Epoch 69/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6808 - val_loss: 0.9219\n",
      "Epoch 70/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6798 - val_loss: 0.9234\n",
      "Epoch 71/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6788 - val_loss: 0.9249\n",
      "Epoch 72/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6779 - val_loss: 0.9264\n",
      "Epoch 73/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6770 - val_loss: 0.9277\n",
      "Epoch 74/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6762 - val_loss: 0.9291\n",
      "Epoch 75/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6754 - val_loss: 0.9303\n",
      "Epoch 76/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6746 - val_loss: 0.9315\n",
      "Epoch 77/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6739 - val_loss: 0.9327\n",
      "Epoch 78/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6732 - val_loss: 0.9337\n",
      "Epoch 79/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6726 - val_loss: 0.9347\n",
      "Epoch 80/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6720 - val_loss: 0.9357\n",
      "Epoch 81/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6715 - val_loss: 0.9365\n",
      "Epoch 82/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6709 - val_loss: 0.9373\n",
      "Epoch 83/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6704 - val_loss: 0.9380\n",
      "Epoch 84/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6700 - val_loss: 0.9386\n",
      "Epoch 85/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6696 - val_loss: 0.9391\n",
      "Epoch 86/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6692 - val_loss: 0.9395\n",
      "Epoch 87/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6688 - val_loss: 0.9398\n",
      "Epoch 88/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6685 - val_loss: 0.9399\n",
      "Epoch 89/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6682 - val_loss: 0.9400\n",
      "Epoch 90/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6679 - val_loss: 0.9399\n",
      "Epoch 91/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6677 - val_loss: 0.9397\n",
      "Epoch 92/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6675 - val_loss: 0.9394\n",
      "Epoch 93/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6673 - val_loss: 0.9390\n",
      "Epoch 94/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6671 - val_loss: 0.9384\n",
      "Epoch 95/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6670 - val_loss: 0.9378\n",
      "Epoch 96/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6668 - val_loss: 0.9369\n",
      "Epoch 97/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6667 - val_loss: 0.9360\n",
      "Epoch 98/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6666 - val_loss: 0.9349\n",
      "Epoch 99/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6665 - val_loss: 0.9338\n",
      "Epoch 100/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6664 - val_loss: 0.9325\n",
      "Epoch 101/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6663 - val_loss: 0.9311\n",
      "Epoch 102/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6662 - val_loss: 0.9296\n",
      "Epoch 103/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6661 - val_loss: 0.9280\n",
      "Epoch 104/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6660 - val_loss: 0.9263\n",
      "Epoch 105/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6658 - val_loss: 0.9245\n",
      "Epoch 106/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6657 - val_loss: 0.9227\n",
      "Epoch 107/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6656 - val_loss: 0.9208\n",
      "Epoch 108/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6654 - val_loss: 0.9188\n",
      "Epoch 109/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6653 - val_loss: 0.9168\n",
      "Epoch 110/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6651 - val_loss: 0.9147\n",
      "Epoch 111/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6649 - val_loss: 0.9126\n",
      "Epoch 112/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6647 - val_loss: 0.9104\n",
      "Epoch 113/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6644 - val_loss: 0.9083\n",
      "Epoch 114/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6642 - val_loss: 0.9061\n",
      "Epoch 115/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6639 - val_loss: 0.9039\n",
      "Epoch 116/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6636 - val_loss: 0.9016\n",
      "Epoch 117/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6633 - val_loss: 0.8994\n",
      "Epoch 118/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6629 - val_loss: 0.8972\n",
      "Epoch 119/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6626 - val_loss: 0.8949\n",
      "Epoch 120/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6622 - val_loss: 0.8927\n",
      "Epoch 121/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6618 - val_loss: 0.8905\n",
      "Epoch 122/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6613 - val_loss: 0.8882\n",
      "Epoch 123/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6609 - val_loss: 0.8860\n",
      "Epoch 124/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6604 - val_loss: 0.8838\n",
      "Epoch 125/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6599 - val_loss: 0.8817\n",
      "Epoch 126/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6594 - val_loss: 0.8795\n",
      "Epoch 127/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6588 - val_loss: 0.8774\n",
      "Epoch 128/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6583 - val_loss: 0.8752\n",
      "Epoch 129/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6577 - val_loss: 0.8731\n",
      "Epoch 130/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6571 - val_loss: 0.8710\n",
      "Epoch 131/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6565 - val_loss: 0.8690\n",
      "Epoch 132/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6558 - val_loss: 0.8669\n",
      "Epoch 133/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6552 - val_loss: 0.8649\n",
      "Epoch 134/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6545 - val_loss: 0.8629\n",
      "Epoch 135/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6538 - val_loss: 0.8609\n",
      "Epoch 136/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6531 - val_loss: 0.8590\n",
      "Epoch 137/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6523 - val_loss: 0.8570\n",
      "Epoch 138/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6516 - val_loss: 0.8551\n",
      "Epoch 139/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6508 - val_loss: 0.8532\n",
      "Epoch 140/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6500 - val_loss: 0.8513\n",
      "Epoch 141/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6492 - val_loss: 0.8495\n",
      "Epoch 142/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6484 - val_loss: 0.8476\n",
      "Epoch 143/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6475 - val_loss: 0.8458\n",
      "Epoch 144/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6467 - val_loss: 0.8440\n",
      "Epoch 145/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6458 - val_loss: 0.8423\n",
      "Epoch 146/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6449 - val_loss: 0.8405\n",
      "Epoch 147/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6440 - val_loss: 0.8388\n",
      "Epoch 148/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6431 - val_loss: 0.8371\n",
      "Epoch 149/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6422 - val_loss: 0.8354\n",
      "Epoch 150/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6412 - val_loss: 0.8337\n",
      "Epoch 151/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6403 - val_loss: 0.8321\n",
      "Epoch 152/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6393 - val_loss: 0.8305\n",
      "Epoch 153/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6383 - val_loss: 0.8289\n",
      "Epoch 154/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6374 - val_loss: 0.8273\n",
      "Epoch 155/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.8258\n",
      "Epoch 156/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6354 - val_loss: 0.8242\n",
      "Epoch 157/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6344 - val_loss: 0.8227\n",
      "Epoch 158/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6334 - val_loss: 0.8212\n",
      "Epoch 159/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6324 - val_loss: 0.8198\n",
      "Epoch 160/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6314 - val_loss: 0.8184\n",
      "Epoch 161/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6304 - val_loss: 0.8169\n",
      "Epoch 162/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6294 - val_loss: 0.8156\n",
      "Epoch 163/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6284 - val_loss: 0.8142\n",
      "Epoch 164/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6275 - val_loss: 0.8129\n",
      "Epoch 165/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6265 - val_loss: 0.8116\n",
      "Epoch 166/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6255 - val_loss: 0.8103\n",
      "Epoch 167/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6246 - val_loss: 0.8091\n",
      "Epoch 168/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6236 - val_loss: 0.8079\n",
      "Epoch 169/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6227 - val_loss: 0.8067\n",
      "Epoch 170/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6218 - val_loss: 0.8055\n",
      "Epoch 171/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6208 - val_loss: 0.8044\n",
      "Epoch 172/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6199 - val_loss: 0.8033\n",
      "Epoch 173/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6191 - val_loss: 0.8022\n",
      "Epoch 174/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6182 - val_loss: 0.8012\n",
      "Epoch 175/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6173 - val_loss: 0.8001\n",
      "Epoch 176/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6165 - val_loss: 0.7992\n",
      "Epoch 177/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6157 - val_loss: 0.7982\n",
      "Epoch 178/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6149 - val_loss: 0.7973\n",
      "Epoch 179/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6141 - val_loss: 0.7964\n",
      "Epoch 180/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6133 - val_loss: 0.7955\n",
      "Epoch 181/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6126 - val_loss: 0.7946\n",
      "Epoch 182/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6118 - val_loss: 0.7938\n",
      "Epoch 183/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6111 - val_loss: 0.7930\n",
      "Epoch 184/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6104 - val_loss: 0.7922\n",
      "Epoch 185/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6097 - val_loss: 0.7915\n",
      "Epoch 186/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6091 - val_loss: 0.7907\n",
      "Epoch 187/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6084 - val_loss: 0.7900\n",
      "Epoch 188/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6078 - val_loss: 0.7893\n",
      "Epoch 189/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6072 - val_loss: 0.7887\n",
      "Epoch 190/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6066 - val_loss: 0.7880\n",
      "Epoch 191/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6060 - val_loss: 0.7874\n",
      "Epoch 192/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6054 - val_loss: 0.7868\n",
      "Epoch 193/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6049 - val_loss: 0.7862\n",
      "Epoch 194/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6044 - val_loss: 0.7857\n",
      "Epoch 195/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6038 - val_loss: 0.7851\n",
      "Epoch 196/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6033 - val_loss: 0.7846\n",
      "Epoch 197/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6028 - val_loss: 0.7841\n",
      "Epoch 198/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6024 - val_loss: 0.7836\n",
      "Epoch 199/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6019 - val_loss: 0.7831\n",
      "Epoch 200/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6014 - val_loss: 0.7826\n",
      "Epoch 201/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6010 - val_loss: 0.7822\n",
      "Epoch 202/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6006 - val_loss: 0.7817\n",
      "Epoch 203/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6002 - val_loss: 0.7813\n",
      "Epoch 204/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5998 - val_loss: 0.7809\n",
      "Epoch 205/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5994 - val_loss: 0.7805\n",
      "Epoch 206/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5990 - val_loss: 0.7801\n",
      "Epoch 207/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5986 - val_loss: 0.7797\n",
      "Epoch 208/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5983 - val_loss: 0.7794\n",
      "Epoch 209/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5979 - val_loss: 0.7790\n",
      "Epoch 210/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5976 - val_loss: 0.7787\n",
      "Epoch 211/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5972 - val_loss: 0.7783\n",
      "Epoch 212/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5969 - val_loss: 0.7780\n",
      "Epoch 213/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5966 - val_loss: 0.7777\n",
      "Epoch 214/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5963 - val_loss: 0.7774\n",
      "Epoch 215/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5960 - val_loss: 0.7771\n",
      "Epoch 216/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5957 - val_loss: 0.7768\n",
      "Epoch 217/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5954 - val_loss: 0.7765\n",
      "Epoch 218/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5952 - val_loss: 0.7763\n",
      "Epoch 219/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5949 - val_loss: 0.7760\n",
      "Epoch 220/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5946 - val_loss: 0.7758\n",
      "Epoch 221/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5944 - val_loss: 0.7755\n",
      "Epoch 222/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5941 - val_loss: 0.7753\n",
      "Epoch 223/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5939 - val_loss: 0.7750\n",
      "Epoch 224/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5937 - val_loss: 0.7748\n",
      "Epoch 225/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5935 - val_loss: 0.7746\n",
      "Epoch 226/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5932 - val_loss: 0.7744\n",
      "Epoch 227/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5930 - val_loss: 0.7742\n",
      "Epoch 228/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5928 - val_loss: 0.7740\n",
      "Epoch 229/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5926 - val_loss: 0.7738\n",
      "Epoch 230/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5924 - val_loss: 0.7736\n",
      "Epoch 231/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5922 - val_loss: 0.7734\n",
      "Epoch 232/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5920 - val_loss: 0.7733\n",
      "Epoch 233/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5919 - val_loss: 0.7731\n",
      "Epoch 234/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5917 - val_loss: 0.7730\n",
      "Epoch 235/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5915 - val_loss: 0.7728\n",
      "Epoch 236/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5913 - val_loss: 0.7726\n",
      "Epoch 237/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5912 - val_loss: 0.7725\n",
      "Epoch 238/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5910 - val_loss: 0.7724\n",
      "Epoch 239/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5909 - val_loss: 0.7722\n",
      "Epoch 240/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5907 - val_loss: 0.7721\n",
      "Epoch 241/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5906 - val_loss: 0.7720\n",
      "Epoch 242/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5904 - val_loss: 0.7719\n",
      "Epoch 243/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5903 - val_loss: 0.7717\n",
      "Epoch 244/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5901 - val_loss: 0.7716\n",
      "Epoch 245/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5900 - val_loss: 0.7715\n",
      "Epoch 246/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5899 - val_loss: 0.7714\n",
      "Epoch 247/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5898 - val_loss: 0.7713\n",
      "Epoch 248/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5896 - val_loss: 0.7712\n",
      "Epoch 249/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5895 - val_loss: 0.7711\n",
      "Epoch 250/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5894 - val_loss: 0.7711\n",
      "Epoch 251/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5893 - val_loss: 0.7710\n",
      "Epoch 252/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5892 - val_loss: 0.7709\n",
      "Epoch 253/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5891 - val_loss: 0.7708\n",
      "Epoch 254/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5890 - val_loss: 0.7708\n",
      "Epoch 255/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5889 - val_loss: 0.7707\n",
      "Epoch 256/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5888 - val_loss: 0.7706\n",
      "Epoch 257/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5887 - val_loss: 0.7706\n",
      "Epoch 258/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5886 - val_loss: 0.7705\n",
      "Epoch 259/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5885 - val_loss: 0.7705\n",
      "Epoch 260/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5884 - val_loss: 0.7704\n",
      "Epoch 261/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5883 - val_loss: 0.7704\n",
      "Epoch 262/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5883 - val_loss: 0.7703\n",
      "Epoch 263/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5882 - val_loss: 0.7703\n",
      "Epoch 264/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5881 - val_loss: 0.7703\n",
      "Epoch 265/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5880 - val_loss: 0.7702\n",
      "Epoch 266/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5880 - val_loss: 0.7702\n",
      "Epoch 267/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5879 - val_loss: 0.7702\n",
      "Epoch 268/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5878 - val_loss: 0.7701\n",
      "Epoch 269/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5877 - val_loss: 0.7701\n",
      "Epoch 270/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5877 - val_loss: 0.7701\n",
      "Epoch 271/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5876 - val_loss: 0.7701\n",
      "Epoch 272/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5876 - val_loss: 0.7701\n",
      "Epoch 273/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5875 - val_loss: 0.7701\n",
      "Epoch 274/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5874 - val_loss: 0.7701\n",
      "Epoch 275/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5874 - val_loss: 0.7700\n",
      "Epoch 276/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5873 - val_loss: 0.7700\n",
      "Epoch 277/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5873 - val_loss: 0.7700\n",
      "Epoch 278/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5872 - val_loss: 0.7700\n",
      "Epoch 279/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5872 - val_loss: 0.7701\n",
      "Epoch 280/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5871 - val_loss: 0.7701\n",
      "Epoch 281/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5871 - val_loss: 0.7701\n",
      "Epoch 282/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5870 - val_loss: 0.7701\n",
      "Epoch 283/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5870 - val_loss: 0.7701\n",
      "Epoch 284/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5870 - val_loss: 0.7701\n",
      "Epoch 285/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5869 - val_loss: 0.7701\n",
      "Epoch 286/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5869 - val_loss: 0.7702\n",
      "Epoch 287/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5869 - val_loss: 0.7702\n",
      "Epoch 288/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5868 - val_loss: 0.7702\n",
      "Epoch 289/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5868 - val_loss: 0.7702\n",
      "Epoch 290/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5867 - val_loss: 0.7703\n",
      "Epoch 291/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5867 - val_loss: 0.7703\n",
      "Epoch 292/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5867 - val_loss: 0.7703\n",
      "Epoch 293/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5867 - val_loss: 0.7704\n",
      "Epoch 294/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5866 - val_loss: 0.7704\n",
      "Epoch 295/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5866 - val_loss: 0.7704\n",
      "Epoch 296/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5866 - val_loss: 0.7705\n",
      "Epoch 297/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5866 - val_loss: 0.7705\n",
      "Epoch 298/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5865 - val_loss: 0.7706\n",
      "Epoch 299/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5865 - val_loss: 0.7706\n",
      "Epoch 300/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5865 - val_loss: 0.7707\n",
      "Epoch 301/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5865 - val_loss: 0.7707\n",
      "Epoch 302/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5864 - val_loss: 0.7708\n",
      "Epoch 303/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5864 - val_loss: 0.7708\n",
      "Epoch 304/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5864 - val_loss: 0.7709\n",
      "Epoch 305/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5864 - val_loss: 0.7709\n",
      "Epoch 306/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5864 - val_loss: 0.7710\n",
      "Epoch 307/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5864 - val_loss: 0.7710\n",
      "Epoch 308/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5864 - val_loss: 0.7711\n",
      "Epoch 309/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5863 - val_loss: 0.7712\n",
      "Epoch 310/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5863 - val_loss: 0.7712\n",
      "Epoch 311/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5863 - val_loss: 0.7713\n",
      "Epoch 312/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5863 - val_loss: 0.7714\n",
      "Epoch 313/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5863 - val_loss: 0.7714\n",
      "Epoch 314/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5863 - val_loss: 0.7715\n",
      "Epoch 315/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5863 - val_loss: 0.7716\n",
      "Epoch 316/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5863 - val_loss: 0.7716\n",
      "Epoch 317/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5863 - val_loss: 0.7717\n",
      "Epoch 318/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5863 - val_loss: 0.7718\n",
      "Epoch 319/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5863 - val_loss: 0.7719\n",
      "Epoch 320/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5862 - val_loss: 0.7720\n",
      "Epoch 321/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5862 - val_loss: 0.7720\n",
      "Epoch 322/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5862 - val_loss: 0.7721\n",
      "Epoch 323/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5862 - val_loss: 0.7722\n",
      "Epoch 324/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5862 - val_loss: 0.7723\n",
      "Epoch 325/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5862 - val_loss: 0.7724\n",
      "Epoch 326/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5862 - val_loss: 0.7725\n",
      "Epoch 327/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5862 - val_loss: 0.7726\n",
      "Epoch 328/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5862 - val_loss: 0.7726\n",
      "Epoch 329/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5862 - val_loss: 0.7727\n",
      "Epoch 330/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5862 - val_loss: 0.7728\n",
      "Epoch 331/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5862 - val_loss: 0.7729\n",
      "Epoch 332/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5862 - val_loss: 0.7730\n",
      "Epoch 333/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5862 - val_loss: 0.7731\n",
      "Epoch 334/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5862 - val_loss: 0.7732\n",
      "Epoch 335/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5862 - val_loss: 0.7733\n",
      "Epoch 336/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5863 - val_loss: 0.7734\n",
      "Epoch 337/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5863 - val_loss: 0.7735\n",
      "Epoch 338/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5863 - val_loss: 0.7736\n",
      "Epoch 339/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5863 - val_loss: 0.7737\n",
      "Epoch 340/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5863 - val_loss: 0.7738\n",
      "Epoch 341/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5863 - val_loss: 0.7739\n",
      "Epoch 342/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5863 - val_loss: 0.7740\n",
      "Epoch 343/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5863 - val_loss: 0.7742\n",
      "Epoch 344/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5863 - val_loss: 0.7743\n",
      "Epoch 345/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5863 - val_loss: 0.7744\n",
      "Epoch 346/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5863 - val_loss: 0.7745\n",
      "Epoch 347/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5863 - val_loss: 0.7746\n",
      "Epoch 348/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5863 - val_loss: 0.7747\n",
      "Epoch 349/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5864 - val_loss: 0.7748\n",
      "Epoch 350/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5864 - val_loss: 0.7750\n",
      "Epoch 351/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5864 - val_loss: 0.7751\n",
      "Epoch 352/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5864 - val_loss: 0.7752\n",
      "Epoch 353/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5864 - val_loss: 0.7753\n",
      "Epoch 354/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5864 - val_loss: 0.7754\n",
      "Epoch 355/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5864 - val_loss: 0.7756\n",
      "Epoch 356/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5864 - val_loss: 0.7757\n",
      "Epoch 357/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5864 - val_loss: 0.7758\n",
      "Epoch 358/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5865 - val_loss: 0.7759\n",
      "Epoch 359/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5865 - val_loss: 0.7761\n",
      "Epoch 360/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5865 - val_loss: 0.7762\n",
      "Epoch 361/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5865 - val_loss: 0.7763\n",
      "Epoch 362/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5865 - val_loss: 0.7764\n",
      "Epoch 363/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5865 - val_loss: 0.7766\n",
      "Epoch 364/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5865 - val_loss: 0.7767\n",
      "Epoch 365/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5866 - val_loss: 0.7768\n",
      "Epoch 366/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5866 - val_loss: 0.7770\n",
      "Epoch 367/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5866 - val_loss: 0.7771\n",
      "Epoch 368/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5866 - val_loss: 0.7772\n",
      "Epoch 369/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5866 - val_loss: 0.7774\n",
      "Epoch 370/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5866 - val_loss: 0.7775\n",
      "Epoch 371/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5867 - val_loss: 0.7777\n",
      "Epoch 372/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5867 - val_loss: 0.7778\n",
      "Epoch 373/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5867 - val_loss: 0.7779\n",
      "Epoch 374/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5867 - val_loss: 0.7781\n",
      "Epoch 375/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5867 - val_loss: 0.7782\n",
      "Epoch 376/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5868 - val_loss: 0.7784\n",
      "Epoch 377/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5868 - val_loss: 0.7785\n",
      "Epoch 378/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5868 - val_loss: 0.7786\n",
      "Epoch 379/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5868 - val_loss: 0.7788\n",
      "Epoch 380/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5868 - val_loss: 0.7789\n",
      "Epoch 381/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5869 - val_loss: 0.7791\n",
      "Epoch 382/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5869 - val_loss: 0.7792\n",
      "Epoch 383/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5869 - val_loss: 0.7794\n",
      "Epoch 384/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5869 - val_loss: 0.7795\n",
      "Epoch 385/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5869 - val_loss: 0.7797\n",
      "Epoch 386/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5870 - val_loss: 0.7798\n",
      "Epoch 387/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5870 - val_loss: 0.7800\n",
      "Epoch 388/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5870 - val_loss: 0.7802\n",
      "Epoch 389/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5870 - val_loss: 0.7803\n",
      "Epoch 390/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5871 - val_loss: 0.7805\n",
      "Epoch 391/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5871 - val_loss: 0.7806\n",
      "Epoch 392/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5871 - val_loss: 0.7808\n",
      "Epoch 393/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5871 - val_loss: 0.7809\n",
      "Epoch 394/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5872 - val_loss: 0.7811\n",
      "Epoch 395/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5872 - val_loss: 0.7813\n",
      "Epoch 396/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5872 - val_loss: 0.7814\n",
      "Epoch 397/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5872 - val_loss: 0.7816\n",
      "Epoch 398/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5873 - val_loss: 0.7818\n",
      "Epoch 399/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5873 - val_loss: 0.7819\n",
      "Epoch 400/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5873 - val_loss: 0.7821\n",
      "Epoch 401/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5873 - val_loss: 0.7822\n",
      "Epoch 402/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5874 - val_loss: 0.7824\n",
      "Epoch 403/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5874 - val_loss: 0.7826\n",
      "Epoch 404/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5874 - val_loss: 0.7828\n",
      "Epoch 405/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5875 - val_loss: 0.7829\n",
      "Epoch 406/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5875 - val_loss: 0.7831\n",
      "Epoch 407/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5875 - val_loss: 0.7833\n",
      "Epoch 408/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5875 - val_loss: 0.7834\n",
      "Epoch 409/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5876 - val_loss: 0.7836\n",
      "Epoch 410/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5876 - val_loss: 0.7838\n",
      "Epoch 411/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5876 - val_loss: 0.7840\n",
      "Epoch 412/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5877 - val_loss: 0.7841\n",
      "Epoch 413/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5877 - val_loss: 0.7843\n",
      "Epoch 414/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5877 - val_loss: 0.7845\n",
      "Epoch 415/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5878 - val_loss: 0.7847\n",
      "Epoch 416/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5878 - val_loss: 0.7848\n",
      "Epoch 417/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5878 - val_loss: 0.7850\n",
      "Epoch 418/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5879 - val_loss: 0.7852\n",
      "Epoch 419/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5879 - val_loss: 0.7854\n",
      "Epoch 420/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5879 - val_loss: 0.7856\n",
      "Epoch 421/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5880 - val_loss: 0.7858\n",
      "Epoch 422/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5880 - val_loss: 0.7859\n",
      "Epoch 423/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5880 - val_loss: 0.7861\n",
      "Epoch 424/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5881 - val_loss: 0.7863\n",
      "Epoch 425/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5881 - val_loss: 0.7865\n",
      "Epoch 426/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5881 - val_loss: 0.7867\n",
      "Epoch 427/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5882 - val_loss: 0.7869\n",
      "Epoch 428/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5882 - val_loss: 0.7871\n",
      "Epoch 429/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5882 - val_loss: 0.7873\n",
      "Epoch 430/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5883 - val_loss: 0.7874\n",
      "Epoch 431/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5883 - val_loss: 0.7876\n",
      "Epoch 432/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5884 - val_loss: 0.7878\n",
      "Epoch 433/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5884 - val_loss: 0.7880\n",
      "Epoch 434/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5884 - val_loss: 0.7882\n",
      "Epoch 435/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5885 - val_loss: 0.7884\n",
      "Epoch 436/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5885 - val_loss: 0.7886\n",
      "Epoch 437/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5886 - val_loss: 0.7888\n",
      "Epoch 438/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5886 - val_loss: 0.7890\n",
      "Epoch 439/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5886 - val_loss: 0.7892\n",
      "Epoch 440/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5887 - val_loss: 0.7894\n",
      "Epoch 441/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5887 - val_loss: 0.7896\n",
      "Epoch 442/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5888 - val_loss: 0.7898\n",
      "Epoch 443/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5888 - val_loss: 0.7900\n",
      "Epoch 444/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5889 - val_loss: 0.7902\n",
      "Epoch 445/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5889 - val_loss: 0.7904\n",
      "Epoch 446/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5890 - val_loss: 0.7906\n",
      "Epoch 447/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5890 - val_loss: 0.7908\n",
      "Epoch 448/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5890 - val_loss: 0.7910\n",
      "Epoch 449/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5891 - val_loss: 0.7912\n",
      "Epoch 450/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5891 - val_loss: 0.7914\n",
      "Epoch 451/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5892 - val_loss: 0.7916\n",
      "Epoch 452/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5892 - val_loss: 0.7918\n",
      "Epoch 453/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5893 - val_loss: 0.7921\n",
      "Epoch 454/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5893 - val_loss: 0.7923\n",
      "Epoch 455/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5894 - val_loss: 0.7925\n",
      "Epoch 456/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5894 - val_loss: 0.7927\n",
      "Epoch 457/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5895 - val_loss: 0.7929\n",
      "Epoch 458/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5895 - val_loss: 0.7931\n",
      "Epoch 459/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5896 - val_loss: 0.7933\n",
      "Epoch 460/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5896 - val_loss: 0.7935\n",
      "Epoch 461/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5897 - val_loss: 0.7938\n",
      "Epoch 462/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5898 - val_loss: 0.7940\n",
      "Epoch 463/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5898 - val_loss: 0.7942\n",
      "Epoch 464/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5899 - val_loss: 0.7944\n",
      "Epoch 465/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5899 - val_loss: 0.7946\n",
      "Epoch 466/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5900 - val_loss: 0.7948\n",
      "Epoch 467/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5900 - val_loss: 0.7951\n",
      "Epoch 468/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5901 - val_loss: 0.7953\n",
      "Epoch 469/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5901 - val_loss: 0.7955\n",
      "Epoch 470/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5902 - val_loss: 0.7957\n",
      "Epoch 471/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5903 - val_loss: 0.7959\n",
      "Epoch 472/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5903 - val_loss: 0.7962\n",
      "Epoch 473/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5904 - val_loss: 0.7964\n",
      "Epoch 474/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5904 - val_loss: 0.7966\n",
      "Epoch 475/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5905 - val_loss: 0.7968\n",
      "Epoch 476/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5906 - val_loss: 0.7971\n",
      "Epoch 477/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5906 - val_loss: 0.7973\n",
      "Epoch 478/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5907 - val_loss: 0.7975\n",
      "Epoch 479/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5907 - val_loss: 0.7977\n",
      "Epoch 480/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5908 - val_loss: 0.7980\n",
      "Epoch 481/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5909 - val_loss: 0.7982\n",
      "Epoch 482/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5909 - val_loss: 0.7984\n",
      "Epoch 483/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5910 - val_loss: 0.7987\n",
      "Epoch 484/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5911 - val_loss: 0.7989\n",
      "Epoch 485/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5911 - val_loss: 0.7991\n",
      "Epoch 486/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5912 - val_loss: 0.7994\n",
      "Epoch 487/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5913 - val_loss: 0.7996\n",
      "Epoch 488/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5913 - val_loss: 0.7998\n",
      "Epoch 489/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5914 - val_loss: 0.8001\n",
      "Epoch 490/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5915 - val_loss: 0.8003\n",
      "Epoch 491/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5916 - val_loss: 0.8005\n",
      "Epoch 492/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5916 - val_loss: 0.8008\n",
      "Epoch 493/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5917 - val_loss: 0.8010\n",
      "Epoch 494/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5918 - val_loss: 0.8013\n",
      "Epoch 495/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5918 - val_loss: 0.8015\n",
      "Epoch 496/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5919 - val_loss: 0.8017\n",
      "Epoch 497/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5920 - val_loss: 0.8020\n",
      "Epoch 498/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5921 - val_loss: 0.8022\n",
      "Epoch 499/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5921 - val_loss: 0.8025\n",
      "Epoch 500/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5922 - val_loss: 0.8027\n",
      "Epoch 501/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5923 - val_loss: 0.8029\n",
      "Epoch 502/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5924 - val_loss: 0.8032\n",
      "Epoch 503/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5924 - val_loss: 0.8034\n",
      "Epoch 504/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5925 - val_loss: 0.8037\n",
      "Epoch 505/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5926 - val_loss: 0.8039\n",
      "Epoch 506/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5927 - val_loss: 0.8042\n",
      "Epoch 507/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5928 - val_loss: 0.8044\n",
      "Epoch 508/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5928 - val_loss: 0.8047\n",
      "Epoch 509/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5929 - val_loss: 0.8049\n",
      "Epoch 510/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5930 - val_loss: 0.8052\n",
      "Epoch 511/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5931 - val_loss: 0.8054\n",
      "Epoch 512/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5932 - val_loss: 0.8057\n",
      "Epoch 513/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5933 - val_loss: 0.8059\n",
      "Epoch 514/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5933 - val_loss: 0.8062\n",
      "Epoch 515/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5934 - val_loss: 0.8064\n",
      "Epoch 516/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5935 - val_loss: 0.8067\n",
      "Epoch 517/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5936 - val_loss: 0.8069\n",
      "Epoch 518/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5937 - val_loss: 0.8072\n",
      "Epoch 519/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5938 - val_loss: 0.8074\n",
      "Epoch 520/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5939 - val_loss: 0.8077\n",
      "Epoch 521/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5939 - val_loss: 0.8079\n",
      "Epoch 522/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5940 - val_loss: 0.8082\n",
      "Epoch 523/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5941 - val_loss: 0.8085\n",
      "Epoch 524/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5942 - val_loss: 0.8087\n",
      "Epoch 525/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5943 - val_loss: 0.8090\n",
      "Epoch 526/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5944 - val_loss: 0.8092\n",
      "Epoch 527/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5945 - val_loss: 0.8095\n",
      "Epoch 528/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5946 - val_loss: 0.8098\n",
      "Epoch 529/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5947 - val_loss: 0.8100\n",
      "Epoch 530/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5948 - val_loss: 0.8103\n",
      "Epoch 531/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5949 - val_loss: 0.8105\n",
      "Epoch 532/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5950 - val_loss: 0.8108\n",
      "Epoch 533/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5950 - val_loss: 0.8111\n",
      "Epoch 534/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5951 - val_loss: 0.8113\n",
      "Epoch 535/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5952 - val_loss: 0.8116\n",
      "Epoch 536/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5953 - val_loss: 0.8119\n",
      "Epoch 537/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5954 - val_loss: 0.8121\n",
      "Epoch 538/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5955 - val_loss: 0.8124\n",
      "Epoch 539/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5956 - val_loss: 0.8127\n",
      "Epoch 540/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5957 - val_loss: 0.8129\n",
      "Epoch 541/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5958 - val_loss: 0.8132\n",
      "Epoch 542/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5959 - val_loss: 0.8135\n",
      "Epoch 543/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5960 - val_loss: 0.8138\n",
      "Epoch 544/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5961 - val_loss: 0.8140\n",
      "Epoch 545/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5962 - val_loss: 0.8143\n",
      "Epoch 546/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5963 - val_loss: 0.8146\n",
      "Epoch 547/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5964 - val_loss: 0.8148\n",
      "Epoch 548/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5965 - val_loss: 0.8151\n",
      "Epoch 549/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5966 - val_loss: 0.8154\n",
      "Epoch 550/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5967 - val_loss: 0.8157\n",
      "Epoch 551/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5968 - val_loss: 0.8160\n",
      "Epoch 552/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5969 - val_loss: 0.8162\n",
      "Epoch 553/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5970 - val_loss: 0.8165\n",
      "Epoch 554/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5971 - val_loss: 0.8168\n",
      "Epoch 555/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5972 - val_loss: 0.8171\n",
      "Epoch 556/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5973 - val_loss: 0.8173\n",
      "Epoch 557/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5974 - val_loss: 0.8176\n",
      "Epoch 558/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5975 - val_loss: 0.8179\n",
      "Epoch 559/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5976 - val_loss: 0.8182\n",
      "Epoch 560/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5977 - val_loss: 0.8185\n",
      "Epoch 561/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5978 - val_loss: 0.8188\n",
      "Epoch 562/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5979 - val_loss: 0.8190\n",
      "Epoch 563/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5980 - val_loss: 0.8193\n",
      "Epoch 564/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5981 - val_loss: 0.8196\n",
      "Epoch 565/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5982 - val_loss: 0.8199\n",
      "Epoch 566/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5983 - val_loss: 0.8202\n",
      "Epoch 567/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5984 - val_loss: 0.8205\n",
      "Epoch 568/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5986 - val_loss: 0.8208\n",
      "Epoch 569/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5987 - val_loss: 0.8211\n",
      "Epoch 570/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5988 - val_loss: 0.8214\n",
      "Epoch 571/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5989 - val_loss: 0.8217\n",
      "Epoch 572/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5990 - val_loss: 0.8219\n",
      "Epoch 573/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5991 - val_loss: 0.8222\n",
      "Epoch 574/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5992 - val_loss: 0.8225\n",
      "Epoch 575/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5993 - val_loss: 0.8228\n",
      "Epoch 576/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5994 - val_loss: 0.8231\n",
      "Epoch 577/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5995 - val_loss: 0.8234\n",
      "Epoch 578/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5996 - val_loss: 0.8237\n",
      "Epoch 579/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5997 - val_loss: 0.8240\n",
      "Epoch 580/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5998 - val_loss: 0.8243\n",
      "Epoch 581/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5999 - val_loss: 0.8246\n",
      "Epoch 582/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6000 - val_loss: 0.8249\n",
      "Epoch 583/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6001 - val_loss: 0.8252\n",
      "Epoch 584/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6002 - val_loss: 0.8255\n",
      "Epoch 585/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6003 - val_loss: 0.8258\n",
      "Epoch 586/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6004 - val_loss: 0.8262\n",
      "Epoch 587/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6005 - val_loss: 0.8265\n",
      "Epoch 588/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6006 - val_loss: 0.8268\n",
      "Epoch 589/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6007 - val_loss: 0.8271\n",
      "Epoch 590/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6008 - val_loss: 0.8274\n",
      "Epoch 591/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6009 - val_loss: 0.8277\n",
      "Epoch 592/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6010 - val_loss: 0.8280\n",
      "Epoch 593/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6011 - val_loss: 0.8283\n",
      "Epoch 594/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6012 - val_loss: 0.8286\n",
      "Epoch 595/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6013 - val_loss: 0.8290\n",
      "Epoch 596/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6014 - val_loss: 0.8293\n",
      "Epoch 597/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6015 - val_loss: 0.8296\n",
      "Epoch 598/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6016 - val_loss: 0.8299\n",
      "Epoch 599/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6017 - val_loss: 0.8302\n",
      "Epoch 600/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6018 - val_loss: 0.8305\n",
      "Epoch 601/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6019 - val_loss: 0.8309\n",
      "Epoch 602/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6020 - val_loss: 0.8312\n",
      "Epoch 603/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6021 - val_loss: 0.8315\n",
      "Epoch 604/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6022 - val_loss: 0.8318\n",
      "Epoch 605/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6023 - val_loss: 0.8322\n",
      "Epoch 606/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6024 - val_loss: 0.8325\n",
      "Epoch 607/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6025 - val_loss: 0.8328\n",
      "Epoch 608/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6026 - val_loss: 0.8331\n",
      "Epoch 609/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6027 - val_loss: 0.8335\n",
      "Epoch 610/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6028 - val_loss: 0.8338\n",
      "Epoch 611/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6029 - val_loss: 0.8341\n",
      "Epoch 612/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6030 - val_loss: 0.8345\n",
      "Epoch 613/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6031 - val_loss: 0.8348\n",
      "Epoch 614/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6032 - val_loss: 0.8351\n",
      "Epoch 615/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6032 - val_loss: 0.8355\n",
      "Epoch 616/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6033 - val_loss: 0.8358\n",
      "Epoch 617/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6034 - val_loss: 0.8362\n",
      "Epoch 618/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6035 - val_loss: 0.8365\n",
      "Epoch 619/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6036 - val_loss: 0.8368\n",
      "Epoch 620/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6037 - val_loss: 0.8372\n",
      "Epoch 621/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6038 - val_loss: 0.8375\n",
      "Epoch 622/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6039 - val_loss: 0.8379\n",
      "Epoch 623/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6040 - val_loss: 0.8382\n",
      "Epoch 624/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6041 - val_loss: 0.8386\n",
      "Epoch 625/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6041 - val_loss: 0.8389\n",
      "Epoch 626/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6042 - val_loss: 0.8393\n",
      "Epoch 627/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6043 - val_loss: 0.8396\n",
      "Epoch 628/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6044 - val_loss: 0.8400\n",
      "Epoch 629/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6045 - val_loss: 0.8403\n",
      "Epoch 630/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6046 - val_loss: 0.8407\n",
      "Epoch 631/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6047 - val_loss: 0.8410\n",
      "Epoch 632/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6047 - val_loss: 0.8414\n",
      "Epoch 633/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6048 - val_loss: 0.8417\n",
      "Epoch 634/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6049 - val_loss: 0.8421\n",
      "Epoch 635/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6050 - val_loss: 0.8424\n",
      "Epoch 636/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6051 - val_loss: 0.8428\n",
      "Epoch 637/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6052 - val_loss: 0.8432\n",
      "Epoch 638/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6052 - val_loss: 0.8435\n",
      "Epoch 639/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6053 - val_loss: 0.8439\n",
      "Epoch 640/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6054 - val_loss: 0.8443\n",
      "Epoch 641/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6055 - val_loss: 0.8446\n",
      "Epoch 642/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6056 - val_loss: 0.8450\n",
      "Epoch 643/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6056 - val_loss: 0.8454\n",
      "Epoch 644/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6057 - val_loss: 0.8457\n",
      "Epoch 645/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6058 - val_loss: 0.8461\n",
      "Epoch 646/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6059 - val_loss: 0.8465\n",
      "Epoch 647/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6060 - val_loss: 0.8469\n",
      "Epoch 648/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6060 - val_loss: 0.8472\n",
      "Epoch 649/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6061 - val_loss: 0.8476\n",
      "Epoch 650/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6062 - val_loss: 0.8480\n",
      "Epoch 651/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6063 - val_loss: 0.8484\n",
      "Epoch 652/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6063 - val_loss: 0.8488\n",
      "Epoch 653/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6064 - val_loss: 0.8491\n",
      "Epoch 654/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6065 - val_loss: 0.8495\n",
      "Epoch 655/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6066 - val_loss: 0.8499\n",
      "Epoch 656/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6066 - val_loss: 0.8503\n",
      "Epoch 657/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6067 - val_loss: 0.8507\n",
      "Epoch 658/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6068 - val_loss: 0.8511\n",
      "Epoch 659/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6068 - val_loss: 0.8515\n",
      "Epoch 660/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6069 - val_loss: 0.8518\n",
      "Epoch 661/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6070 - val_loss: 0.8522\n",
      "Epoch 662/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6070 - val_loss: 0.8526\n",
      "Epoch 663/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6071 - val_loss: 0.8530\n",
      "Epoch 664/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6072 - val_loss: 0.8534\n",
      "Epoch 665/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6073 - val_loss: 0.8538\n",
      "Epoch 666/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6073 - val_loss: 0.8542\n",
      "Epoch 667/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6074 - val_loss: 0.8546\n",
      "Epoch 668/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6075 - val_loss: 0.8550\n",
      "Epoch 669/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6075 - val_loss: 0.8554\n",
      "Epoch 670/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6076 - val_loss: 0.8558\n",
      "Epoch 671/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6076 - val_loss: 0.8562\n",
      "Epoch 672/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6077 - val_loss: 0.8566\n",
      "Epoch 673/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6078 - val_loss: 0.8570\n",
      "Epoch 674/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6078 - val_loss: 0.8574\n",
      "Epoch 675/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6079 - val_loss: 0.8579\n",
      "Epoch 676/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6080 - val_loss: 0.8583\n",
      "Epoch 677/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6080 - val_loss: 0.8587\n",
      "Epoch 678/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6081 - val_loss: 0.8591\n",
      "Epoch 679/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6081 - val_loss: 0.8595\n",
      "Epoch 680/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6082 - val_loss: 0.8599\n",
      "Epoch 681/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6083 - val_loss: 0.8603\n",
      "Epoch 682/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6083 - val_loss: 0.8608\n",
      "Epoch 683/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6084 - val_loss: 0.8612\n",
      "Epoch 684/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6084 - val_loss: 0.8616\n",
      "Epoch 685/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6085 - val_loss: 0.8620\n",
      "Epoch 686/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6086 - val_loss: 0.8625\n",
      "Epoch 687/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6086 - val_loss: 0.8629\n",
      "Epoch 688/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6087 - val_loss: 0.8633\n",
      "Epoch 689/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6087 - val_loss: 0.8637\n",
      "Epoch 690/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6088 - val_loss: 0.8642\n",
      "Epoch 691/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6088 - val_loss: 0.8646\n",
      "Epoch 692/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6089 - val_loss: 0.8650\n",
      "Epoch 693/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6089 - val_loss: 0.8655\n",
      "Epoch 694/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6090 - val_loss: 0.8659\n",
      "Epoch 695/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6091 - val_loss: 0.8663\n",
      "Epoch 696/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6091 - val_loss: 0.8668\n",
      "Epoch 697/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6092 - val_loss: 0.8672\n",
      "Epoch 698/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6092 - val_loss: 0.8676\n",
      "Epoch 699/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6093 - val_loss: 0.8681\n",
      "Epoch 700/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6093 - val_loss: 0.8685\n",
      "Epoch 701/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6094 - val_loss: 0.8690\n",
      "Epoch 702/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6094 - val_loss: 0.8694\n",
      "Epoch 703/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6095 - val_loss: 0.8699\n",
      "Epoch 704/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6095 - val_loss: 0.8703\n",
      "Epoch 705/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6096 - val_loss: 0.8707\n",
      "Epoch 706/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6096 - val_loss: 0.8712\n",
      "Epoch 707/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6097 - val_loss: 0.8716\n",
      "Epoch 708/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6097 - val_loss: 0.8721\n",
      "Epoch 709/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6097 - val_loss: 0.8725\n",
      "Epoch 710/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6098 - val_loss: 0.8730\n",
      "Epoch 711/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6098 - val_loss: 0.8735\n",
      "Epoch 712/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6099 - val_loss: 0.8739\n",
      "Epoch 713/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6099 - val_loss: 0.8744\n",
      "Epoch 714/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6100 - val_loss: 0.8748\n",
      "Epoch 715/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6100 - val_loss: 0.8753\n",
      "Epoch 716/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6101 - val_loss: 0.8758\n",
      "Epoch 717/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6101 - val_loss: 0.8762\n",
      "Epoch 718/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6102 - val_loss: 0.8767\n",
      "Epoch 719/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6102 - val_loss: 0.8771\n",
      "Epoch 720/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6102 - val_loss: 0.8776\n",
      "Epoch 721/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6103 - val_loss: 0.8781\n",
      "Epoch 722/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6103 - val_loss: 0.8785\n",
      "Epoch 723/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6104 - val_loss: 0.8790\n",
      "Epoch 724/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6104 - val_loss: 0.8795\n",
      "Epoch 725/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6104 - val_loss: 0.8800\n",
      "Epoch 726/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6105 - val_loss: 0.8804\n",
      "Epoch 727/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6105 - val_loss: 0.8809\n",
      "Epoch 728/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6106 - val_loss: 0.8814\n",
      "Epoch 729/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6106 - val_loss: 0.8819\n",
      "Epoch 730/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6106 - val_loss: 0.8823\n",
      "Epoch 731/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6107 - val_loss: 0.8828\n",
      "Epoch 732/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6107 - val_loss: 0.8833\n",
      "Epoch 733/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6108 - val_loss: 0.8838\n",
      "Epoch 734/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6108 - val_loss: 0.8843\n",
      "Epoch 735/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6108 - val_loss: 0.8847\n",
      "Epoch 736/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6109 - val_loss: 0.8852\n",
      "Epoch 737/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6109 - val_loss: 0.8857\n",
      "Epoch 738/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6109 - val_loss: 0.8862\n",
      "Epoch 739/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6110 - val_loss: 0.8867\n",
      "Epoch 740/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6110 - val_loss: 0.8872\n",
      "Epoch 741/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6110 - val_loss: 0.8877\n",
      "Epoch 742/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6111 - val_loss: 0.8881\n",
      "Epoch 743/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6111 - val_loss: 0.8886\n",
      "Epoch 744/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6111 - val_loss: 0.8891\n",
      "Epoch 745/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6112 - val_loss: 0.8896\n",
      "Epoch 746/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6112 - val_loss: 0.8901\n",
      "Epoch 747/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6112 - val_loss: 0.8906\n",
      "Epoch 748/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6113 - val_loss: 0.8911\n",
      "Epoch 749/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6113 - val_loss: 0.8916\n",
      "Epoch 750/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6113 - val_loss: 0.8921\n",
      "Epoch 751/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6114 - val_loss: 0.8926\n",
      "Epoch 752/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6114 - val_loss: 0.8931\n",
      "Epoch 753/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6114 - val_loss: 0.8936\n",
      "Epoch 754/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6115 - val_loss: 0.8941\n",
      "Epoch 755/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6115 - val_loss: 0.8946\n",
      "Epoch 756/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6115 - val_loss: 0.8951\n",
      "Epoch 757/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6115 - val_loss: 0.8956\n",
      "Epoch 758/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6116 - val_loss: 0.8961\n",
      "Epoch 759/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6116 - val_loss: 0.8966\n",
      "Epoch 760/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6116 - val_loss: 0.8972\n",
      "Epoch 761/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6117 - val_loss: 0.8977\n",
      "Epoch 762/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6117 - val_loss: 0.8982\n",
      "Epoch 763/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6117 - val_loss: 0.8987\n",
      "Epoch 764/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6117 - val_loss: 0.8992\n",
      "Epoch 765/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6118 - val_loss: 0.8997\n",
      "Epoch 766/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6118 - val_loss: 0.9002\n",
      "Epoch 767/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6118 - val_loss: 0.9007\n",
      "Epoch 768/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6118 - val_loss: 0.9013\n",
      "Epoch 769/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6119 - val_loss: 0.9018\n",
      "Epoch 770/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6119 - val_loss: 0.9023\n",
      "Epoch 771/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6119 - val_loss: 0.9028\n",
      "Epoch 772/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6119 - val_loss: 0.9033\n",
      "Epoch 773/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6120 - val_loss: 0.9039\n",
      "Epoch 774/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6120 - val_loss: 0.9044\n",
      "Epoch 775/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6120 - val_loss: 0.9049\n",
      "Epoch 776/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6120 - val_loss: 0.9054\n",
      "Epoch 777/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6121 - val_loss: 0.9059\n",
      "Epoch 778/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6121 - val_loss: 0.9065\n",
      "Epoch 779/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6121 - val_loss: 0.9070\n",
      "Epoch 780/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6121 - val_loss: 0.9075\n",
      "Epoch 781/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6121 - val_loss: 0.9080\n",
      "Epoch 782/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6122 - val_loss: 0.9086\n",
      "Epoch 783/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6122 - val_loss: 0.9091\n",
      "Epoch 784/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6122 - val_loss: 0.9096\n",
      "Epoch 785/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6122 - val_loss: 0.9102\n",
      "Epoch 786/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6122 - val_loss: 0.9107\n",
      "Epoch 787/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6123 - val_loss: 0.9112\n",
      "Epoch 788/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6123 - val_loss: 0.9118\n",
      "Epoch 789/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6123 - val_loss: 0.9123\n",
      "Epoch 790/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6123 - val_loss: 0.9128\n",
      "Epoch 791/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6123 - val_loss: 0.9134\n",
      "Epoch 792/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6124 - val_loss: 0.9139\n",
      "Epoch 793/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6124 - val_loss: 0.9144\n",
      "Epoch 794/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6124 - val_loss: 0.9149\n",
      "Epoch 795/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6124 - val_loss: 0.9155\n",
      "Epoch 796/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6124 - val_loss: 0.9160\n",
      "Epoch 797/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6125 - val_loss: 0.9166\n",
      "Epoch 798/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6125 - val_loss: 0.9171\n",
      "Epoch 799/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6125 - val_loss: 0.9176\n",
      "Epoch 800/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6125 - val_loss: 0.9182\n",
      "Epoch 801/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6125 - val_loss: 0.9187\n",
      "Epoch 802/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6125 - val_loss: 0.9192\n",
      "Epoch 803/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6126 - val_loss: 0.9198\n",
      "Epoch 804/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6126 - val_loss: 0.9203\n",
      "Epoch 805/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6126 - val_loss: 0.9209\n",
      "Epoch 806/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6126 - val_loss: 0.9214\n",
      "Epoch 807/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6126 - val_loss: 0.9220\n",
      "Epoch 808/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6126 - val_loss: 0.9225\n",
      "Epoch 809/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6126 - val_loss: 0.9230\n",
      "Epoch 810/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6127 - val_loss: 0.9236\n",
      "Epoch 811/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6127 - val_loss: 0.9241\n",
      "Epoch 812/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6127 - val_loss: 0.9247\n",
      "Epoch 813/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6127 - val_loss: 0.9252\n",
      "Epoch 814/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6127 - val_loss: 0.9258\n",
      "Epoch 815/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6127 - val_loss: 0.9263\n",
      "Epoch 816/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6127 - val_loss: 0.9269\n",
      "Epoch 817/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6128 - val_loss: 0.9274\n",
      "Epoch 818/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6128 - val_loss: 0.9279\n",
      "Epoch 819/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6128 - val_loss: 0.9285\n",
      "Epoch 820/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6128 - val_loss: 0.9290\n",
      "Epoch 821/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6128 - val_loss: 0.9296\n",
      "Epoch 822/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6128 - val_loss: 0.9301\n",
      "Epoch 823/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6128 - val_loss: 0.9307\n",
      "Epoch 824/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6128 - val_loss: 0.9312\n",
      "Epoch 825/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6129 - val_loss: 0.9318\n",
      "Epoch 826/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6129 - val_loss: 0.9323\n",
      "Epoch 827/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6129 - val_loss: 0.9329\n",
      "Epoch 828/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6129 - val_loss: 0.9334\n",
      "Epoch 829/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6129 - val_loss: 0.9340\n",
      "Epoch 830/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6129 - val_loss: 0.9345\n",
      "Epoch 831/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6129 - val_loss: 0.9351\n",
      "Epoch 832/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6129 - val_loss: 0.9356\n",
      "Epoch 833/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6129 - val_loss: 0.9362\n",
      "Epoch 834/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6129 - val_loss: 0.9367\n",
      "Epoch 835/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6130 - val_loss: 0.9372\n",
      "Epoch 836/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6130 - val_loss: 0.9378\n",
      "Epoch 837/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6130 - val_loss: 0.9384\n",
      "Epoch 838/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6130 - val_loss: 0.9389\n",
      "Epoch 839/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6130 - val_loss: 0.9394\n",
      "Epoch 840/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6130 - val_loss: 0.9400\n",
      "Epoch 841/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6130 - val_loss: 0.9405\n",
      "Epoch 842/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6130 - val_loss: 0.9411\n",
      "Epoch 843/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6130 - val_loss: 0.9416\n",
      "Epoch 844/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6130 - val_loss: 0.9422\n",
      "Epoch 845/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6130 - val_loss: 0.9427\n",
      "Epoch 846/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6130 - val_loss: 0.9433\n",
      "Epoch 847/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6130 - val_loss: 0.9438\n",
      "Epoch 848/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9444\n",
      "Epoch 849/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9449\n",
      "Epoch 850/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9455\n",
      "Epoch 851/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9460\n",
      "Epoch 852/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9466\n",
      "Epoch 853/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9471\n",
      "Epoch 854/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9476\n",
      "Epoch 855/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9482\n",
      "Epoch 856/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9487\n",
      "Epoch 857/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9493\n",
      "Epoch 858/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9498\n",
      "Epoch 859/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9504\n",
      "Epoch 860/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9509\n",
      "Epoch 861/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9514\n",
      "Epoch 862/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9520\n",
      "Epoch 863/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9525\n",
      "Epoch 864/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9530\n",
      "Epoch 865/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9536\n",
      "Epoch 866/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9541\n",
      "Epoch 867/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9547\n",
      "Epoch 868/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9552\n",
      "Epoch 869/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9557\n",
      "Epoch 870/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9563\n",
      "Epoch 871/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9568\n",
      "Epoch 872/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9573\n",
      "Epoch 873/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9579\n",
      "Epoch 874/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9584\n",
      "Epoch 875/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9589\n",
      "Epoch 876/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9595\n",
      "Epoch 877/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9600\n",
      "Epoch 878/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9605\n",
      "Epoch 879/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9611\n",
      "Epoch 880/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9616\n",
      "Epoch 881/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9621\n",
      "Epoch 882/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9626\n",
      "Epoch 883/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9632\n",
      "Epoch 884/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9637\n",
      "Epoch 885/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9642\n",
      "Epoch 886/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9647\n",
      "Epoch 887/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9652\n",
      "Epoch 888/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9657\n",
      "Epoch 889/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9663\n",
      "Epoch 890/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9668\n",
      "Epoch 891/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9673\n",
      "Epoch 892/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9678\n",
      "Epoch 893/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9683\n",
      "Epoch 894/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9688\n",
      "Epoch 895/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9693\n",
      "Epoch 896/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9698\n",
      "Epoch 897/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9703\n",
      "Epoch 898/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9708\n",
      "Epoch 899/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9714\n",
      "Epoch 900/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9718\n",
      "Epoch 901/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9723\n",
      "Epoch 902/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9728\n",
      "Epoch 903/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9733\n",
      "Epoch 904/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9738\n",
      "Epoch 905/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9743\n",
      "Epoch 906/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9748\n",
      "Epoch 907/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9753\n",
      "Epoch 908/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9758\n",
      "Epoch 909/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9763\n",
      "Epoch 910/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9767\n",
      "Epoch 911/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9772\n",
      "Epoch 912/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9777\n",
      "Epoch 913/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9782\n",
      "Epoch 914/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9786\n",
      "Epoch 915/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9791\n",
      "Epoch 916/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9796\n",
      "Epoch 917/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9800\n",
      "Epoch 918/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9805\n",
      "Epoch 919/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9810\n",
      "Epoch 920/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9814\n",
      "Epoch 921/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9819\n",
      "Epoch 922/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9823\n",
      "Epoch 923/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 0.9828\n",
      "Epoch 924/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9833\n",
      "Epoch 925/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9837\n",
      "Epoch 926/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9842\n",
      "Epoch 927/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9846\n",
      "Epoch 928/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9850\n",
      "Epoch 929/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9855\n",
      "Epoch 930/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9859\n",
      "Epoch 931/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9863\n",
      "Epoch 932/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9868\n",
      "Epoch 933/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9872\n",
      "Epoch 934/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9876\n",
      "Epoch 935/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9881\n",
      "Epoch 936/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9885\n",
      "Epoch 937/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9889\n",
      "Epoch 938/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9893\n",
      "Epoch 939/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9897\n",
      "Epoch 940/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9901\n",
      "Epoch 941/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9905\n",
      "Epoch 942/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9909\n",
      "Epoch 943/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9913\n",
      "Epoch 944/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9917\n",
      "Epoch 945/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9921\n",
      "Epoch 946/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9925\n",
      "Epoch 947/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9929\n",
      "Epoch 948/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.9933\n",
      "Epoch 949/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6130 - val_loss: 0.9937\n",
      "Epoch 950/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6130 - val_loss: 0.9940\n",
      "Epoch 951/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6130 - val_loss: 0.9944\n",
      "Epoch 952/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6130 - val_loss: 0.9948\n",
      "Epoch 953/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6130 - val_loss: 0.9951\n",
      "Epoch 954/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6130 - val_loss: 0.9955\n",
      "Epoch 955/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6130 - val_loss: 0.9959\n",
      "Epoch 956/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6130 - val_loss: 0.9962\n",
      "Epoch 957/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6130 - val_loss: 0.9966\n",
      "Epoch 958/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6130 - val_loss: 0.9969\n",
      "Epoch 959/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6130 - val_loss: 0.9973\n",
      "Epoch 960/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6130 - val_loss: 0.9976\n",
      "Epoch 961/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6130 - val_loss: 0.9980\n",
      "Epoch 962/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6130 - val_loss: 0.9983\n",
      "Epoch 963/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6130 - val_loss: 0.9986\n",
      "Epoch 964/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6130 - val_loss: 0.9989\n",
      "Epoch 965/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6130 - val_loss: 0.9993\n",
      "Epoch 966/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6130 - val_loss: 0.9996\n",
      "Epoch 967/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6130 - val_loss: 0.9999\n",
      "Epoch 968/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6129 - val_loss: 1.0002\n",
      "Epoch 969/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6129 - val_loss: 1.0005\n",
      "Epoch 970/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6129 - val_loss: 1.0008\n",
      "Epoch 971/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6129 - val_loss: 1.0011\n",
      "Epoch 972/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6129 - val_loss: 1.0014\n",
      "Epoch 973/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6129 - val_loss: 1.0017\n",
      "Epoch 974/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6129 - val_loss: 1.0019\n",
      "Epoch 975/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6129 - val_loss: 1.0022\n",
      "Epoch 976/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6129 - val_loss: 1.0025\n",
      "Epoch 977/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6129 - val_loss: 1.0028\n",
      "Epoch 978/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6129 - val_loss: 1.0031\n",
      "Epoch 979/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6129 - val_loss: 1.0033\n",
      "Epoch 980/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6129 - val_loss: 1.0036\n",
      "Epoch 981/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6129 - val_loss: 1.0038\n",
      "Epoch 982/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6129 - val_loss: 1.0041\n",
      "Epoch 983/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6128 - val_loss: 1.0043\n",
      "Epoch 984/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6128 - val_loss: 1.0046\n",
      "Epoch 985/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6128 - val_loss: 1.0048\n",
      "Epoch 986/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6128 - val_loss: 1.0050\n",
      "Epoch 987/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6128 - val_loss: 1.0053\n",
      "Epoch 988/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6128 - val_loss: 1.0055\n",
      "Epoch 989/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6128 - val_loss: 1.0057\n",
      "Epoch 990/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6128 - val_loss: 1.0059\n",
      "Epoch 991/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6128 - val_loss: 1.0061\n",
      "Epoch 992/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6128 - val_loss: 1.0063\n",
      "Epoch 993/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6128 - val_loss: 1.0065\n",
      "Epoch 994/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6128 - val_loss: 1.0067\n",
      "Epoch 995/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6128 - val_loss: 1.0069\n",
      "Epoch 996/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6128 - val_loss: 1.0071\n",
      "Epoch 997/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6128 - val_loss: 1.0072\n",
      "Epoch 998/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6128 - val_loss: 1.0074\n",
      "Epoch 999/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6127 - val_loss: 1.0076\n",
      "Epoch 1000/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6127 - val_loss: 1.0077\n"
     ]
    }
   ],
   "source": [
    "epocas = 1000\n",
    "history= modelo2.fit(\n",
    "    x = x_train,\n",
    "    y = y_train,\n",
    "    batch_size = lote,\n",
    "    epochs = epocas,\n",
    "    shuffle = False,\n",
    "    validation_data = (x_val,y_val),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2f767d110>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos multivariables (2 variables - Precio y mes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_diesel = pd.read_csv('data/data_diesel_diff.csv', sep=',')\n",
    "data_diesel = data_diesel.drop(['anio'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data_diesel_scaled = scaler.fit_transform(data_diesel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    }
   ],
   "source": [
    "entrenamiento = round(0.6 * len(data_diesel_scaled))\n",
    "val_prueba = round(0.2 * len(data_diesel_scaled))\n",
    "\n",
    "train = data_diesel_scaled[:entrenamiento]\n",
    "validation = data_diesel_scaled[entrenamiento:entrenamiento+val_prueba]\n",
    "test = data_diesel_scaled[entrenamiento+val_prueba:]\n",
    "\n",
    "train = np.insert(train, 0, [0, 0], axis=0)\n",
    "\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervisada_multi(serie, retrasos=1):\n",
    "    serie_x = []\n",
    "    serie_y = []\n",
    "    for i in range(len(serie)-retrasos):\n",
    "        valor = serie[i:(i+retrasos), :]\n",
    "        valor_sig = serie[i+retrasos, :]\n",
    "        serie_x.append(valor)\n",
    "        serie_y.append(valor_sig[1])  \n",
    "    return np.array(serie_x), np.array(serie_y)\n",
    "\n",
    "x_train, y_train = supervisada_multi(train)\n",
    "x_val, y_val = supervisada_multi(validation)\n",
    "x_test, y_test = supervisada_multi(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train, (x_train.shape[0], 1, 2))\n",
    "x_val = np.reshape(x_val, (x_val.shape[0], 1, 2))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_33 (LSTM)              (1, 1)                    16        \n",
      "                                                                 \n",
      " dense_11 (Dense)            (1, 1)                    2         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18 (72.00 Byte)\n",
      "Trainable params: 18 (72.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelo3 = Sequential()\n",
    "lote = 1\n",
    "paso = 1\n",
    "caracteristicas = 2\n",
    "modelo3.add(LSTM(lote, batch_input_shape=(lote, paso, caracteristicas), stateful=True))\n",
    "modelo3.add(Dense(1))\n",
    "modelo3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo3.compile(loss='mean_squared_error', optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "161/161 [==============================] - 1s 2ms/step - loss: 1.0946 - val_loss: 1.2915\n",
      "Epoch 2/1000\n",
      "161/161 [==============================] - 0s 914us/step - loss: 1.0776 - val_loss: 1.2733\n",
      "Epoch 3/1000\n",
      "161/161 [==============================] - 0s 806us/step - loss: 1.0635 - val_loss: 1.2579\n",
      "Epoch 4/1000\n",
      "161/161 [==============================] - 0s 793us/step - loss: 1.0516 - val_loss: 1.2451\n",
      "Epoch 5/1000\n",
      "161/161 [==============================] - 0s 836us/step - loss: 1.0417 - val_loss: 1.2345\n",
      "Epoch 6/1000\n",
      "161/161 [==============================] - 0s 932us/step - loss: 1.0335 - val_loss: 1.2258\n",
      "Epoch 7/1000\n",
      "161/161 [==============================] - 0s 913us/step - loss: 1.0267 - val_loss: 1.2187\n",
      "Epoch 8/1000\n",
      "161/161 [==============================] - 0s 840us/step - loss: 1.0211 - val_loss: 1.2128\n",
      "Epoch 9/1000\n",
      "161/161 [==============================] - 0s 845us/step - loss: 1.0164 - val_loss: 1.2078\n",
      "Epoch 10/1000\n",
      "161/161 [==============================] - 0s 886us/step - loss: 1.0121 - val_loss: 1.2034\n",
      "Epoch 11/1000\n",
      "161/161 [==============================] - 0s 785us/step - loss: 1.0080 - val_loss: 1.1992\n",
      "Epoch 12/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 1.0040 - val_loss: 1.1950\n",
      "Epoch 13/1000\n",
      "161/161 [==============================] - 0s 782us/step - loss: 0.9998 - val_loss: 1.1907\n",
      "Epoch 14/1000\n",
      "161/161 [==============================] - 0s 801us/step - loss: 0.9955 - val_loss: 1.1861\n",
      "Epoch 15/1000\n",
      "161/161 [==============================] - 0s 871us/step - loss: 0.9910 - val_loss: 1.1812\n",
      "Epoch 16/1000\n",
      "161/161 [==============================] - 0s 869us/step - loss: 0.9862 - val_loss: 1.1759\n",
      "Epoch 17/1000\n",
      "161/161 [==============================] - 0s 902us/step - loss: 0.9812 - val_loss: 1.1702\n",
      "Epoch 18/1000\n",
      "161/161 [==============================] - 0s 820us/step - loss: 0.9759 - val_loss: 1.1640\n",
      "Epoch 19/1000\n",
      "161/161 [==============================] - 0s 895us/step - loss: 0.9702 - val_loss: 1.1572\n",
      "Epoch 20/1000\n",
      "161/161 [==============================] - 0s 778us/step - loss: 0.9639 - val_loss: 1.1498\n",
      "Epoch 21/1000\n",
      "161/161 [==============================] - 0s 781us/step - loss: 0.9570 - val_loss: 1.1419\n",
      "Epoch 22/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.9497 - val_loss: 1.1337\n",
      "Epoch 23/1000\n",
      "161/161 [==============================] - 0s 782us/step - loss: 0.9420 - val_loss: 1.1252\n",
      "Epoch 24/1000\n",
      "161/161 [==============================] - 0s 775us/step - loss: 0.9343 - val_loss: 1.1166\n",
      "Epoch 25/1000\n",
      "161/161 [==============================] - 0s 787us/step - loss: 0.9264 - val_loss: 1.1081\n",
      "Epoch 26/1000\n",
      "161/161 [==============================] - 0s 783us/step - loss: 0.9187 - val_loss: 1.0995\n",
      "Epoch 27/1000\n",
      "161/161 [==============================] - 0s 776us/step - loss: 0.9110 - val_loss: 1.0911\n",
      "Epoch 28/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.9035 - val_loss: 1.0829\n",
      "Epoch 29/1000\n",
      "161/161 [==============================] - 0s 780us/step - loss: 0.8962 - val_loss: 1.0749\n",
      "Epoch 30/1000\n",
      "161/161 [==============================] - 0s 861us/step - loss: 0.8891 - val_loss: 1.0671\n",
      "Epoch 31/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.8823 - val_loss: 1.0595\n",
      "Epoch 32/1000\n",
      "161/161 [==============================] - 0s 789us/step - loss: 0.8758 - val_loss: 1.0522\n",
      "Epoch 33/1000\n",
      "161/161 [==============================] - 0s 796us/step - loss: 0.8695 - val_loss: 1.0451\n",
      "Epoch 34/1000\n",
      "161/161 [==============================] - 0s 841us/step - loss: 0.8634 - val_loss: 1.0383\n",
      "Epoch 35/1000\n",
      "161/161 [==============================] - 0s 802us/step - loss: 0.8576 - val_loss: 1.0318\n",
      "Epoch 36/1000\n",
      "161/161 [==============================] - 0s 809us/step - loss: 0.8521 - val_loss: 1.0255\n",
      "Epoch 37/1000\n",
      "161/161 [==============================] - 0s 796us/step - loss: 0.8468 - val_loss: 1.0194\n",
      "Epoch 38/1000\n",
      "161/161 [==============================] - 0s 797us/step - loss: 0.8417 - val_loss: 1.0136\n",
      "Epoch 39/1000\n",
      "161/161 [==============================] - 0s 797us/step - loss: 0.8369 - val_loss: 1.0080\n",
      "Epoch 40/1000\n",
      "161/161 [==============================] - 0s 876us/step - loss: 0.8322 - val_loss: 1.0026\n",
      "Epoch 41/1000\n",
      "161/161 [==============================] - 0s 830us/step - loss: 0.8278 - val_loss: 0.9974\n",
      "Epoch 42/1000\n",
      "161/161 [==============================] - 0s 788us/step - loss: 0.8236 - val_loss: 0.9924\n",
      "Epoch 43/1000\n",
      "161/161 [==============================] - 0s 775us/step - loss: 0.8195 - val_loss: 0.9876\n",
      "Epoch 44/1000\n",
      "161/161 [==============================] - 0s 785us/step - loss: 0.8156 - val_loss: 0.9830\n",
      "Epoch 45/1000\n",
      "161/161 [==============================] - 0s 795us/step - loss: 0.8119 - val_loss: 0.9786\n",
      "Epoch 46/1000\n",
      "161/161 [==============================] - 0s 809us/step - loss: 0.8084 - val_loss: 0.9744\n",
      "Epoch 47/1000\n",
      "161/161 [==============================] - 0s 838us/step - loss: 0.8050 - val_loss: 0.9703\n",
      "Epoch 48/1000\n",
      "161/161 [==============================] - 0s 939us/step - loss: 0.8018 - val_loss: 0.9664\n",
      "Epoch 49/1000\n",
      "161/161 [==============================] - 0s 973us/step - loss: 0.7986 - val_loss: 0.9626\n",
      "Epoch 50/1000\n",
      "161/161 [==============================] - 0s 849us/step - loss: 0.7957 - val_loss: 0.9590\n",
      "Epoch 51/1000\n",
      "161/161 [==============================] - 0s 776us/step - loss: 0.7928 - val_loss: 0.9555\n",
      "Epoch 52/1000\n",
      "161/161 [==============================] - 0s 780us/step - loss: 0.7901 - val_loss: 0.9521\n",
      "Epoch 53/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.7874 - val_loss: 0.9489\n",
      "Epoch 54/1000\n",
      "161/161 [==============================] - 0s 782us/step - loss: 0.7849 - val_loss: 0.9457\n",
      "Epoch 55/1000\n",
      "161/161 [==============================] - 0s 782us/step - loss: 0.7825 - val_loss: 0.9427\n",
      "Epoch 56/1000\n",
      "161/161 [==============================] - 0s 779us/step - loss: 0.7801 - val_loss: 0.9398\n",
      "Epoch 57/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.7779 - val_loss: 0.9370\n",
      "Epoch 58/1000\n",
      "161/161 [==============================] - 0s 854us/step - loss: 0.7757 - val_loss: 0.9343\n",
      "Epoch 59/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.7736 - val_loss: 0.9317\n",
      "Epoch 60/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.7716 - val_loss: 0.9292\n",
      "Epoch 61/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.7697 - val_loss: 0.9267\n",
      "Epoch 62/1000\n",
      "161/161 [==============================] - 0s 775us/step - loss: 0.7678 - val_loss: 0.9243\n",
      "Epoch 63/1000\n",
      "161/161 [==============================] - 0s 775us/step - loss: 0.7660 - val_loss: 0.9220\n",
      "Epoch 64/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.7643 - val_loss: 0.9198\n",
      "Epoch 65/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.7626 - val_loss: 0.9177\n",
      "Epoch 66/1000\n",
      "161/161 [==============================] - 0s 776us/step - loss: 0.7610 - val_loss: 0.9156\n",
      "Epoch 67/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.7594 - val_loss: 0.9135\n",
      "Epoch 68/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.7579 - val_loss: 0.9115\n",
      "Epoch 69/1000\n",
      "161/161 [==============================] - 0s 859us/step - loss: 0.7564 - val_loss: 0.9096\n",
      "Epoch 70/1000\n",
      "161/161 [==============================] - 0s 779us/step - loss: 0.7550 - val_loss: 0.9078\n",
      "Epoch 71/1000\n",
      "161/161 [==============================] - 0s 777us/step - loss: 0.7536 - val_loss: 0.9059\n",
      "Epoch 72/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.7522 - val_loss: 0.9042\n",
      "Epoch 73/1000\n",
      "161/161 [==============================] - 0s 775us/step - loss: 0.7509 - val_loss: 0.9024\n",
      "Epoch 74/1000\n",
      "161/161 [==============================] - 0s 777us/step - loss: 0.7496 - val_loss: 0.9008\n",
      "Epoch 75/1000\n",
      "161/161 [==============================] - 0s 776us/step - loss: 0.7484 - val_loss: 0.8991\n",
      "Epoch 76/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.7472 - val_loss: 0.8975\n",
      "Epoch 77/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.7460 - val_loss: 0.8960\n",
      "Epoch 78/1000\n",
      "161/161 [==============================] - 0s 780us/step - loss: 0.7449 - val_loss: 0.8944\n",
      "Epoch 79/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.7438 - val_loss: 0.8929\n",
      "Epoch 80/1000\n",
      "161/161 [==============================] - 0s 858us/step - loss: 0.7427 - val_loss: 0.8915\n",
      "Epoch 81/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.7417 - val_loss: 0.8901\n",
      "Epoch 82/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.7406 - val_loss: 0.8887\n",
      "Epoch 83/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.7397 - val_loss: 0.8873\n",
      "Epoch 84/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.7387 - val_loss: 0.8860\n",
      "Epoch 85/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.7377 - val_loss: 0.8847\n",
      "Epoch 86/1000\n",
      "161/161 [==============================] - 0s 777us/step - loss: 0.7368 - val_loss: 0.8834\n",
      "Epoch 87/1000\n",
      "161/161 [==============================] - 0s 803us/step - loss: 0.7359 - val_loss: 0.8822\n",
      "Epoch 88/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.7350 - val_loss: 0.8810\n",
      "Epoch 89/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.7342 - val_loss: 0.8798\n",
      "Epoch 90/1000\n",
      "161/161 [==============================] - 0s 857us/step - loss: 0.7333 - val_loss: 0.8787\n",
      "Epoch 91/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.7325 - val_loss: 0.8775\n",
      "Epoch 92/1000\n",
      "161/161 [==============================] - 0s 776us/step - loss: 0.7317 - val_loss: 0.8764\n",
      "Epoch 93/1000\n",
      "161/161 [==============================] - 0s 796us/step - loss: 0.7309 - val_loss: 0.8753\n",
      "Epoch 94/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.7302 - val_loss: 0.8743\n",
      "Epoch 95/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.7294 - val_loss: 0.8733\n",
      "Epoch 96/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.7287 - val_loss: 0.8722\n",
      "Epoch 97/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.7280 - val_loss: 0.8713\n",
      "Epoch 98/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.7273 - val_loss: 0.8703\n",
      "Epoch 99/1000\n",
      "161/161 [==============================] - 0s 777us/step - loss: 0.7266 - val_loss: 0.8693\n",
      "Epoch 100/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.7259 - val_loss: 0.8684\n",
      "Epoch 101/1000\n",
      "161/161 [==============================] - 0s 861us/step - loss: 0.7252 - val_loss: 0.8675\n",
      "Epoch 102/1000\n",
      "161/161 [==============================] - 0s 780us/step - loss: 0.7246 - val_loss: 0.8666\n",
      "Epoch 103/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.7239 - val_loss: 0.8658\n",
      "Epoch 104/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.7233 - val_loss: 0.8649\n",
      "Epoch 105/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.7227 - val_loss: 0.8641\n",
      "Epoch 106/1000\n",
      "161/161 [==============================] - 0s 776us/step - loss: 0.7221 - val_loss: 0.8633\n",
      "Epoch 107/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.7215 - val_loss: 0.8625\n",
      "Epoch 108/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.7209 - val_loss: 0.8617\n",
      "Epoch 109/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.7203 - val_loss: 0.8609\n",
      "Epoch 110/1000\n",
      "161/161 [==============================] - 0s 779us/step - loss: 0.7198 - val_loss: 0.8602\n",
      "Epoch 111/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.7192 - val_loss: 0.8595\n",
      "Epoch 112/1000\n",
      "161/161 [==============================] - 0s 866us/step - loss: 0.7187 - val_loss: 0.8588\n",
      "Epoch 113/1000\n",
      "161/161 [==============================] - 0s 777us/step - loss: 0.7181 - val_loss: 0.8581\n",
      "Epoch 114/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.7176 - val_loss: 0.8574\n",
      "Epoch 115/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.7171 - val_loss: 0.8567\n",
      "Epoch 116/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.7166 - val_loss: 0.8561\n",
      "Epoch 117/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.7161 - val_loss: 0.8554\n",
      "Epoch 118/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.7156 - val_loss: 0.8548\n",
      "Epoch 119/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.7151 - val_loss: 0.8542\n",
      "Epoch 120/1000\n",
      "161/161 [==============================] - 0s 780us/step - loss: 0.7146 - val_loss: 0.8536\n",
      "Epoch 121/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.7141 - val_loss: 0.8530\n",
      "Epoch 122/1000\n",
      "161/161 [==============================] - 0s 849us/step - loss: 0.7136 - val_loss: 0.8524\n",
      "Epoch 123/1000\n",
      "161/161 [==============================] - 0s 779us/step - loss: 0.7132 - val_loss: 0.8518\n",
      "Epoch 124/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.7127 - val_loss: 0.8513\n",
      "Epoch 125/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.7123 - val_loss: 0.8507\n",
      "Epoch 126/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.7118 - val_loss: 0.8502\n",
      "Epoch 127/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.7114 - val_loss: 0.8497\n",
      "Epoch 128/1000\n",
      "161/161 [==============================] - 0s 798us/step - loss: 0.7109 - val_loss: 0.8491\n",
      "Epoch 129/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.7105 - val_loss: 0.8486\n",
      "Epoch 130/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.7101 - val_loss: 0.8481\n",
      "Epoch 131/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.7096 - val_loss: 0.8476\n",
      "Epoch 132/1000\n",
      "161/161 [==============================] - 0s 864us/step - loss: 0.7092 - val_loss: 0.8472\n",
      "Epoch 133/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.7088 - val_loss: 0.8467\n",
      "Epoch 134/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.7084 - val_loss: 0.8462\n",
      "Epoch 135/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.7080 - val_loss: 0.8457\n",
      "Epoch 136/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.7076 - val_loss: 0.8453\n",
      "Epoch 137/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.7072 - val_loss: 0.8448\n",
      "Epoch 138/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.7068 - val_loss: 0.8444\n",
      "Epoch 139/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.7064 - val_loss: 0.8440\n",
      "Epoch 140/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.7060 - val_loss: 0.8435\n",
      "Epoch 141/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.7056 - val_loss: 0.8431\n",
      "Epoch 142/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.7052 - val_loss: 0.8427\n",
      "Epoch 143/1000\n",
      "161/161 [==============================] - 0s 849us/step - loss: 0.7048 - val_loss: 0.8423\n",
      "Epoch 144/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.7045 - val_loss: 0.8419\n",
      "Epoch 145/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.7041 - val_loss: 0.8415\n",
      "Epoch 146/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.7037 - val_loss: 0.8411\n",
      "Epoch 147/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.7034 - val_loss: 0.8407\n",
      "Epoch 148/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.7030 - val_loss: 0.8403\n",
      "Epoch 149/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.7026 - val_loss: 0.8399\n",
      "Epoch 150/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.7023 - val_loss: 0.8396\n",
      "Epoch 151/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.7019 - val_loss: 0.8392\n",
      "Epoch 152/1000\n",
      "161/161 [==============================] - 0s 854us/step - loss: 0.7016 - val_loss: 0.8388\n",
      "Epoch 153/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.7012 - val_loss: 0.8385\n",
      "Epoch 154/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.7009 - val_loss: 0.8381\n",
      "Epoch 155/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.7006 - val_loss: 0.8377\n",
      "Epoch 156/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.7002 - val_loss: 0.8374\n",
      "Epoch 157/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.6999 - val_loss: 0.8370\n",
      "Epoch 158/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6996 - val_loss: 0.8367\n",
      "Epoch 159/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6992 - val_loss: 0.8364\n",
      "Epoch 160/1000\n",
      "161/161 [==============================] - 0s 777us/step - loss: 0.6989 - val_loss: 0.8360\n",
      "Epoch 161/1000\n",
      "161/161 [==============================] - 0s 856us/step - loss: 0.6986 - val_loss: 0.8357\n",
      "Epoch 162/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.6983 - val_loss: 0.8354\n",
      "Epoch 163/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.6980 - val_loss: 0.8350\n",
      "Epoch 164/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.6976 - val_loss: 0.8347\n",
      "Epoch 165/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.6973 - val_loss: 0.8344\n",
      "Epoch 166/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6970 - val_loss: 0.8341\n",
      "Epoch 167/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.6967 - val_loss: 0.8338\n",
      "Epoch 168/1000\n",
      "161/161 [==============================] - 0s 777us/step - loss: 0.6964 - val_loss: 0.8334\n",
      "Epoch 169/1000\n",
      "161/161 [==============================] - 0s 800us/step - loss: 0.6961 - val_loss: 0.8331\n",
      "Epoch 170/1000\n",
      "161/161 [==============================] - 0s 850us/step - loss: 0.6958 - val_loss: 0.8328\n",
      "Epoch 171/1000\n",
      "161/161 [==============================] - 0s 777us/step - loss: 0.6955 - val_loss: 0.8325\n",
      "Epoch 172/1000\n",
      "161/161 [==============================] - 0s 779us/step - loss: 0.6953 - val_loss: 0.8322\n",
      "Epoch 173/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.6950 - val_loss: 0.8319\n",
      "Epoch 174/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6947 - val_loss: 0.8316\n",
      "Epoch 175/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.6944 - val_loss: 0.8313\n",
      "Epoch 176/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.6941 - val_loss: 0.8310\n",
      "Epoch 177/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.6939 - val_loss: 0.8308\n",
      "Epoch 178/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6936 - val_loss: 0.8305\n",
      "Epoch 179/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.6933 - val_loss: 0.8302\n",
      "Epoch 180/1000\n",
      "161/161 [==============================] - 0s 847us/step - loss: 0.6931 - val_loss: 0.8299\n",
      "Epoch 181/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.6928 - val_loss: 0.8296\n",
      "Epoch 182/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.6926 - val_loss: 0.8294\n",
      "Epoch 183/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.6923 - val_loss: 0.8291\n",
      "Epoch 184/1000\n",
      "161/161 [==============================] - 0s 780us/step - loss: 0.6920 - val_loss: 0.8288\n",
      "Epoch 185/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.6918 - val_loss: 0.8286\n",
      "Epoch 186/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.6916 - val_loss: 0.8283\n",
      "Epoch 187/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.6913 - val_loss: 0.8280\n",
      "Epoch 188/1000\n",
      "161/161 [==============================] - 0s 775us/step - loss: 0.6911 - val_loss: 0.8278\n",
      "Epoch 189/1000\n",
      "161/161 [==============================] - 0s 844us/step - loss: 0.6909 - val_loss: 0.8275\n",
      "Epoch 190/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.6906 - val_loss: 0.8273\n",
      "Epoch 191/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6904 - val_loss: 0.8270\n",
      "Epoch 192/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.6902 - val_loss: 0.8268\n",
      "Epoch 193/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.6900 - val_loss: 0.8265\n",
      "Epoch 194/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6897 - val_loss: 0.8263\n",
      "Epoch 195/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.6895 - val_loss: 0.8261\n",
      "Epoch 196/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6893 - val_loss: 0.8258\n",
      "Epoch 197/1000\n",
      "161/161 [==============================] - 0s 822us/step - loss: 0.6891 - val_loss: 0.8256\n",
      "Epoch 198/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6889 - val_loss: 0.8254\n",
      "Epoch 199/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6887 - val_loss: 0.8251\n",
      "Epoch 200/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.6885 - val_loss: 0.8249\n",
      "Epoch 201/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.6883 - val_loss: 0.8247\n",
      "Epoch 202/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6881 - val_loss: 0.8245\n",
      "Epoch 203/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6879 - val_loss: 0.8242\n",
      "Epoch 204/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6878 - val_loss: 0.8240\n",
      "Epoch 205/1000\n",
      "161/161 [==============================] - 0s 806us/step - loss: 0.6876 - val_loss: 0.8238\n",
      "Epoch 206/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.6874 - val_loss: 0.8236\n",
      "Epoch 207/1000\n",
      "161/161 [==============================] - 0s 853us/step - loss: 0.6872 - val_loss: 0.8234\n",
      "Epoch 208/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6871 - val_loss: 0.8232\n",
      "Epoch 209/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6869 - val_loss: 0.8230\n",
      "Epoch 210/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6867 - val_loss: 0.8228\n",
      "Epoch 211/1000\n",
      "161/161 [==============================] - 0s 778us/step - loss: 0.6866 - val_loss: 0.8226\n",
      "Epoch 212/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.6864 - val_loss: 0.8224\n",
      "Epoch 213/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6863 - val_loss: 0.8222\n",
      "Epoch 214/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.6861 - val_loss: 0.8220\n",
      "Epoch 215/1000\n",
      "161/161 [==============================] - 0s 851us/step - loss: 0.6860 - val_loss: 0.8218\n",
      "Epoch 216/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6858 - val_loss: 0.8216\n",
      "Epoch 217/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6857 - val_loss: 0.8214\n",
      "Epoch 218/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.6856 - val_loss: 0.8212\n",
      "Epoch 219/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.6854 - val_loss: 0.8210\n",
      "Epoch 220/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6853 - val_loss: 0.8208\n",
      "Epoch 221/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6852 - val_loss: 0.8206\n",
      "Epoch 222/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.6851 - val_loss: 0.8205\n",
      "Epoch 223/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.6850 - val_loss: 0.8203\n",
      "Epoch 224/1000\n",
      "161/161 [==============================] - 0s 857us/step - loss: 0.6848 - val_loss: 0.8201\n",
      "Epoch 225/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6847 - val_loss: 0.8199\n",
      "Epoch 226/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6846 - val_loss: 0.8198\n",
      "Epoch 227/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.6845 - val_loss: 0.8196\n",
      "Epoch 228/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.6844 - val_loss: 0.8194\n",
      "Epoch 229/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.6843 - val_loss: 0.8193\n",
      "Epoch 230/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.6843 - val_loss: 0.8191\n",
      "Epoch 231/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6842 - val_loss: 0.8189\n",
      "Epoch 232/1000\n",
      "161/161 [==============================] - 0s 776us/step - loss: 0.6841 - val_loss: 0.8188\n",
      "Epoch 233/1000\n",
      "161/161 [==============================] - 0s 843us/step - loss: 0.6840 - val_loss: 0.8186\n",
      "Epoch 234/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.6840 - val_loss: 0.8185\n",
      "Epoch 235/1000\n",
      "161/161 [==============================] - 0s 776us/step - loss: 0.6839 - val_loss: 0.8183\n",
      "Epoch 236/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.6838 - val_loss: 0.8182\n",
      "Epoch 237/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6838 - val_loss: 0.8180\n",
      "Epoch 238/1000\n",
      "161/161 [==============================] - 0s 797us/step - loss: 0.6837 - val_loss: 0.8179\n",
      "Epoch 239/1000\n",
      "161/161 [==============================] - 0s 775us/step - loss: 0.6837 - val_loss: 0.8178\n",
      "Epoch 240/1000\n",
      "161/161 [==============================] - 0s 776us/step - loss: 0.6837 - val_loss: 0.8176\n",
      "Epoch 241/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.6837 - val_loss: 0.8175\n",
      "Epoch 242/1000\n",
      "161/161 [==============================] - 0s 858us/step - loss: 0.6836 - val_loss: 0.8174\n",
      "Epoch 243/1000\n",
      "161/161 [==============================] - 0s 780us/step - loss: 0.6836 - val_loss: 0.8173\n",
      "Epoch 244/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.6836 - val_loss: 0.8171\n",
      "Epoch 245/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6836 - val_loss: 0.8170\n",
      "Epoch 246/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6836 - val_loss: 0.8169\n",
      "Epoch 247/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.6836 - val_loss: 0.8168\n",
      "Epoch 248/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.6837 - val_loss: 0.8167\n",
      "Epoch 249/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.6837 - val_loss: 0.8166\n",
      "Epoch 250/1000\n",
      "161/161 [==============================] - 0s 859us/step - loss: 0.6837 - val_loss: 0.8165\n",
      "Epoch 251/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.6838 - val_loss: 0.8164\n",
      "Epoch 252/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.6838 - val_loss: 0.8163\n",
      "Epoch 253/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.6839 - val_loss: 0.8163\n",
      "Epoch 254/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.6839 - val_loss: 0.8162\n",
      "Epoch 255/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.6840 - val_loss: 0.8161\n",
      "Epoch 256/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6841 - val_loss: 0.8161\n",
      "Epoch 257/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.6842 - val_loss: 0.8160\n",
      "Epoch 258/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6842 - val_loss: 0.8160\n",
      "Epoch 259/1000\n",
      "161/161 [==============================] - 0s 858us/step - loss: 0.6843 - val_loss: 0.8159\n",
      "Epoch 260/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.6844 - val_loss: 0.8159\n",
      "Epoch 261/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6845 - val_loss: 0.8159\n",
      "Epoch 262/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6847 - val_loss: 0.8159\n",
      "Epoch 263/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.6848 - val_loss: 0.8159\n",
      "Epoch 264/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.6849 - val_loss: 0.8159\n",
      "Epoch 265/1000\n",
      "161/161 [==============================] - 0s 796us/step - loss: 0.6850 - val_loss: 0.8159\n",
      "Epoch 266/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.6852 - val_loss: 0.8159\n",
      "Epoch 267/1000\n",
      "161/161 [==============================] - 0s 776us/step - loss: 0.6853 - val_loss: 0.8159\n",
      "Epoch 268/1000\n",
      "161/161 [==============================] - 0s 854us/step - loss: 0.6855 - val_loss: 0.8160\n",
      "Epoch 269/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.6856 - val_loss: 0.8160\n",
      "Epoch 270/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6858 - val_loss: 0.8161\n",
      "Epoch 271/1000\n",
      "161/161 [==============================] - 0s 779us/step - loss: 0.6860 - val_loss: 0.8161\n",
      "Epoch 272/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6862 - val_loss: 0.8162\n",
      "Epoch 273/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.6863 - val_loss: 0.8163\n",
      "Epoch 274/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.6865 - val_loss: 0.8164\n",
      "Epoch 275/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.6867 - val_loss: 0.8165\n",
      "Epoch 276/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6869 - val_loss: 0.8166\n",
      "Epoch 277/1000\n",
      "161/161 [==============================] - 0s 851us/step - loss: 0.6871 - val_loss: 0.8167\n",
      "Epoch 278/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.6874 - val_loss: 0.8168\n",
      "Epoch 279/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.6876 - val_loss: 0.8170\n",
      "Epoch 280/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.6878 - val_loss: 0.8171\n",
      "Epoch 281/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.6881 - val_loss: 0.8173\n",
      "Epoch 282/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.6883 - val_loss: 0.8175\n",
      "Epoch 283/1000\n",
      "161/161 [==============================] - 0s 775us/step - loss: 0.6886 - val_loss: 0.8176\n",
      "Epoch 284/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.6888 - val_loss: 0.8178\n",
      "Epoch 285/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6891 - val_loss: 0.8180\n",
      "Epoch 286/1000\n",
      "161/161 [==============================] - 0s 853us/step - loss: 0.6894 - val_loss: 0.8182\n",
      "Epoch 287/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6897 - val_loss: 0.8185\n",
      "Epoch 288/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6900 - val_loss: 0.8187\n",
      "Epoch 289/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.6903 - val_loss: 0.8189\n",
      "Epoch 290/1000\n",
      "161/161 [==============================] - 0s 811us/step - loss: 0.6906 - val_loss: 0.8192\n",
      "Epoch 291/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.6910 - val_loss: 0.8194\n",
      "Epoch 292/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.6913 - val_loss: 0.8197\n",
      "Epoch 293/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6917 - val_loss: 0.8200\n",
      "Epoch 294/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.6921 - val_loss: 0.8202\n",
      "Epoch 295/1000\n",
      "161/161 [==============================] - 0s 845us/step - loss: 0.6924 - val_loss: 0.8205\n",
      "Epoch 296/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.6928 - val_loss: 0.8208\n",
      "Epoch 297/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.6932 - val_loss: 0.8212\n",
      "Epoch 298/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.6936 - val_loss: 0.8215\n",
      "Epoch 299/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.6941 - val_loss: 0.8218\n",
      "Epoch 300/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6945 - val_loss: 0.8222\n",
      "Epoch 301/1000\n",
      "161/161 [==============================] - 0s 781us/step - loss: 0.6950 - val_loss: 0.8225\n",
      "Epoch 302/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6954 - val_loss: 0.8229\n",
      "Epoch 303/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6959 - val_loss: 0.8233\n",
      "Epoch 304/1000\n",
      "161/161 [==============================] - 0s 850us/step - loss: 0.6964 - val_loss: 0.8237\n",
      "Epoch 305/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.6969 - val_loss: 0.8241\n",
      "Epoch 306/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.6974 - val_loss: 0.8245\n",
      "Epoch 307/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.6979 - val_loss: 0.8249\n",
      "Epoch 308/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.6985 - val_loss: 0.8254\n",
      "Epoch 309/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.6990 - val_loss: 0.8258\n",
      "Epoch 310/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.6996 - val_loss: 0.8263\n",
      "Epoch 311/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.7002 - val_loss: 0.8268\n",
      "Epoch 312/1000\n",
      "161/161 [==============================] - 0s 849us/step - loss: 0.7007 - val_loss: 0.8273\n",
      "Epoch 313/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.7013 - val_loss: 0.8278\n",
      "Epoch 314/1000\n",
      "161/161 [==============================] - 0s 802us/step - loss: 0.7019 - val_loss: 0.8283\n",
      "Epoch 315/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.7025 - val_loss: 0.8288\n",
      "Epoch 316/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.7032 - val_loss: 0.8294\n",
      "Epoch 317/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.7038 - val_loss: 0.8299\n",
      "Epoch 318/1000\n",
      "161/161 [==============================] - 0s 777us/step - loss: 0.7044 - val_loss: 0.8305\n",
      "Epoch 319/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.7051 - val_loss: 0.8310\n",
      "Epoch 320/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.7057 - val_loss: 0.8316\n",
      "Epoch 321/1000\n",
      "161/161 [==============================] - 0s 840us/step - loss: 0.7064 - val_loss: 0.8322\n",
      "Epoch 322/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.7070 - val_loss: 0.8328\n",
      "Epoch 323/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.7077 - val_loss: 0.8334\n",
      "Epoch 324/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.7084 - val_loss: 0.8340\n",
      "Epoch 325/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.7090 - val_loss: 0.8346\n",
      "Epoch 326/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.7097 - val_loss: 0.8352\n",
      "Epoch 327/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.7104 - val_loss: 0.8359\n",
      "Epoch 328/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.7111 - val_loss: 0.8365\n",
      "Epoch 329/1000\n",
      "161/161 [==============================] - 0s 844us/step - loss: 0.7117 - val_loss: 0.8372\n",
      "Epoch 330/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.7124 - val_loss: 0.8378\n",
      "Epoch 331/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.7131 - val_loss: 0.8385\n",
      "Epoch 332/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.7138 - val_loss: 0.8391\n",
      "Epoch 333/1000\n",
      "161/161 [==============================] - 0s 776us/step - loss: 0.7145 - val_loss: 0.8398\n",
      "Epoch 334/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.7152 - val_loss: 0.8405\n",
      "Epoch 335/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.7158 - val_loss: 0.8412\n",
      "Epoch 336/1000\n",
      "161/161 [==============================] - 0s 848us/step - loss: 0.7165 - val_loss: 0.8418\n",
      "Epoch 337/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.7172 - val_loss: 0.8425\n",
      "Epoch 338/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.7179 - val_loss: 0.8432\n",
      "Epoch 339/1000\n",
      "161/161 [==============================] - 0s 797us/step - loss: 0.7186 - val_loss: 0.8439\n",
      "Epoch 340/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.7192 - val_loss: 0.8446\n",
      "Epoch 341/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.7199 - val_loss: 0.8453\n",
      "Epoch 342/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.7206 - val_loss: 0.8460\n",
      "Epoch 343/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.7213 - val_loss: 0.8466\n",
      "Epoch 344/1000\n",
      "161/161 [==============================] - 0s 852us/step - loss: 0.7219 - val_loss: 0.8473\n",
      "Epoch 345/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.7226 - val_loss: 0.8480\n",
      "Epoch 346/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.7232 - val_loss: 0.8487\n",
      "Epoch 347/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.7239 - val_loss: 0.8494\n",
      "Epoch 348/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.7245 - val_loss: 0.8501\n",
      "Epoch 349/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.7252 - val_loss: 0.8507\n",
      "Epoch 350/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.7258 - val_loss: 0.8514\n",
      "Epoch 351/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.7265 - val_loss: 0.8521\n",
      "Epoch 352/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.7271 - val_loss: 0.8528\n",
      "Epoch 353/1000\n",
      "161/161 [==============================] - 0s 856us/step - loss: 0.7277 - val_loss: 0.8534\n",
      "Epoch 354/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.7284 - val_loss: 0.8541\n",
      "Epoch 355/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.7290 - val_loss: 0.8548\n",
      "Epoch 356/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.7296 - val_loss: 0.8554\n",
      "Epoch 357/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.7302 - val_loss: 0.8561\n",
      "Epoch 358/1000\n",
      "161/161 [==============================] - 0s 782us/step - loss: 0.7308 - val_loss: 0.8567\n",
      "Epoch 359/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.7314 - val_loss: 0.8574\n",
      "Epoch 360/1000\n",
      "161/161 [==============================] - 0s 905us/step - loss: 0.7320 - val_loss: 0.8580\n",
      "Epoch 361/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.7326 - val_loss: 0.8586\n",
      "Epoch 362/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.7332 - val_loss: 0.8593\n",
      "Epoch 363/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.7338 - val_loss: 0.8599\n",
      "Epoch 364/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.7344 - val_loss: 0.8605\n",
      "Epoch 365/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.7350 - val_loss: 0.8611\n",
      "Epoch 366/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.7355 - val_loss: 0.8617\n",
      "Epoch 367/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.7361 - val_loss: 0.8624\n",
      "Epoch 368/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.7367 - val_loss: 0.8630\n",
      "Epoch 369/1000\n",
      "161/161 [==============================] - 0s 860us/step - loss: 0.7372 - val_loss: 0.8636\n",
      "Epoch 370/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.7378 - val_loss: 0.8641\n",
      "Epoch 371/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.7383 - val_loss: 0.8647\n",
      "Epoch 372/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.7389 - val_loss: 0.8653\n",
      "Epoch 373/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.7394 - val_loss: 0.8659\n",
      "Epoch 374/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.7399 - val_loss: 0.8665\n",
      "Epoch 375/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.7405 - val_loss: 0.8670\n",
      "Epoch 376/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.7410 - val_loss: 0.8676\n",
      "Epoch 377/1000\n",
      "161/161 [==============================] - 0s 842us/step - loss: 0.7415 - val_loss: 0.8682\n",
      "Epoch 378/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.7421 - val_loss: 0.8687\n",
      "Epoch 379/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.7426 - val_loss: 0.8693\n",
      "Epoch 380/1000\n",
      "161/161 [==============================] - 0s 797us/step - loss: 0.7431 - val_loss: 0.8698\n",
      "Epoch 381/1000\n",
      "161/161 [==============================] - 0s 777us/step - loss: 0.7436 - val_loss: 0.8704\n",
      "Epoch 382/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.7441 - val_loss: 0.8709\n",
      "Epoch 383/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.7446 - val_loss: 0.8714\n",
      "Epoch 384/1000\n",
      "161/161 [==============================] - 0s 778us/step - loss: 0.7451 - val_loss: 0.8719\n",
      "Epoch 385/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.7456 - val_loss: 0.8725\n",
      "Epoch 386/1000\n",
      "161/161 [==============================] - 0s 833us/step - loss: 0.7461 - val_loss: 0.8730\n",
      "Epoch 387/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.7466 - val_loss: 0.8735\n",
      "Epoch 388/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.7471 - val_loss: 0.8740\n",
      "Epoch 389/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.7476 - val_loss: 0.8745\n",
      "Epoch 390/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.7481 - val_loss: 0.8750\n",
      "Epoch 391/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.7486 - val_loss: 0.8755\n",
      "Epoch 392/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.7490 - val_loss: 0.8760\n",
      "Epoch 393/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.7495 - val_loss: 0.8765\n",
      "Epoch 394/1000\n",
      "161/161 [==============================] - 0s 851us/step - loss: 0.7500 - val_loss: 0.8769\n",
      "Epoch 395/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.7504 - val_loss: 0.8774\n",
      "Epoch 396/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.7509 - val_loss: 0.8779\n",
      "Epoch 397/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.7514 - val_loss: 0.8784\n",
      "Epoch 398/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.7518 - val_loss: 0.8788\n",
      "Epoch 399/1000\n",
      "161/161 [==============================] - 0s 802us/step - loss: 0.7523 - val_loss: 0.8793\n",
      "Epoch 400/1000\n",
      "161/161 [==============================] - 0s 775us/step - loss: 0.7527 - val_loss: 0.8797\n",
      "Epoch 401/1000\n",
      "161/161 [==============================] - 0s 854us/step - loss: 0.7532 - val_loss: 0.8802\n",
      "Epoch 402/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.7536 - val_loss: 0.8806\n",
      "Epoch 403/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.7541 - val_loss: 0.8810\n",
      "Epoch 404/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.7545 - val_loss: 0.8815\n",
      "Epoch 405/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.7550 - val_loss: 0.8819\n",
      "Epoch 406/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.7554 - val_loss: 0.8823\n",
      "Epoch 407/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.7558 - val_loss: 0.8827\n",
      "Epoch 408/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.7563 - val_loss: 0.8831\n",
      "Epoch 409/1000\n",
      "161/161 [==============================] - 0s 863us/step - loss: 0.7567 - val_loss: 0.8835\n",
      "Epoch 410/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.7571 - val_loss: 0.8839\n",
      "Epoch 411/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.7575 - val_loss: 0.8843\n",
      "Epoch 412/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.7580 - val_loss: 0.8847\n",
      "Epoch 413/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.7584 - val_loss: 0.8851\n",
      "Epoch 414/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.7588 - val_loss: 0.8855\n",
      "Epoch 415/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.7592 - val_loss: 0.8859\n",
      "Epoch 416/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.7596 - val_loss: 0.8863\n",
      "Epoch 417/1000\n",
      "161/161 [==============================] - 0s 868us/step - loss: 0.7600 - val_loss: 0.8866\n",
      "Epoch 418/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.7605 - val_loss: 0.8870\n",
      "Epoch 419/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.7609 - val_loss: 0.8874\n",
      "Epoch 420/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.7613 - val_loss: 0.8877\n",
      "Epoch 421/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.7617 - val_loss: 0.8881\n",
      "Epoch 422/1000\n",
      "161/161 [==============================] - 0s 775us/step - loss: 0.7621 - val_loss: 0.8884\n",
      "Epoch 423/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.7625 - val_loss: 0.8888\n",
      "Epoch 424/1000\n",
      "161/161 [==============================] - 0s 849us/step - loss: 0.7629 - val_loss: 0.8891\n",
      "Epoch 425/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.7633 - val_loss: 0.8895\n",
      "Epoch 426/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.7637 - val_loss: 0.8898\n",
      "Epoch 427/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.7641 - val_loss: 0.8902\n",
      "Epoch 428/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.7645 - val_loss: 0.8905\n",
      "Epoch 429/1000\n",
      "161/161 [==============================] - 0s 775us/step - loss: 0.7649 - val_loss: 0.8908\n",
      "Epoch 430/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.7653 - val_loss: 0.8912\n",
      "Epoch 431/1000\n",
      "161/161 [==============================] - 0s 861us/step - loss: 0.7657 - val_loss: 0.8915\n",
      "Epoch 432/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.7661 - val_loss: 0.8918\n",
      "Epoch 433/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.7665 - val_loss: 0.8921\n",
      "Epoch 434/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.7669 - val_loss: 0.8925\n",
      "Epoch 435/1000\n",
      "161/161 [==============================] - 0s 799us/step - loss: 0.7674 - val_loss: 0.8928\n",
      "Epoch 436/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.7678 - val_loss: 0.8931\n",
      "Epoch 437/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.7682 - val_loss: 0.8934\n",
      "Epoch 438/1000\n",
      "161/161 [==============================] - 0s 855us/step - loss: 0.7686 - val_loss: 0.8938\n",
      "Epoch 439/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.7690 - val_loss: 0.8941\n",
      "Epoch 440/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.7694 - val_loss: 0.8944\n",
      "Epoch 441/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.7698 - val_loss: 0.8947\n",
      "Epoch 442/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.7703 - val_loss: 0.8951\n",
      "Epoch 443/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.7707 - val_loss: 0.8954\n",
      "Epoch 444/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.7711 - val_loss: 0.8957\n",
      "Epoch 445/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.7716 - val_loss: 0.8961\n",
      "Epoch 446/1000\n",
      "161/161 [==============================] - 0s 846us/step - loss: 0.7720 - val_loss: 0.8964\n",
      "Epoch 447/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.7724 - val_loss: 0.8968\n",
      "Epoch 448/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.7729 - val_loss: 0.8971\n",
      "Epoch 449/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.7733 - val_loss: 0.8975\n",
      "Epoch 450/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.7738 - val_loss: 0.8978\n",
      "Epoch 451/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.7742 - val_loss: 0.8982\n",
      "Epoch 452/1000\n",
      "161/161 [==============================] - 0s 799us/step - loss: 0.7747 - val_loss: 0.8986\n",
      "Epoch 453/1000\n",
      "161/161 [==============================] - 0s 834us/step - loss: 0.7751 - val_loss: 0.8989\n",
      "Epoch 454/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.7756 - val_loss: 0.8993\n",
      "Epoch 455/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.7761 - val_loss: 0.8997\n",
      "Epoch 456/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.7765 - val_loss: 0.9001\n",
      "Epoch 457/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.7770 - val_loss: 0.9005\n",
      "Epoch 458/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.7775 - val_loss: 0.9010\n",
      "Epoch 459/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.7780 - val_loss: 0.9014\n",
      "Epoch 460/1000\n",
      "161/161 [==============================] - 0s 837us/step - loss: 0.7784 - val_loss: 0.9018\n",
      "Epoch 461/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.7789 - val_loss: 0.9023\n",
      "Epoch 462/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.7794 - val_loss: 0.9028\n",
      "Epoch 463/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.7798 - val_loss: 0.9033\n",
      "Epoch 464/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.7803 - val_loss: 0.9038\n",
      "Epoch 465/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.7808 - val_loss: 0.9043\n",
      "Epoch 466/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.7812 - val_loss: 0.9048\n",
      "Epoch 467/1000\n",
      "161/161 [==============================] - 0s 847us/step - loss: 0.7817 - val_loss: 0.9053\n",
      "Epoch 468/1000\n",
      "161/161 [==============================] - 0s 810us/step - loss: 0.7821 - val_loss: 0.9059\n",
      "Epoch 469/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.7826 - val_loss: 0.9065\n",
      "Epoch 470/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.7830 - val_loss: 0.9071\n",
      "Epoch 471/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.7834 - val_loss: 0.9077\n",
      "Epoch 472/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.7838 - val_loss: 0.9083\n",
      "Epoch 473/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.7842 - val_loss: 0.9089\n",
      "Epoch 474/1000\n",
      "161/161 [==============================] - 0s 854us/step - loss: 0.7846 - val_loss: 0.9096\n",
      "Epoch 475/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.7850 - val_loss: 0.9102\n",
      "Epoch 476/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.7853 - val_loss: 0.9109\n",
      "Epoch 477/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.7857 - val_loss: 0.9116\n",
      "Epoch 478/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.7860 - val_loss: 0.9123\n",
      "Epoch 479/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.7863 - val_loss: 0.9130\n",
      "Epoch 480/1000\n",
      "161/161 [==============================] - 0s 841us/step - loss: 0.7866 - val_loss: 0.9137\n",
      "Epoch 481/1000\n",
      "161/161 [==============================] - 0s 775us/step - loss: 0.7869 - val_loss: 0.9144\n",
      "Epoch 482/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.7872 - val_loss: 0.9151\n",
      "Epoch 483/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.7874 - val_loss: 0.9158\n",
      "Epoch 484/1000\n",
      "161/161 [==============================] - 0s 799us/step - loss: 0.7876 - val_loss: 0.9166\n",
      "Epoch 485/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.7878 - val_loss: 0.9173\n",
      "Epoch 486/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.7880 - val_loss: 0.9180\n",
      "Epoch 487/1000\n",
      "161/161 [==============================] - 0s 844us/step - loss: 0.7882 - val_loss: 0.9187\n",
      "Epoch 488/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.7883 - val_loss: 0.9194\n",
      "Epoch 489/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.7885 - val_loss: 0.9201\n",
      "Epoch 490/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.7886 - val_loss: 0.9208\n",
      "Epoch 491/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.7887 - val_loss: 0.9214\n",
      "Epoch 492/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.7887 - val_loss: 0.9221\n",
      "Epoch 493/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.7888 - val_loss: 0.9227\n",
      "Epoch 494/1000\n",
      "161/161 [==============================] - 0s 847us/step - loss: 0.7888 - val_loss: 0.9233\n",
      "Epoch 495/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.7888 - val_loss: 0.9239\n",
      "Epoch 496/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.7888 - val_loss: 0.9245\n",
      "Epoch 497/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.7888 - val_loss: 0.9250\n",
      "Epoch 498/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.7887 - val_loss: 0.9255\n",
      "Epoch 499/1000\n",
      "161/161 [==============================] - 0s 793us/step - loss: 0.7887 - val_loss: 0.9260\n",
      "Epoch 500/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.7886 - val_loss: 0.9265\n",
      "Epoch 501/1000\n",
      "161/161 [==============================] - 0s 848us/step - loss: 0.7885 - val_loss: 0.9270\n",
      "Epoch 502/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.7884 - val_loss: 0.9274\n",
      "Epoch 503/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.7883 - val_loss: 0.9278\n",
      "Epoch 504/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.7882 - val_loss: 0.9282\n",
      "Epoch 505/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.7881 - val_loss: 0.9286\n",
      "Epoch 506/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.7879 - val_loss: 0.9289\n",
      "Epoch 507/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.7878 - val_loss: 0.9292\n",
      "Epoch 508/1000\n",
      "161/161 [==============================] - 0s 863us/step - loss: 0.7876 - val_loss: 0.9295\n",
      "Epoch 509/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.7874 - val_loss: 0.9298\n",
      "Epoch 510/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.7872 - val_loss: 0.9300\n",
      "Epoch 511/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.7870 - val_loss: 0.9303\n",
      "Epoch 512/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.7868 - val_loss: 0.9305\n",
      "Epoch 513/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.7865 - val_loss: 0.9306\n",
      "Epoch 514/1000\n",
      "161/161 [==============================] - 0s 879us/step - loss: 0.7863 - val_loss: 0.9308\n",
      "Epoch 515/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.7860 - val_loss: 0.9310\n",
      "Epoch 516/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.7858 - val_loss: 0.9311\n",
      "Epoch 517/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.7855 - val_loss: 0.9312\n",
      "Epoch 518/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.7852 - val_loss: 0.9313\n",
      "Epoch 519/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.7849 - val_loss: 0.9313\n",
      "Epoch 520/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.7846 - val_loss: 0.9314\n",
      "Epoch 521/1000\n",
      "161/161 [==============================] - 0s 863us/step - loss: 0.7843 - val_loss: 0.9314\n",
      "Epoch 522/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.7840 - val_loss: 0.9314\n",
      "Epoch 523/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.7837 - val_loss: 0.9314\n",
      "Epoch 524/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.7833 - val_loss: 0.9314\n",
      "Epoch 525/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.7829 - val_loss: 0.9313\n",
      "Epoch 526/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.7826 - val_loss: 0.9312\n",
      "Epoch 527/1000\n",
      "161/161 [==============================] - 0s 829us/step - loss: 0.7822 - val_loss: 0.9312\n",
      "Epoch 528/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.7818 - val_loss: 0.9310\n",
      "Epoch 529/1000\n",
      "161/161 [==============================] - 0s 800us/step - loss: 0.7814 - val_loss: 0.9309\n",
      "Epoch 530/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.7809 - val_loss: 0.9308\n",
      "Epoch 531/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.7805 - val_loss: 0.9306\n",
      "Epoch 532/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.7801 - val_loss: 0.9304\n",
      "Epoch 533/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.7796 - val_loss: 0.9302\n",
      "Epoch 534/1000\n",
      "161/161 [==============================] - 0s 832us/step - loss: 0.7791 - val_loss: 0.9300\n",
      "Epoch 535/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.7786 - val_loss: 0.9297\n",
      "Epoch 536/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.7781 - val_loss: 0.9295\n",
      "Epoch 537/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.7776 - val_loss: 0.9292\n",
      "Epoch 538/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.7770 - val_loss: 0.9289\n",
      "Epoch 539/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.7764 - val_loss: 0.9285\n",
      "Epoch 540/1000\n",
      "161/161 [==============================] - 0s 855us/step - loss: 0.7759 - val_loss: 0.9282\n",
      "Epoch 541/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.7753 - val_loss: 0.9278\n",
      "Epoch 542/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.7746 - val_loss: 0.9274\n",
      "Epoch 543/1000\n",
      "161/161 [==============================] - 0s 793us/step - loss: 0.7740 - val_loss: 0.9270\n",
      "Epoch 544/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.7733 - val_loss: 0.9265\n",
      "Epoch 545/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.7726 - val_loss: 0.9261\n",
      "Epoch 546/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.7719 - val_loss: 0.9256\n",
      "Epoch 547/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.7712 - val_loss: 0.9251\n",
      "Epoch 548/1000\n",
      "161/161 [==============================] - 0s 851us/step - loss: 0.7704 - val_loss: 0.9245\n",
      "Epoch 549/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.7696 - val_loss: 0.9239\n",
      "Epoch 550/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.7688 - val_loss: 0.9233\n",
      "Epoch 551/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.7680 - val_loss: 0.9227\n",
      "Epoch 552/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.7671 - val_loss: 0.9221\n",
      "Epoch 553/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.7662 - val_loss: 0.9214\n",
      "Epoch 554/1000\n",
      "161/161 [==============================] - 0s 850us/step - loss: 0.7652 - val_loss: 0.9207\n",
      "Epoch 555/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.7643 - val_loss: 0.9199\n",
      "Epoch 556/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.7633 - val_loss: 0.9192\n",
      "Epoch 557/1000\n",
      "161/161 [==============================] - 0s 803us/step - loss: 0.7622 - val_loss: 0.9184\n",
      "Epoch 558/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.7612 - val_loss: 0.9175\n",
      "Epoch 559/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.7601 - val_loss: 0.9167\n",
      "Epoch 560/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.7589 - val_loss: 0.9158\n",
      "Epoch 561/1000\n",
      "161/161 [==============================] - 0s 858us/step - loss: 0.7578 - val_loss: 0.9149\n",
      "Epoch 562/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.7565 - val_loss: 0.9139\n",
      "Epoch 563/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.7553 - val_loss: 0.9129\n",
      "Epoch 564/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.7540 - val_loss: 0.9119\n",
      "Epoch 565/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.7527 - val_loss: 0.9109\n",
      "Epoch 566/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.7513 - val_loss: 0.9098\n",
      "Epoch 567/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.7499 - val_loss: 0.9087\n",
      "Epoch 568/1000\n",
      "161/161 [==============================] - 0s 846us/step - loss: 0.7484 - val_loss: 0.9076\n",
      "Epoch 569/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.7469 - val_loss: 0.9065\n",
      "Epoch 570/1000\n",
      "161/161 [==============================] - 0s 799us/step - loss: 0.7454 - val_loss: 0.9053\n",
      "Epoch 571/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.7438 - val_loss: 0.9041\n",
      "Epoch 572/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.7422 - val_loss: 0.9029\n",
      "Epoch 573/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.7406 - val_loss: 0.9016\n",
      "Epoch 574/1000\n",
      "161/161 [==============================] - 0s 815us/step - loss: 0.7389 - val_loss: 0.9004\n",
      "Epoch 575/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.7372 - val_loss: 0.8991\n",
      "Epoch 576/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.7355 - val_loss: 0.8978\n",
      "Epoch 577/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.7338 - val_loss: 0.8965\n",
      "Epoch 578/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.7320 - val_loss: 0.8953\n",
      "Epoch 579/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.7303 - val_loss: 0.8940\n",
      "Epoch 580/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.7285 - val_loss: 0.8927\n",
      "Epoch 581/1000\n",
      "161/161 [==============================] - 0s 850us/step - loss: 0.7268 - val_loss: 0.8914\n",
      "Epoch 582/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.7250 - val_loss: 0.8901\n",
      "Epoch 583/1000\n",
      "161/161 [==============================] - 0s 784us/step - loss: 0.7233 - val_loss: 0.8888\n",
      "Epoch 584/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.7216 - val_loss: 0.8875\n",
      "Epoch 585/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.7198 - val_loss: 0.8862\n",
      "Epoch 586/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.7182 - val_loss: 0.8850\n",
      "Epoch 587/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.7165 - val_loss: 0.8837\n",
      "Epoch 588/1000\n",
      "161/161 [==============================] - 0s 850us/step - loss: 0.7149 - val_loss: 0.8825\n",
      "Epoch 589/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.7133 - val_loss: 0.8813\n",
      "Epoch 590/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.7118 - val_loss: 0.8801\n",
      "Epoch 591/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.7102 - val_loss: 0.8790\n",
      "Epoch 592/1000\n",
      "161/161 [==============================] - 0s 777us/step - loss: 0.7088 - val_loss: 0.8778\n",
      "Epoch 593/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.7073 - val_loss: 0.8767\n",
      "Epoch 594/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.7059 - val_loss: 0.8756\n",
      "Epoch 595/1000\n",
      "161/161 [==============================] - 0s 889us/step - loss: 0.7045 - val_loss: 0.8745\n",
      "Epoch 596/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.7032 - val_loss: 0.8734\n",
      "Epoch 597/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.7018 - val_loss: 0.8723\n",
      "Epoch 598/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.7005 - val_loss: 0.8712\n",
      "Epoch 599/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6992 - val_loss: 0.8701\n",
      "Epoch 600/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.6980 - val_loss: 0.8691\n",
      "Epoch 601/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6967 - val_loss: 0.8680\n",
      "Epoch 602/1000\n",
      "161/161 [==============================] - 0s 852us/step - loss: 0.6955 - val_loss: 0.8670\n",
      "Epoch 603/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6943 - val_loss: 0.8660\n",
      "Epoch 604/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.6931 - val_loss: 0.8649\n",
      "Epoch 605/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.6920 - val_loss: 0.8639\n",
      "Epoch 606/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.6908 - val_loss: 0.8629\n",
      "Epoch 607/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.6897 - val_loss: 0.8619\n",
      "Epoch 608/1000\n",
      "161/161 [==============================] - 0s 790us/step - loss: 0.6886 - val_loss: 0.8609\n",
      "Epoch 609/1000\n",
      "161/161 [==============================] - 0s 845us/step - loss: 0.6874 - val_loss: 0.8599\n",
      "Epoch 610/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.6864 - val_loss: 0.8589\n",
      "Epoch 611/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.6853 - val_loss: 0.8579\n",
      "Epoch 612/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6842 - val_loss: 0.8570\n",
      "Epoch 613/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6832 - val_loss: 0.8560\n",
      "Epoch 614/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.6821 - val_loss: 0.8551\n",
      "Epoch 615/1000\n",
      "161/161 [==============================] - 0s 855us/step - loss: 0.6811 - val_loss: 0.8541\n",
      "Epoch 616/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.6801 - val_loss: 0.8532\n",
      "Epoch 617/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6791 - val_loss: 0.8523\n",
      "Epoch 618/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.6781 - val_loss: 0.8513\n",
      "Epoch 619/1000\n",
      "161/161 [==============================] - 0s 800us/step - loss: 0.6771 - val_loss: 0.8504\n",
      "Epoch 620/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.6762 - val_loss: 0.8495\n",
      "Epoch 621/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.6752 - val_loss: 0.8486\n",
      "Epoch 622/1000\n",
      "161/161 [==============================] - 0s 859us/step - loss: 0.6743 - val_loss: 0.8478\n",
      "Epoch 623/1000\n",
      "161/161 [==============================] - 0s 775us/step - loss: 0.6734 - val_loss: 0.8469\n",
      "Epoch 624/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.6725 - val_loss: 0.8460\n",
      "Epoch 625/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.6716 - val_loss: 0.8452\n",
      "Epoch 626/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.6707 - val_loss: 0.8443\n",
      "Epoch 627/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.6699 - val_loss: 0.8435\n",
      "Epoch 628/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.6690 - val_loss: 0.8427\n",
      "Epoch 629/1000\n",
      "161/161 [==============================] - 0s 845us/step - loss: 0.6682 - val_loss: 0.8419\n",
      "Epoch 630/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6673 - val_loss: 0.8411\n",
      "Epoch 631/1000\n",
      "161/161 [==============================] - 0s 794us/step - loss: 0.6665 - val_loss: 0.8403\n",
      "Epoch 632/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6657 - val_loss: 0.8395\n",
      "Epoch 633/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.6649 - val_loss: 0.8387\n",
      "Epoch 634/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.6641 - val_loss: 0.8380\n",
      "Epoch 635/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.6634 - val_loss: 0.8372\n",
      "Epoch 636/1000\n",
      "161/161 [==============================] - 0s 857us/step - loss: 0.6626 - val_loss: 0.8365\n",
      "Epoch 637/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.6618 - val_loss: 0.8358\n",
      "Epoch 638/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.6611 - val_loss: 0.8351\n",
      "Epoch 639/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.6604 - val_loss: 0.8344\n",
      "Epoch 640/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.6597 - val_loss: 0.8337\n",
      "Epoch 641/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.6589 - val_loss: 0.8330\n",
      "Epoch 642/1000\n",
      "161/161 [==============================] - 0s 865us/step - loss: 0.6583 - val_loss: 0.8323\n",
      "Epoch 643/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6576 - val_loss: 0.8316\n",
      "Epoch 644/1000\n",
      "161/161 [==============================] - 0s 775us/step - loss: 0.6569 - val_loss: 0.8310\n",
      "Epoch 645/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.6562 - val_loss: 0.8303\n",
      "Epoch 646/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.6556 - val_loss: 0.8297\n",
      "Epoch 647/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.6549 - val_loss: 0.8291\n",
      "Epoch 648/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.6543 - val_loss: 0.8285\n",
      "Epoch 649/1000\n",
      "161/161 [==============================] - 0s 860us/step - loss: 0.6536 - val_loss: 0.8279\n",
      "Epoch 650/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.6530 - val_loss: 0.8273\n",
      "Epoch 651/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.6524 - val_loss: 0.8267\n",
      "Epoch 652/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.6518 - val_loss: 0.8261\n",
      "Epoch 653/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.6512 - val_loss: 0.8255\n",
      "Epoch 654/1000\n",
      "161/161 [==============================] - 0s 796us/step - loss: 0.6506 - val_loss: 0.8250\n",
      "Epoch 655/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6501 - val_loss: 0.8244\n",
      "Epoch 656/1000\n",
      "161/161 [==============================] - 0s 842us/step - loss: 0.6495 - val_loss: 0.8239\n",
      "Epoch 657/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6489 - val_loss: 0.8233\n",
      "Epoch 658/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.6484 - val_loss: 0.8228\n",
      "Epoch 659/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.6478 - val_loss: 0.8223\n",
      "Epoch 660/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.6473 - val_loss: 0.8218\n",
      "Epoch 661/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6468 - val_loss: 0.8213\n",
      "Epoch 662/1000\n",
      "161/161 [==============================] - 0s 840us/step - loss: 0.6462 - val_loss: 0.8208\n",
      "Epoch 663/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.6457 - val_loss: 0.8203\n",
      "Epoch 664/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6452 - val_loss: 0.8198\n",
      "Epoch 665/1000\n",
      "161/161 [==============================] - 0s 802us/step - loss: 0.6447 - val_loss: 0.8193\n",
      "Epoch 666/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.6442 - val_loss: 0.8189\n",
      "Epoch 667/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6437 - val_loss: 0.8184\n",
      "Epoch 668/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.6433 - val_loss: 0.8180\n",
      "Epoch 669/1000\n",
      "161/161 [==============================] - 0s 859us/step - loss: 0.6428 - val_loss: 0.8175\n",
      "Epoch 670/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6423 - val_loss: 0.8171\n",
      "Epoch 671/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.6418 - val_loss: 0.8166\n",
      "Epoch 672/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.6414 - val_loss: 0.8162\n",
      "Epoch 673/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.6409 - val_loss: 0.8158\n",
      "Epoch 674/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6405 - val_loss: 0.8154\n",
      "Epoch 675/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.6401 - val_loss: 0.8150\n",
      "Epoch 676/1000\n",
      "161/161 [==============================] - 0s 904us/step - loss: 0.6396 - val_loss: 0.8146\n",
      "Epoch 677/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.6392 - val_loss: 0.8142\n",
      "Epoch 678/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.6388 - val_loss: 0.8138\n",
      "Epoch 679/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6384 - val_loss: 0.8134\n",
      "Epoch 680/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.6380 - val_loss: 0.8130\n",
      "Epoch 681/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.6375 - val_loss: 0.8126\n",
      "Epoch 682/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.6371 - val_loss: 0.8123\n",
      "Epoch 683/1000\n",
      "161/161 [==============================] - 0s 871us/step - loss: 0.6368 - val_loss: 0.8119\n",
      "Epoch 684/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6364 - val_loss: 0.8115\n",
      "Epoch 685/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.6360 - val_loss: 0.8112\n",
      "Epoch 686/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.6356 - val_loss: 0.8108\n",
      "Epoch 687/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.6352 - val_loss: 0.8105\n",
      "Epoch 688/1000\n",
      "161/161 [==============================] - 0s 795us/step - loss: 0.6348 - val_loss: 0.8102\n",
      "Epoch 689/1000\n",
      "161/161 [==============================] - 0s 829us/step - loss: 0.6345 - val_loss: 0.8098\n",
      "Epoch 690/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.6341 - val_loss: 0.8095\n",
      "Epoch 691/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.6338 - val_loss: 0.8092\n",
      "Epoch 692/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.6334 - val_loss: 0.8088\n",
      "Epoch 693/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.6331 - val_loss: 0.8085\n",
      "Epoch 694/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.6327 - val_loss: 0.8082\n",
      "Epoch 695/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6324 - val_loss: 0.8079\n",
      "Epoch 696/1000\n",
      "161/161 [==============================] - 0s 862us/step - loss: 0.6320 - val_loss: 0.8076\n",
      "Epoch 697/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.6317 - val_loss: 0.8073\n",
      "Epoch 698/1000\n",
      "161/161 [==============================] - 0s 775us/step - loss: 0.6314 - val_loss: 0.8070\n",
      "Epoch 699/1000\n",
      "161/161 [==============================] - 0s 794us/step - loss: 0.6310 - val_loss: 0.8067\n",
      "Epoch 700/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.6307 - val_loss: 0.8064\n",
      "Epoch 701/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.6304 - val_loss: 0.8061\n",
      "Epoch 702/1000\n",
      "161/161 [==============================] - 0s 836us/step - loss: 0.6301 - val_loss: 0.8058\n",
      "Epoch 703/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.6297 - val_loss: 0.8055\n",
      "Epoch 704/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.6294 - val_loss: 0.8052\n",
      "Epoch 705/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.6291 - val_loss: 0.8049\n",
      "Epoch 706/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.6288 - val_loss: 0.8047\n",
      "Epoch 707/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.6285 - val_loss: 0.8044\n",
      "Epoch 708/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.6282 - val_loss: 0.8041\n",
      "Epoch 709/1000\n",
      "161/161 [==============================] - 0s 888us/step - loss: 0.6279 - val_loss: 0.8039\n",
      "Epoch 710/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.6276 - val_loss: 0.8036\n",
      "Epoch 711/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.6273 - val_loss: 0.8033\n",
      "Epoch 712/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.6271 - val_loss: 0.8031\n",
      "Epoch 713/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.6268 - val_loss: 0.8028\n",
      "Epoch 714/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.6265 - val_loss: 0.8025\n",
      "Epoch 715/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.6262 - val_loss: 0.8023\n",
      "Epoch 716/1000\n",
      "161/161 [==============================] - 0s 853us/step - loss: 0.6259 - val_loss: 0.8020\n",
      "Epoch 717/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6256 - val_loss: 0.8018\n",
      "Epoch 718/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6254 - val_loss: 0.8015\n",
      "Epoch 719/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.6251 - val_loss: 0.8013\n",
      "Epoch 720/1000\n",
      "161/161 [==============================] - 0s 800us/step - loss: 0.6248 - val_loss: 0.8011\n",
      "Epoch 721/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6246 - val_loss: 0.8008\n",
      "Epoch 722/1000\n",
      "161/161 [==============================] - 0s 848us/step - loss: 0.6243 - val_loss: 0.8006\n",
      "Epoch 723/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6240 - val_loss: 0.8003\n",
      "Epoch 724/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.6238 - val_loss: 0.8001\n",
      "Epoch 725/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.6235 - val_loss: 0.7999\n",
      "Epoch 726/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6233 - val_loss: 0.7996\n",
      "Epoch 727/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.6230 - val_loss: 0.7994\n",
      "Epoch 728/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.6228 - val_loss: 0.7992\n",
      "Epoch 729/1000\n",
      "161/161 [==============================] - 0s 855us/step - loss: 0.6225 - val_loss: 0.7989\n",
      "Epoch 730/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.6223 - val_loss: 0.7987\n",
      "Epoch 731/1000\n",
      "161/161 [==============================] - 0s 793us/step - loss: 0.6220 - val_loss: 0.7985\n",
      "Epoch 732/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.6218 - val_loss: 0.7983\n",
      "Epoch 733/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.6215 - val_loss: 0.7980\n",
      "Epoch 734/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.6213 - val_loss: 0.7978\n",
      "Epoch 735/1000\n",
      "161/161 [==============================] - 0s 842us/step - loss: 0.6211 - val_loss: 0.7976\n",
      "Epoch 736/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.6208 - val_loss: 0.7974\n",
      "Epoch 737/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.6206 - val_loss: 0.7971\n",
      "Epoch 738/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.6204 - val_loss: 0.7969\n",
      "Epoch 739/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.6201 - val_loss: 0.7967\n",
      "Epoch 740/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.6199 - val_loss: 0.7965\n",
      "Epoch 741/1000\n",
      "161/161 [==============================] - 0s 790us/step - loss: 0.6197 - val_loss: 0.7962\n",
      "Epoch 742/1000\n",
      "161/161 [==============================] - 0s 845us/step - loss: 0.6194 - val_loss: 0.7960\n",
      "Epoch 743/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.6192 - val_loss: 0.7958\n",
      "Epoch 744/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.6190 - val_loss: 0.7956\n",
      "Epoch 745/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.6188 - val_loss: 0.7954\n",
      "Epoch 746/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6185 - val_loss: 0.7952\n",
      "Epoch 747/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.6183 - val_loss: 0.7949\n",
      "Epoch 748/1000\n",
      "161/161 [==============================] - 0s 853us/step - loss: 0.6181 - val_loss: 0.7947\n",
      "Epoch 749/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.6179 - val_loss: 0.7945\n",
      "Epoch 750/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6177 - val_loss: 0.7943\n",
      "Epoch 751/1000\n",
      "161/161 [==============================] - 0s 804us/step - loss: 0.6174 - val_loss: 0.7941\n",
      "Epoch 752/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6172 - val_loss: 0.7938\n",
      "Epoch 753/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.6170 - val_loss: 0.7936\n",
      "Epoch 754/1000\n",
      "161/161 [==============================] - 0s 861us/step - loss: 0.6168 - val_loss: 0.7934\n",
      "Epoch 755/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6166 - val_loss: 0.7932\n",
      "Epoch 756/1000\n",
      "161/161 [==============================] - 0s 776us/step - loss: 0.6164 - val_loss: 0.7930\n",
      "Epoch 757/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.6162 - val_loss: 0.7928\n",
      "Epoch 758/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.6160 - val_loss: 0.7925\n",
      "Epoch 759/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.6157 - val_loss: 0.7923\n",
      "Epoch 760/1000\n",
      "161/161 [==============================] - 0s 844us/step - loss: 0.6155 - val_loss: 0.7921\n",
      "Epoch 761/1000\n",
      "161/161 [==============================] - 0s 803us/step - loss: 0.6153 - val_loss: 0.7919\n",
      "Epoch 762/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.6151 - val_loss: 0.7917\n",
      "Epoch 763/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.6149 - val_loss: 0.7914\n",
      "Epoch 764/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.6147 - val_loss: 0.7912\n",
      "Epoch 765/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.6145 - val_loss: 0.7910\n",
      "Epoch 766/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.6143 - val_loss: 0.7908\n",
      "Epoch 767/1000\n",
      "161/161 [==============================] - 0s 847us/step - loss: 0.6141 - val_loss: 0.7906\n",
      "Epoch 768/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.6139 - val_loss: 0.7903\n",
      "Epoch 769/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.6137 - val_loss: 0.7901\n",
      "Epoch 770/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6135 - val_loss: 0.7899\n",
      "Epoch 771/1000\n",
      "161/161 [==============================] - 0s 794us/step - loss: 0.6133 - val_loss: 0.7897\n",
      "Epoch 772/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.6131 - val_loss: 0.7894\n",
      "Epoch 773/1000\n",
      "161/161 [==============================] - 0s 843us/step - loss: 0.6129 - val_loss: 0.7892\n",
      "Epoch 774/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.6127 - val_loss: 0.7890\n",
      "Epoch 775/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.6125 - val_loss: 0.7887\n",
      "Epoch 776/1000\n",
      "161/161 [==============================] - 0s 775us/step - loss: 0.6123 - val_loss: 0.7885\n",
      "Epoch 777/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.6121 - val_loss: 0.7883\n",
      "Epoch 778/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6119 - val_loss: 0.7881\n",
      "Epoch 779/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.6118 - val_loss: 0.7878\n",
      "Epoch 780/1000\n",
      "161/161 [==============================] - 0s 893us/step - loss: 0.6116 - val_loss: 0.7876\n",
      "Epoch 781/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.6114 - val_loss: 0.7874\n",
      "Epoch 782/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.6112 - val_loss: 0.7871\n",
      "Epoch 783/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.6110 - val_loss: 0.7869\n",
      "Epoch 784/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6108 - val_loss: 0.7867\n",
      "Epoch 785/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.6106 - val_loss: 0.7864\n",
      "Epoch 786/1000\n",
      "161/161 [==============================] - 0s 855us/step - loss: 0.6104 - val_loss: 0.7862\n",
      "Epoch 787/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.6102 - val_loss: 0.7859\n",
      "Epoch 788/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.6100 - val_loss: 0.7857\n",
      "Epoch 789/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.6098 - val_loss: 0.7855\n",
      "Epoch 790/1000\n",
      "161/161 [==============================] - 0s 789us/step - loss: 0.6097 - val_loss: 0.7852\n",
      "Epoch 791/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.6095 - val_loss: 0.7850\n",
      "Epoch 792/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.6093 - val_loss: 0.7847\n",
      "Epoch 793/1000\n",
      "161/161 [==============================] - 0s 861us/step - loss: 0.6091 - val_loss: 0.7845\n",
      "Epoch 794/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.6089 - val_loss: 0.7843\n",
      "Epoch 795/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.6087 - val_loss: 0.7840\n",
      "Epoch 796/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.6085 - val_loss: 0.7838\n",
      "Epoch 797/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.6084 - val_loss: 0.7835\n",
      "Epoch 798/1000\n",
      "161/161 [==============================] - 0s 855us/step - loss: 0.6082 - val_loss: 0.7833\n",
      "Epoch 799/1000\n",
      "161/161 [==============================] - 0s 799us/step - loss: 0.6080 - val_loss: 0.7830\n",
      "Epoch 800/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.6078 - val_loss: 0.7828\n",
      "Epoch 801/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.6076 - val_loss: 0.7825\n",
      "Epoch 802/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.6074 - val_loss: 0.7823\n",
      "Epoch 803/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.6073 - val_loss: 0.7820\n",
      "Epoch 804/1000\n",
      "161/161 [==============================] - 0s 855us/step - loss: 0.6071 - val_loss: 0.7818\n",
      "Epoch 805/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.6069 - val_loss: 0.7815\n",
      "Epoch 806/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.6067 - val_loss: 0.7813\n",
      "Epoch 807/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6065 - val_loss: 0.7810\n",
      "Epoch 808/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.6063 - val_loss: 0.7808\n",
      "Epoch 809/1000\n",
      "161/161 [==============================] - 0s 793us/step - loss: 0.6062 - val_loss: 0.7805\n",
      "Epoch 810/1000\n",
      "161/161 [==============================] - 0s 853us/step - loss: 0.6060 - val_loss: 0.7803\n",
      "Epoch 811/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.6058 - val_loss: 0.7800\n",
      "Epoch 812/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.6056 - val_loss: 0.7798\n",
      "Epoch 813/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6054 - val_loss: 0.7795\n",
      "Epoch 814/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.6053 - val_loss: 0.7793\n",
      "Epoch 815/1000\n",
      "161/161 [==============================] - 0s 752us/step - loss: 0.6051 - val_loss: 0.7790\n",
      "Epoch 816/1000\n",
      "161/161 [==============================] - 0s 859us/step - loss: 0.6049 - val_loss: 0.7788\n",
      "Epoch 817/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.6047 - val_loss: 0.7785\n",
      "Epoch 818/1000\n",
      "161/161 [==============================] - 0s 794us/step - loss: 0.6045 - val_loss: 0.7783\n",
      "Epoch 819/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6044 - val_loss: 0.7780\n",
      "Epoch 820/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.6042 - val_loss: 0.7778\n",
      "Epoch 821/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.6040 - val_loss: 0.7775\n",
      "Epoch 822/1000\n",
      "161/161 [==============================] - 0s 854us/step - loss: 0.6038 - val_loss: 0.7773\n",
      "Epoch 823/1000\n",
      "161/161 [==============================] - 0s 752us/step - loss: 0.6036 - val_loss: 0.7770\n",
      "Epoch 824/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6035 - val_loss: 0.7768\n",
      "Epoch 825/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.6033 - val_loss: 0.7765\n",
      "Epoch 826/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.6031 - val_loss: 0.7763\n",
      "Epoch 827/1000\n",
      "161/161 [==============================] - 0s 794us/step - loss: 0.6029 - val_loss: 0.7760\n",
      "Epoch 828/1000\n",
      "161/161 [==============================] - 0s 858us/step - loss: 0.6028 - val_loss: 0.7758\n",
      "Epoch 829/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.6026 - val_loss: 0.7755\n",
      "Epoch 830/1000\n",
      "161/161 [==============================] - 0s 776us/step - loss: 0.6024 - val_loss: 0.7753\n",
      "Epoch 831/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.6022 - val_loss: 0.7751\n",
      "Epoch 832/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.6021 - val_loss: 0.7748\n",
      "Epoch 833/1000\n",
      "161/161 [==============================] - 0s 826us/step - loss: 0.6019 - val_loss: 0.7746\n",
      "Epoch 834/1000\n",
      "161/161 [==============================] - 0s 835us/step - loss: 0.6017 - val_loss: 0.7743\n",
      "Epoch 835/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.6015 - val_loss: 0.7741\n",
      "Epoch 836/1000\n",
      "161/161 [==============================] - 0s 811us/step - loss: 0.6014 - val_loss: 0.7738\n",
      "Epoch 837/1000\n",
      "161/161 [==============================] - 0s 780us/step - loss: 0.6012 - val_loss: 0.7736\n",
      "Epoch 838/1000\n",
      "161/161 [==============================] - 0s 790us/step - loss: 0.6010 - val_loss: 0.7734\n",
      "Epoch 839/1000\n",
      "161/161 [==============================] - 0s 866us/step - loss: 0.6008 - val_loss: 0.7731\n",
      "Epoch 840/1000\n",
      "161/161 [==============================] - 0s 778us/step - loss: 0.6007 - val_loss: 0.7729\n",
      "Epoch 841/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.6005 - val_loss: 0.7727\n",
      "Epoch 842/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.6003 - val_loss: 0.7724\n",
      "Epoch 843/1000\n",
      "161/161 [==============================] - 0s 776us/step - loss: 0.6001 - val_loss: 0.7722\n",
      "Epoch 844/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.6000 - val_loss: 0.7720\n",
      "Epoch 845/1000\n",
      "161/161 [==============================] - 0s 882us/step - loss: 0.5998 - val_loss: 0.7718\n",
      "Epoch 846/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5996 - val_loss: 0.7715\n",
      "Epoch 847/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5995 - val_loss: 0.7713\n",
      "Epoch 848/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5993 - val_loss: 0.7711\n",
      "Epoch 849/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.5991 - val_loss: 0.7709\n",
      "Epoch 850/1000\n",
      "161/161 [==============================] - 0s 853us/step - loss: 0.5989 - val_loss: 0.7706\n",
      "Epoch 851/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5988 - val_loss: 0.7704\n",
      "Epoch 852/1000\n",
      "161/161 [==============================] - 0s 776us/step - loss: 0.5986 - val_loss: 0.7702\n",
      "Epoch 853/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5984 - val_loss: 0.7700\n",
      "Epoch 854/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.5983 - val_loss: 0.7698\n",
      "Epoch 855/1000\n",
      "161/161 [==============================] - 0s 809us/step - loss: 0.5981 - val_loss: 0.7696\n",
      "Epoch 856/1000\n",
      "161/161 [==============================] - 0s 852us/step - loss: 0.5979 - val_loss: 0.7694\n",
      "Epoch 857/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5978 - val_loss: 0.7692\n",
      "Epoch 858/1000\n",
      "161/161 [==============================] - 0s 782us/step - loss: 0.5976 - val_loss: 0.7690\n",
      "Epoch 859/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5974 - val_loss: 0.7687\n",
      "Epoch 860/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5973 - val_loss: 0.7685\n",
      "Epoch 861/1000\n",
      "161/161 [==============================] - 0s 867us/step - loss: 0.5971 - val_loss: 0.7683\n",
      "Epoch 862/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5969 - val_loss: 0.7681\n",
      "Epoch 863/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5968 - val_loss: 0.7679\n",
      "Epoch 864/1000\n",
      "161/161 [==============================] - 0s 797us/step - loss: 0.5966 - val_loss: 0.7678\n",
      "Epoch 865/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5964 - val_loss: 0.7676\n",
      "Epoch 866/1000\n",
      "161/161 [==============================] - 0s 851us/step - loss: 0.5963 - val_loss: 0.7674\n",
      "Epoch 867/1000\n",
      "161/161 [==============================] - 0s 776us/step - loss: 0.5961 - val_loss: 0.7672\n",
      "Epoch 868/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5960 - val_loss: 0.7670\n",
      "Epoch 869/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.5958 - val_loss: 0.7668\n",
      "Epoch 870/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5956 - val_loss: 0.7666\n",
      "Epoch 871/1000\n",
      "161/161 [==============================] - 0s 855us/step - loss: 0.5955 - val_loss: 0.7664\n",
      "Epoch 872/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5953 - val_loss: 0.7662\n",
      "Epoch 873/1000\n",
      "161/161 [==============================] - 0s 787us/step - loss: 0.5951 - val_loss: 0.7661\n",
      "Epoch 874/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5950 - val_loss: 0.7659\n",
      "Epoch 875/1000\n",
      "161/161 [==============================] - 0s 776us/step - loss: 0.5948 - val_loss: 0.7657\n",
      "Epoch 876/1000\n",
      "161/161 [==============================] - 0s 853us/step - loss: 0.5947 - val_loss: 0.7655\n",
      "Epoch 877/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.5945 - val_loss: 0.7654\n",
      "Epoch 878/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5944 - val_loss: 0.7652\n",
      "Epoch 879/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5942 - val_loss: 0.7650\n",
      "Epoch 880/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.5940 - val_loss: 0.7649\n",
      "Epoch 881/1000\n",
      "161/161 [==============================] - 0s 876us/step - loss: 0.5939 - val_loss: 0.7647\n",
      "Epoch 882/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.5937 - val_loss: 0.7645\n",
      "Epoch 883/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5936 - val_loss: 0.7644\n",
      "Epoch 884/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5934 - val_loss: 0.7642\n",
      "Epoch 885/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.5933 - val_loss: 0.7640\n",
      "Epoch 886/1000\n",
      "161/161 [==============================] - 0s 858us/step - loss: 0.5931 - val_loss: 0.7639\n",
      "Epoch 887/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5930 - val_loss: 0.7637\n",
      "Epoch 888/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5928 - val_loss: 0.7636\n",
      "Epoch 889/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.5926 - val_loss: 0.7634\n",
      "Epoch 890/1000\n",
      "161/161 [==============================] - 0s 799us/step - loss: 0.5925 - val_loss: 0.7633\n",
      "Epoch 891/1000\n",
      "161/161 [==============================] - 0s 848us/step - loss: 0.5923 - val_loss: 0.7631\n",
      "Epoch 892/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5922 - val_loss: 0.7629\n",
      "Epoch 893/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5920 - val_loss: 0.7628\n",
      "Epoch 894/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5919 - val_loss: 0.7627\n",
      "Epoch 895/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5917 - val_loss: 0.7625\n",
      "Epoch 896/1000\n",
      "161/161 [==============================] - 0s 850us/step - loss: 0.5916 - val_loss: 0.7624\n",
      "Epoch 897/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5914 - val_loss: 0.7622\n",
      "Epoch 898/1000\n",
      "161/161 [==============================] - 0s 795us/step - loss: 0.5913 - val_loss: 0.7621\n",
      "Epoch 899/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.5912 - val_loss: 0.7619\n",
      "Epoch 900/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5910 - val_loss: 0.7618\n",
      "Epoch 901/1000\n",
      "161/161 [==============================] - 0s 844us/step - loss: 0.5909 - val_loss: 0.7617\n",
      "Epoch 902/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5907 - val_loss: 0.7615\n",
      "Epoch 903/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5906 - val_loss: 0.7614\n",
      "Epoch 904/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5904 - val_loss: 0.7613\n",
      "Epoch 905/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5903 - val_loss: 0.7611\n",
      "Epoch 906/1000\n",
      "161/161 [==============================] - 0s 881us/step - loss: 0.5901 - val_loss: 0.7610\n",
      "Epoch 907/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5900 - val_loss: 0.7609\n",
      "Epoch 908/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5898 - val_loss: 0.7607\n",
      "Epoch 909/1000\n",
      "161/161 [==============================] - 0s 775us/step - loss: 0.5897 - val_loss: 0.7606\n",
      "Epoch 910/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.5896 - val_loss: 0.7605\n",
      "Epoch 911/1000\n",
      "161/161 [==============================] - 0s 855us/step - loss: 0.5894 - val_loss: 0.7603\n",
      "Epoch 912/1000\n",
      "161/161 [==============================] - 0s 772us/step - loss: 0.5893 - val_loss: 0.7602\n",
      "Epoch 913/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5891 - val_loss: 0.7601\n",
      "Epoch 914/1000\n",
      "161/161 [==============================] - 0s 780us/step - loss: 0.5890 - val_loss: 0.7600\n",
      "Epoch 915/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5889 - val_loss: 0.7599\n",
      "Epoch 916/1000\n",
      "161/161 [==============================] - 0s 836us/step - loss: 0.5887 - val_loss: 0.7597\n",
      "Epoch 917/1000\n",
      "161/161 [==============================] - 0s 782us/step - loss: 0.5886 - val_loss: 0.7596\n",
      "Epoch 918/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.5884 - val_loss: 0.7595\n",
      "Epoch 919/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5883 - val_loss: 0.7594\n",
      "Epoch 920/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5882 - val_loss: 0.7593\n",
      "Epoch 921/1000\n",
      "161/161 [==============================] - 0s 861us/step - loss: 0.5880 - val_loss: 0.7592\n",
      "Epoch 922/1000\n",
      "161/161 [==============================] - 0s 808us/step - loss: 0.5879 - val_loss: 0.7590\n",
      "Epoch 923/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5877 - val_loss: 0.7589\n",
      "Epoch 924/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5876 - val_loss: 0.7588\n",
      "Epoch 925/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5875 - val_loss: 0.7587\n",
      "Epoch 926/1000\n",
      "161/161 [==============================] - 0s 861us/step - loss: 0.5873 - val_loss: 0.7586\n",
      "Epoch 927/1000\n",
      "161/161 [==============================] - 0s 776us/step - loss: 0.5872 - val_loss: 0.7585\n",
      "Epoch 928/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5871 - val_loss: 0.7584\n",
      "Epoch 929/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.5869 - val_loss: 0.7583\n",
      "Epoch 930/1000\n",
      "161/161 [==============================] - 0s 801us/step - loss: 0.5868 - val_loss: 0.7582\n",
      "Epoch 931/1000\n",
      "161/161 [==============================] - 0s 856us/step - loss: 0.5867 - val_loss: 0.7581\n",
      "Epoch 932/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5865 - val_loss: 0.7579\n",
      "Epoch 933/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5864 - val_loss: 0.7578\n",
      "Epoch 934/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5863 - val_loss: 0.7577\n",
      "Epoch 935/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.5861 - val_loss: 0.7576\n",
      "Epoch 936/1000\n",
      "161/161 [==============================] - 0s 849us/step - loss: 0.5860 - val_loss: 0.7575\n",
      "Epoch 937/1000\n",
      "161/161 [==============================] - 0s 797us/step - loss: 0.5859 - val_loss: 0.7574\n",
      "Epoch 938/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.5857 - val_loss: 0.7573\n",
      "Epoch 939/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5856 - val_loss: 0.7572\n",
      "Epoch 940/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.5855 - val_loss: 0.7571\n",
      "Epoch 941/1000\n",
      "161/161 [==============================] - 0s 853us/step - loss: 0.5853 - val_loss: 0.7570\n",
      "Epoch 942/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.5852 - val_loss: 0.7569\n",
      "Epoch 943/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5851 - val_loss: 0.7568\n",
      "Epoch 944/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.5849 - val_loss: 0.7567\n",
      "Epoch 945/1000\n",
      "161/161 [==============================] - 0s 800us/step - loss: 0.5848 - val_loss: 0.7566\n",
      "Epoch 946/1000\n",
      "161/161 [==============================] - 0s 861us/step - loss: 0.5847 - val_loss: 0.7566\n",
      "Epoch 947/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5846 - val_loss: 0.7565\n",
      "Epoch 948/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5844 - val_loss: 0.7564\n",
      "Epoch 949/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5843 - val_loss: 0.7563\n",
      "Epoch 950/1000\n",
      "161/161 [==============================] - 0s 777us/step - loss: 0.5842 - val_loss: 0.7562\n",
      "Epoch 951/1000\n",
      "161/161 [==============================] - 0s 840us/step - loss: 0.5840 - val_loss: 0.7561\n",
      "Epoch 952/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.5839 - val_loss: 0.7560\n",
      "Epoch 953/1000\n",
      "161/161 [==============================] - 0s 798us/step - loss: 0.5838 - val_loss: 0.7559\n",
      "Epoch 954/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5837 - val_loss: 0.7558\n",
      "Epoch 955/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5835 - val_loss: 0.7557\n",
      "Epoch 956/1000\n",
      "161/161 [==============================] - 0s 844us/step - loss: 0.5834 - val_loss: 0.7556\n",
      "Epoch 957/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5833 - val_loss: 0.7556\n",
      "Epoch 958/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5832 - val_loss: 0.7555\n",
      "Epoch 959/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.5830 - val_loss: 0.7554\n",
      "Epoch 960/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5829 - val_loss: 0.7553\n",
      "Epoch 961/1000\n",
      "161/161 [==============================] - 0s 907us/step - loss: 0.5828 - val_loss: 0.7552\n",
      "Epoch 962/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5827 - val_loss: 0.7551\n",
      "Epoch 963/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5825 - val_loss: 0.7550\n",
      "Epoch 964/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5824 - val_loss: 0.7550\n",
      "Epoch 965/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5823 - val_loss: 0.7549\n",
      "Epoch 966/1000\n",
      "161/161 [==============================] - 0s 851us/step - loss: 0.5822 - val_loss: 0.7548\n",
      "Epoch 967/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5820 - val_loss: 0.7547\n",
      "Epoch 968/1000\n",
      "161/161 [==============================] - 0s 788us/step - loss: 0.5819 - val_loss: 0.7546\n",
      "Epoch 969/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5818 - val_loss: 0.7545\n",
      "Epoch 970/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5817 - val_loss: 0.7545\n",
      "Epoch 971/1000\n",
      "161/161 [==============================] - 0s 849us/step - loss: 0.5816 - val_loss: 0.7544\n",
      "Epoch 972/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5814 - val_loss: 0.7543\n",
      "Epoch 973/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.5813 - val_loss: 0.7542\n",
      "Epoch 974/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5812 - val_loss: 0.7541\n",
      "Epoch 975/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5811 - val_loss: 0.7541\n",
      "Epoch 976/1000\n",
      "161/161 [==============================] - 0s 885us/step - loss: 0.5809 - val_loss: 0.7540\n",
      "Epoch 977/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.5808 - val_loss: 0.7539\n",
      "Epoch 978/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5807 - val_loss: 0.7538\n",
      "Epoch 979/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5806 - val_loss: 0.7538\n",
      "Epoch 980/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5805 - val_loss: 0.7537\n",
      "Epoch 981/1000\n",
      "161/161 [==============================] - 0s 850us/step - loss: 0.5803 - val_loss: 0.7536\n",
      "Epoch 982/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5802 - val_loss: 0.7535\n",
      "Epoch 983/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.5801 - val_loss: 0.7535\n",
      "Epoch 984/1000\n",
      "161/161 [==============================] - 0s 784us/step - loss: 0.5800 - val_loss: 0.7534\n",
      "Epoch 985/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5799 - val_loss: 0.7533\n",
      "Epoch 986/1000\n",
      "161/161 [==============================] - 0s 851us/step - loss: 0.5797 - val_loss: 0.7532\n",
      "Epoch 987/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5796 - val_loss: 0.7532\n",
      "Epoch 988/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5795 - val_loss: 0.7531\n",
      "Epoch 989/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5794 - val_loss: 0.7530\n",
      "Epoch 990/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5793 - val_loss: 0.7529\n",
      "Epoch 991/1000\n",
      "161/161 [==============================] - 0s 858us/step - loss: 0.5792 - val_loss: 0.7529\n",
      "Epoch 992/1000\n",
      "161/161 [==============================] - 0s 793us/step - loss: 0.5790 - val_loss: 0.7528\n",
      "Epoch 993/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5789 - val_loss: 0.7527\n",
      "Epoch 994/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.5788 - val_loss: 0.7527\n",
      "Epoch 995/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.5787 - val_loss: 0.7526\n",
      "Epoch 996/1000\n",
      "161/161 [==============================] - 0s 846us/step - loss: 0.5786 - val_loss: 0.7525\n",
      "Epoch 997/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.5785 - val_loss: 0.7524\n",
      "Epoch 998/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5783 - val_loss: 0.7524\n",
      "Epoch 999/1000\n",
      "161/161 [==============================] - 0s 789us/step - loss: 0.5782 - val_loss: 0.7523\n",
      "Epoch 1000/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5781 - val_loss: 0.7522\n"
     ]
    }
   ],
   "source": [
    "epocas = 1000\n",
    "history = modelo3.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    batch_size=lote,\n",
    "    epochs=epocas,\n",
    "    shuffle=False,\n",
    "    validation_data=(x_val, y_val),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='center')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
