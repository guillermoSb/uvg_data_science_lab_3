{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diesel_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-76576.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86033.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-137814.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>114863.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-54753.87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   diesel_diff\n",
       "0    -76576.19\n",
       "1     86033.88\n",
       "2   -137814.26\n",
       "3    114863.71\n",
       "4    -54753.87"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_diesel = pd.read_csv('data/data_diesel_diff.csv', sep=',')\n",
    "data_diesel = data_diesel.drop(['anio', 'mes'], axis=1)\n",
    "data_diesel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjuntos de entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data_diesel_scaled = scaler.fit_transform(data_diesel) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    }
   ],
   "source": [
    "# calculando los indices de particionamiento\n",
    "entrenamiento = round(0.6 * len(data_diesel_scaled))\n",
    "val_prueba = round(0.2 * len(data_diesel_scaled))\n",
    "\n",
    "# Particionando los datos\n",
    "train = data_diesel_scaled[:entrenamiento]\n",
    "validation = data_diesel_scaled[entrenamiento:entrenamiento+val_prueba]\n",
    "test = data_diesel_scaled[entrenamiento+val_prueba:]\n",
    "\n",
    "train = np.insert(train, 0, 0)\n",
    "train = train.reshape(-1, 1)\n",
    "\n",
    "print(len(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervisada(serie,retrasos = 1):\n",
    "    serie_x = []\n",
    "    serie_y = []\n",
    "    for i in range(len(serie)-retrasos):\n",
    "        valor = serie[i:(i+retrasos),0]\n",
    "        valor_sig = serie[i+retrasos,0]\n",
    "        serie_x.append(valor)\n",
    "        serie_y.append(valor_sig)\n",
    "    return np.array(serie_x), np.array(serie_y)\n",
    "\n",
    "x_train,y_train = supervisada(train)\n",
    "x_val,y_val = supervisada(validation)\n",
    "x_test,y_test = supervisada(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.reshape(x_train,(x_train.shape[0],1,1))\n",
    "x_val = np.reshape(x_val, (x_val.shape[0],1,1))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0],1,1))\n",
    "len(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_10 (LSTM)              (1, 1)                    12        \n",
      "                                                                 \n",
      " dense_10 (Dense)            (1, 1)                    2         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14 (56.00 Byte)\n",
      "Trainable params: 14 (56.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelo1 = Sequential()\n",
    "lote = 1\n",
    "paso = 1\n",
    "caracteristicas = 1\n",
    "modelo1.add(LSTM(lote, batch_input_shape=(lote, paso, caracteristicas), stateful=True))\n",
    "modelo1.add(Dense(1))\n",
    "modelo1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo1.compile(loss='mean_squared_error',optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "161/161 [==============================] - 1s 2ms/step - loss: 1.0059 - val_loss: 1.2106\n",
      "Epoch 2/1000\n",
      "161/161 [==============================] - 0s 904us/step - loss: 0.9945 - val_loss: 1.2016\n",
      "Epoch 3/1000\n",
      "161/161 [==============================] - 0s 872us/step - loss: 0.9907 - val_loss: 1.1979\n",
      "Epoch 4/1000\n",
      "161/161 [==============================] - 0s 870us/step - loss: 0.9904 - val_loss: 1.1968\n",
      "Epoch 5/1000\n",
      "161/161 [==============================] - 0s 882us/step - loss: 0.9926 - val_loss: 1.1975\n",
      "Epoch 6/1000\n",
      "161/161 [==============================] - 0s 863us/step - loss: 0.9971 - val_loss: 1.1999\n",
      "Epoch 7/1000\n",
      "161/161 [==============================] - 0s 887us/step - loss: 1.0043 - val_loss: 1.2043\n",
      "Epoch 8/1000\n",
      "161/161 [==============================] - 0s 864us/step - loss: 1.0146 - val_loss: 1.2111\n",
      "Epoch 9/1000\n",
      "161/161 [==============================] - 0s 847us/step - loss: 1.0274 - val_loss: 1.2193\n",
      "Epoch 10/1000\n",
      "161/161 [==============================] - 0s 844us/step - loss: 1.0407 - val_loss: 1.2271\n",
      "Epoch 11/1000\n",
      "161/161 [==============================] - 0s 841us/step - loss: 1.0510 - val_loss: 1.2316\n",
      "Epoch 12/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 1.0567 - val_loss: 1.2323\n",
      "Epoch 13/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 1.0579 - val_loss: 1.2295\n",
      "Epoch 14/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 1.0554 - val_loss: 1.2238\n",
      "Epoch 15/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 1.0499 - val_loss: 1.2156\n",
      "Epoch 16/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 1.0420 - val_loss: 1.2057\n",
      "Epoch 17/1000\n",
      "161/161 [==============================] - 0s 746us/step - loss: 1.0323 - val_loss: 1.1942\n",
      "Epoch 18/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 1.0212 - val_loss: 1.1815\n",
      "Epoch 19/1000\n",
      "161/161 [==============================] - 0s 752us/step - loss: 1.0090 - val_loss: 1.1677\n",
      "Epoch 20/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.9957 - val_loss: 1.1530\n",
      "Epoch 21/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.9816 - val_loss: 1.1375\n",
      "Epoch 22/1000\n",
      "161/161 [==============================] - 0s 752us/step - loss: 0.9667 - val_loss: 1.1212\n",
      "Epoch 23/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.9510 - val_loss: 1.1041\n",
      "Epoch 24/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.9347 - val_loss: 1.0864\n",
      "Epoch 25/1000\n",
      "161/161 [==============================] - 0s 752us/step - loss: 0.9178 - val_loss: 1.0682\n",
      "Epoch 26/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.9004 - val_loss: 1.0496\n",
      "Epoch 27/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.8828 - val_loss: 1.0306\n",
      "Epoch 28/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.8650 - val_loss: 1.0114\n",
      "Epoch 29/1000\n",
      "161/161 [==============================] - 0s 790us/step - loss: 0.8473 - val_loss: 0.9923\n",
      "Epoch 30/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.8298 - val_loss: 0.9734\n",
      "Epoch 31/1000\n",
      "161/161 [==============================] - 0s 752us/step - loss: 0.8128 - val_loss: 0.9550\n",
      "Epoch 32/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.7965 - val_loss: 0.9374\n",
      "Epoch 33/1000\n",
      "161/161 [==============================] - 0s 830us/step - loss: 0.7810 - val_loss: 0.9208\n",
      "Epoch 34/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.7665 - val_loss: 0.9055\n",
      "Epoch 35/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.7532 - val_loss: 0.8914\n",
      "Epoch 36/1000\n",
      "161/161 [==============================] - 0s 815us/step - loss: 0.7409 - val_loss: 0.8788\n",
      "Epoch 37/1000\n",
      "161/161 [==============================] - 0s 812us/step - loss: 0.7298 - val_loss: 0.8675\n",
      "Epoch 38/1000\n",
      "161/161 [==============================] - 0s 782us/step - loss: 0.7198 - val_loss: 0.8575\n",
      "Epoch 39/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.7108 - val_loss: 0.8488\n",
      "Epoch 40/1000\n",
      "161/161 [==============================] - 0s 752us/step - loss: 0.7028 - val_loss: 0.8412\n",
      "Epoch 41/1000\n",
      "161/161 [==============================] - 0s 751us/step - loss: 0.6957 - val_loss: 0.8346\n",
      "Epoch 42/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.6893 - val_loss: 0.8288\n",
      "Epoch 43/1000\n",
      "161/161 [==============================] - 0s 752us/step - loss: 0.6837 - val_loss: 0.8239\n",
      "Epoch 44/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.6787 - val_loss: 0.8197\n",
      "Epoch 45/1000\n",
      "161/161 [==============================] - 0s 749us/step - loss: 0.6743 - val_loss: 0.8160\n",
      "Epoch 46/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.6704 - val_loss: 0.8129\n",
      "Epoch 47/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.6670 - val_loss: 0.8102\n",
      "Epoch 48/1000\n",
      "161/161 [==============================] - 0s 752us/step - loss: 0.6640 - val_loss: 0.8079\n",
      "Epoch 49/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.6613 - val_loss: 0.8059\n",
      "Epoch 50/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.6590 - val_loss: 0.8042\n",
      "Epoch 51/1000\n",
      "161/161 [==============================] - 0s 841us/step - loss: 0.6569 - val_loss: 0.8027\n",
      "Epoch 52/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.6550 - val_loss: 0.8014\n",
      "Epoch 53/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.6534 - val_loss: 0.8003\n",
      "Epoch 54/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.6519 - val_loss: 0.7993\n",
      "Epoch 55/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.6506 - val_loss: 0.7983\n",
      "Epoch 56/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.6493 - val_loss: 0.7975\n",
      "Epoch 57/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.6483 - val_loss: 0.7967\n",
      "Epoch 58/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.6472 - val_loss: 0.7960\n",
      "Epoch 59/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.6463 - val_loss: 0.7954\n",
      "Epoch 60/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.6455 - val_loss: 0.7947\n",
      "Epoch 61/1000\n",
      "161/161 [==============================] - 0s 746us/step - loss: 0.6447 - val_loss: 0.7942\n",
      "Epoch 62/1000\n",
      "161/161 [==============================] - 0s 752us/step - loss: 0.6439 - val_loss: 0.7936\n",
      "Epoch 63/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.6432 - val_loss: 0.7930\n",
      "Epoch 64/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.6426 - val_loss: 0.7925\n",
      "Epoch 65/1000\n",
      "161/161 [==============================] - 0s 751us/step - loss: 0.6419 - val_loss: 0.7920\n",
      "Epoch 66/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.6413 - val_loss: 0.7915\n",
      "Epoch 67/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.6408 - val_loss: 0.7910\n",
      "Epoch 68/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.6402 - val_loss: 0.7905\n",
      "Epoch 69/1000\n",
      "161/161 [==============================] - 0s 825us/step - loss: 0.6397 - val_loss: 0.7900\n",
      "Epoch 70/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.6391 - val_loss: 0.7895\n",
      "Epoch 71/1000\n",
      "161/161 [==============================] - 0s 751us/step - loss: 0.6386 - val_loss: 0.7890\n",
      "Epoch 72/1000\n",
      "161/161 [==============================] - 0s 746us/step - loss: 0.6381 - val_loss: 0.7885\n",
      "Epoch 73/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.6376 - val_loss: 0.7880\n",
      "Epoch 74/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.6371 - val_loss: 0.7875\n",
      "Epoch 75/1000\n",
      "161/161 [==============================] - 0s 749us/step - loss: 0.6367 - val_loss: 0.7871\n",
      "Epoch 76/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.6362 - val_loss: 0.7866\n",
      "Epoch 77/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.6357 - val_loss: 0.7861\n",
      "Epoch 78/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.6353 - val_loss: 0.7857\n",
      "Epoch 79/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.6348 - val_loss: 0.7852\n",
      "Epoch 80/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.6344 - val_loss: 0.7847\n",
      "Epoch 81/1000\n",
      "161/161 [==============================] - 0s 749us/step - loss: 0.6339 - val_loss: 0.7843\n",
      "Epoch 82/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.6335 - val_loss: 0.7838\n",
      "Epoch 83/1000\n",
      "161/161 [==============================] - 0s 751us/step - loss: 0.6330 - val_loss: 0.7834\n",
      "Epoch 84/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.6326 - val_loss: 0.7829\n",
      "Epoch 85/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.6321 - val_loss: 0.7825\n",
      "Epoch 86/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.6317 - val_loss: 0.7820\n",
      "Epoch 87/1000\n",
      "161/161 [==============================] - 0s 838us/step - loss: 0.6312 - val_loss: 0.7816\n",
      "Epoch 88/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.6308 - val_loss: 0.7812\n",
      "Epoch 89/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.6304 - val_loss: 0.7807\n",
      "Epoch 90/1000\n",
      "161/161 [==============================] - 0s 749us/step - loss: 0.6299 - val_loss: 0.7803\n",
      "Epoch 91/1000\n",
      "161/161 [==============================] - 0s 752us/step - loss: 0.6295 - val_loss: 0.7799\n",
      "Epoch 92/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.6290 - val_loss: 0.7794\n",
      "Epoch 93/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.6286 - val_loss: 0.7790\n",
      "Epoch 94/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.6282 - val_loss: 0.7786\n",
      "Epoch 95/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.6277 - val_loss: 0.7782\n",
      "Epoch 96/1000\n",
      "161/161 [==============================] - 0s 746us/step - loss: 0.6273 - val_loss: 0.7778\n",
      "Epoch 97/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.6269 - val_loss: 0.7774\n",
      "Epoch 98/1000\n",
      "161/161 [==============================] - 0s 746us/step - loss: 0.6264 - val_loss: 0.7770\n",
      "Epoch 99/1000\n",
      "161/161 [==============================] - 0s 749us/step - loss: 0.6260 - val_loss: 0.7766\n",
      "Epoch 100/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.6256 - val_loss: 0.7762\n",
      "Epoch 101/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.6252 - val_loss: 0.7758\n",
      "Epoch 102/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.6247 - val_loss: 0.7754\n",
      "Epoch 103/1000\n",
      "161/161 [==============================] - 0s 825us/step - loss: 0.6243 - val_loss: 0.7750\n",
      "Epoch 104/1000\n",
      "161/161 [==============================] - 0s 751us/step - loss: 0.6239 - val_loss: 0.7747\n",
      "Epoch 105/1000\n",
      "161/161 [==============================] - 0s 751us/step - loss: 0.6234 - val_loss: 0.7743\n",
      "Epoch 106/1000\n",
      "161/161 [==============================] - 0s 741us/step - loss: 0.6230 - val_loss: 0.7739\n",
      "Epoch 107/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.6226 - val_loss: 0.7736\n",
      "Epoch 108/1000\n",
      "161/161 [==============================] - 0s 746us/step - loss: 0.6222 - val_loss: 0.7732\n",
      "Epoch 109/1000\n",
      "161/161 [==============================] - 0s 738us/step - loss: 0.6218 - val_loss: 0.7728\n",
      "Epoch 110/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.6213 - val_loss: 0.7725\n",
      "Epoch 111/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.6209 - val_loss: 0.7721\n",
      "Epoch 112/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.6205 - val_loss: 0.7718\n",
      "Epoch 113/1000\n",
      "161/161 [==============================] - 0s 739us/step - loss: 0.6201 - val_loss: 0.7714\n",
      "Epoch 114/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.6197 - val_loss: 0.7711\n",
      "Epoch 115/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.6193 - val_loss: 0.7708\n",
      "Epoch 116/1000\n",
      "161/161 [==============================] - 0s 752us/step - loss: 0.6189 - val_loss: 0.7704\n",
      "Epoch 117/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.6185 - val_loss: 0.7701\n",
      "Epoch 118/1000\n",
      "161/161 [==============================] - 0s 752us/step - loss: 0.6180 - val_loss: 0.7698\n",
      "Epoch 119/1000\n",
      "161/161 [==============================] - 0s 822us/step - loss: 0.6176 - val_loss: 0.7694\n",
      "Epoch 120/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.6172 - val_loss: 0.7691\n",
      "Epoch 121/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.6168 - val_loss: 0.7688\n",
      "Epoch 122/1000\n",
      "161/161 [==============================] - 0s 740us/step - loss: 0.6164 - val_loss: 0.7685\n",
      "Epoch 123/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.6160 - val_loss: 0.7682\n",
      "Epoch 124/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.6156 - val_loss: 0.7679\n",
      "Epoch 125/1000\n",
      "161/161 [==============================] - 0s 738us/step - loss: 0.6152 - val_loss: 0.7676\n",
      "Epoch 126/1000\n",
      "161/161 [==============================] - 0s 796us/step - loss: 0.6149 - val_loss: 0.7673\n",
      "Epoch 127/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.6145 - val_loss: 0.7670\n",
      "Epoch 128/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.6141 - val_loss: 0.7667\n",
      "Epoch 129/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.6137 - val_loss: 0.7664\n",
      "Epoch 130/1000\n",
      "161/161 [==============================] - 0s 751us/step - loss: 0.6133 - val_loss: 0.7661\n",
      "Epoch 131/1000\n",
      "161/161 [==============================] - 0s 739us/step - loss: 0.6129 - val_loss: 0.7658\n",
      "Epoch 132/1000\n",
      "161/161 [==============================] - 0s 735us/step - loss: 0.6125 - val_loss: 0.7655\n",
      "Epoch 133/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.6122 - val_loss: 0.7653\n",
      "Epoch 134/1000\n",
      "161/161 [==============================] - 0s 821us/step - loss: 0.6118 - val_loss: 0.7650\n",
      "Epoch 135/1000\n",
      "161/161 [==============================] - 0s 751us/step - loss: 0.6114 - val_loss: 0.7647\n",
      "Epoch 136/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.6110 - val_loss: 0.7644\n",
      "Epoch 137/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.6107 - val_loss: 0.7642\n",
      "Epoch 138/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.6103 - val_loss: 0.7639\n",
      "Epoch 139/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.6099 - val_loss: 0.7637\n",
      "Epoch 140/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.6096 - val_loss: 0.7634\n",
      "Epoch 141/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.6092 - val_loss: 0.7631\n",
      "Epoch 142/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.6089 - val_loss: 0.7629\n",
      "Epoch 143/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.6085 - val_loss: 0.7626\n",
      "Epoch 144/1000\n",
      "161/161 [==============================] - 0s 735us/step - loss: 0.6082 - val_loss: 0.7624\n",
      "Epoch 145/1000\n",
      "161/161 [==============================] - 0s 749us/step - loss: 0.6078 - val_loss: 0.7622\n",
      "Epoch 146/1000\n",
      "161/161 [==============================] - 0s 751us/step - loss: 0.6075 - val_loss: 0.7619\n",
      "Epoch 147/1000\n",
      "161/161 [==============================] - 0s 740us/step - loss: 0.6071 - val_loss: 0.7617\n",
      "Epoch 148/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.6068 - val_loss: 0.7614\n",
      "Epoch 149/1000\n",
      "161/161 [==============================] - 0s 824us/step - loss: 0.6064 - val_loss: 0.7612\n",
      "Epoch 150/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.6061 - val_loss: 0.7610\n",
      "Epoch 151/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.6057 - val_loss: 0.7607\n",
      "Epoch 152/1000\n",
      "161/161 [==============================] - 0s 740us/step - loss: 0.6054 - val_loss: 0.7605\n",
      "Epoch 153/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.6051 - val_loss: 0.7603\n",
      "Epoch 154/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.6047 - val_loss: 0.7601\n",
      "Epoch 155/1000\n",
      "161/161 [==============================] - 0s 739us/step - loss: 0.6044 - val_loss: 0.7599\n",
      "Epoch 156/1000\n",
      "161/161 [==============================] - 0s 746us/step - loss: 0.6041 - val_loss: 0.7597\n",
      "Epoch 157/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.6038 - val_loss: 0.7594\n",
      "Epoch 158/1000\n",
      "161/161 [==============================] - 0s 741us/step - loss: 0.6035 - val_loss: 0.7592\n",
      "Epoch 159/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.6031 - val_loss: 0.7590\n",
      "Epoch 160/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.6028 - val_loss: 0.7588\n",
      "Epoch 161/1000\n",
      "161/161 [==============================] - 0s 746us/step - loss: 0.6025 - val_loss: 0.7586\n",
      "Epoch 162/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.6022 - val_loss: 0.7584\n",
      "Epoch 163/1000\n",
      "161/161 [==============================] - 0s 746us/step - loss: 0.6019 - val_loss: 0.7582\n",
      "Epoch 164/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.6016 - val_loss: 0.7580\n",
      "Epoch 165/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.6013 - val_loss: 0.7578\n",
      "Epoch 166/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.6010 - val_loss: 0.7576\n",
      "Epoch 167/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.6007 - val_loss: 0.7575\n",
      "Epoch 168/1000\n",
      "161/161 [==============================] - 0s 810us/step - loss: 0.6004 - val_loss: 0.7573\n",
      "Epoch 169/1000\n",
      "161/161 [==============================] - 0s 739us/step - loss: 0.6001 - val_loss: 0.7571\n",
      "Epoch 170/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.5998 - val_loss: 0.7569\n",
      "Epoch 171/1000\n",
      "161/161 [==============================] - 0s 749us/step - loss: 0.5995 - val_loss: 0.7567\n",
      "Epoch 172/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.5992 - val_loss: 0.7565\n",
      "Epoch 173/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.5989 - val_loss: 0.7564\n",
      "Epoch 174/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.5987 - val_loss: 0.7562\n",
      "Epoch 175/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5984 - val_loss: 0.7560\n",
      "Epoch 176/1000\n",
      "161/161 [==============================] - 0s 749us/step - loss: 0.5981 - val_loss: 0.7559\n",
      "Epoch 177/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.5978 - val_loss: 0.7557\n",
      "Epoch 178/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.5976 - val_loss: 0.7555\n",
      "Epoch 179/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.5973 - val_loss: 0.7554\n",
      "Epoch 180/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.5970 - val_loss: 0.7552\n",
      "Epoch 181/1000\n",
      "161/161 [==============================] - 0s 751us/step - loss: 0.5968 - val_loss: 0.7550\n",
      "Epoch 182/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.5965 - val_loss: 0.7549\n",
      "Epoch 183/1000\n",
      "161/161 [==============================] - 0s 828us/step - loss: 0.5962 - val_loss: 0.7547\n",
      "Epoch 184/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5960 - val_loss: 0.7546\n",
      "Epoch 185/1000\n",
      "161/161 [==============================] - 0s 740us/step - loss: 0.5957 - val_loss: 0.7544\n",
      "Epoch 186/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5955 - val_loss: 0.7543\n",
      "Epoch 187/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.5952 - val_loss: 0.7541\n",
      "Epoch 188/1000\n",
      "161/161 [==============================] - 0s 737us/step - loss: 0.5950 - val_loss: 0.7540\n",
      "Epoch 189/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.5947 - val_loss: 0.7538\n",
      "Epoch 190/1000\n",
      "161/161 [==============================] - 0s 746us/step - loss: 0.5945 - val_loss: 0.7537\n",
      "Epoch 191/1000\n",
      "161/161 [==============================] - 0s 741us/step - loss: 0.5943 - val_loss: 0.7536\n",
      "Epoch 192/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.5940 - val_loss: 0.7534\n",
      "Epoch 193/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5938 - val_loss: 0.7533\n",
      "Epoch 194/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5935 - val_loss: 0.7531\n",
      "Epoch 195/1000\n",
      "161/161 [==============================] - 0s 737us/step - loss: 0.5933 - val_loss: 0.7530\n",
      "Epoch 196/1000\n",
      "161/161 [==============================] - 0s 751us/step - loss: 0.5931 - val_loss: 0.7529\n",
      "Epoch 197/1000\n",
      "161/161 [==============================] - 0s 820us/step - loss: 0.5929 - val_loss: 0.7527\n",
      "Epoch 198/1000\n",
      "161/161 [==============================] - 0s 749us/step - loss: 0.5926 - val_loss: 0.7526\n",
      "Epoch 199/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.5924 - val_loss: 0.7525\n",
      "Epoch 200/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.5922 - val_loss: 0.7524\n",
      "Epoch 201/1000\n",
      "161/161 [==============================] - 0s 736us/step - loss: 0.5920 - val_loss: 0.7522\n",
      "Epoch 202/1000\n",
      "161/161 [==============================] - 0s 749us/step - loss: 0.5918 - val_loss: 0.7521\n",
      "Epoch 203/1000\n",
      "161/161 [==============================] - 0s 746us/step - loss: 0.5915 - val_loss: 0.7520\n",
      "Epoch 204/1000\n",
      "161/161 [==============================] - 0s 751us/step - loss: 0.5913 - val_loss: 0.7519\n",
      "Epoch 205/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5911 - val_loss: 0.7518\n",
      "Epoch 206/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5909 - val_loss: 0.7517\n",
      "Epoch 207/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5907 - val_loss: 0.7515\n",
      "Epoch 208/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.5905 - val_loss: 0.7514\n",
      "Epoch 209/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5903 - val_loss: 0.7513\n",
      "Epoch 210/1000\n",
      "161/161 [==============================] - 0s 737us/step - loss: 0.5901 - val_loss: 0.7512\n",
      "Epoch 211/1000\n",
      "161/161 [==============================] - 0s 814us/step - loss: 0.5899 - val_loss: 0.7511\n",
      "Epoch 212/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.5897 - val_loss: 0.7510\n",
      "Epoch 213/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.5895 - val_loss: 0.7509\n",
      "Epoch 214/1000\n",
      "161/161 [==============================] - 0s 751us/step - loss: 0.5893 - val_loss: 0.7508\n",
      "Epoch 215/1000\n",
      "161/161 [==============================] - 0s 739us/step - loss: 0.5891 - val_loss: 0.7507\n",
      "Epoch 216/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.5890 - val_loss: 0.7506\n",
      "Epoch 217/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5888 - val_loss: 0.7505\n",
      "Epoch 218/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5886 - val_loss: 0.7504\n",
      "Epoch 219/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.5884 - val_loss: 0.7503\n",
      "Epoch 220/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.5882 - val_loss: 0.7502\n",
      "Epoch 221/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5881 - val_loss: 0.7501\n",
      "Epoch 222/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5879 - val_loss: 0.7500\n",
      "Epoch 223/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5877 - val_loss: 0.7499\n",
      "Epoch 224/1000\n",
      "161/161 [==============================] - 0s 749us/step - loss: 0.5875 - val_loss: 0.7498\n",
      "Epoch 225/1000\n",
      "161/161 [==============================] - 0s 826us/step - loss: 0.5874 - val_loss: 0.7497\n",
      "Epoch 226/1000\n",
      "161/161 [==============================] - 0s 751us/step - loss: 0.5872 - val_loss: 0.7497\n",
      "Epoch 227/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5870 - val_loss: 0.7496\n",
      "Epoch 228/1000\n",
      "161/161 [==============================] - 0s 740us/step - loss: 0.5869 - val_loss: 0.7495\n",
      "Epoch 229/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5867 - val_loss: 0.7494\n",
      "Epoch 230/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5866 - val_loss: 0.7493\n",
      "Epoch 231/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5864 - val_loss: 0.7492\n",
      "Epoch 232/1000\n",
      "161/161 [==============================] - 0s 741us/step - loss: 0.5862 - val_loss: 0.7492\n",
      "Epoch 233/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5861 - val_loss: 0.7491\n",
      "Epoch 234/1000\n",
      "161/161 [==============================] - 0s 741us/step - loss: 0.5859 - val_loss: 0.7490\n",
      "Epoch 235/1000\n",
      "161/161 [==============================] - 0s 741us/step - loss: 0.5858 - val_loss: 0.7489\n",
      "Epoch 236/1000\n",
      "161/161 [==============================] - 0s 751us/step - loss: 0.5856 - val_loss: 0.7489\n",
      "Epoch 237/1000\n",
      "161/161 [==============================] - 0s 741us/step - loss: 0.5855 - val_loss: 0.7488\n",
      "Epoch 238/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5853 - val_loss: 0.7487\n",
      "Epoch 239/1000\n",
      "161/161 [==============================] - 0s 818us/step - loss: 0.5852 - val_loss: 0.7486\n",
      "Epoch 240/1000\n",
      "161/161 [==============================] - 0s 740us/step - loss: 0.5851 - val_loss: 0.7486\n",
      "Epoch 241/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5849 - val_loss: 0.7485\n",
      "Epoch 242/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5848 - val_loss: 0.7484\n",
      "Epoch 243/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.5846 - val_loss: 0.7484\n",
      "Epoch 244/1000\n",
      "161/161 [==============================] - 0s 738us/step - loss: 0.5845 - val_loss: 0.7483\n",
      "Epoch 245/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.5844 - val_loss: 0.7482\n",
      "Epoch 246/1000\n",
      "161/161 [==============================] - 0s 741us/step - loss: 0.5842 - val_loss: 0.7482\n",
      "Epoch 247/1000\n",
      "161/161 [==============================] - 0s 736us/step - loss: 0.5841 - val_loss: 0.7481\n",
      "Epoch 248/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5840 - val_loss: 0.7480\n",
      "Epoch 249/1000\n",
      "161/161 [==============================] - 0s 739us/step - loss: 0.5838 - val_loss: 0.7480\n",
      "Epoch 250/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5837 - val_loss: 0.7479\n",
      "Epoch 251/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5836 - val_loss: 0.7478\n",
      "Epoch 252/1000\n",
      "161/161 [==============================] - 0s 749us/step - loss: 0.5835 - val_loss: 0.7478\n",
      "Epoch 253/1000\n",
      "161/161 [==============================] - 0s 830us/step - loss: 0.5833 - val_loss: 0.7477\n",
      "Epoch 254/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.5832 - val_loss: 0.7477\n",
      "Epoch 255/1000\n",
      "161/161 [==============================] - 0s 746us/step - loss: 0.5831 - val_loss: 0.7476\n",
      "Epoch 256/1000\n",
      "161/161 [==============================] - 0s 752us/step - loss: 0.5830 - val_loss: 0.7476\n",
      "Epoch 257/1000\n",
      "161/161 [==============================] - 0s 751us/step - loss: 0.5829 - val_loss: 0.7475\n",
      "Epoch 258/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.5828 - val_loss: 0.7475\n",
      "Epoch 259/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5826 - val_loss: 0.7474\n",
      "Epoch 260/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5825 - val_loss: 0.7473\n",
      "Epoch 261/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.5824 - val_loss: 0.7473\n",
      "Epoch 262/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5823 - val_loss: 0.7472\n",
      "Epoch 263/1000\n",
      "161/161 [==============================] - 0s 749us/step - loss: 0.5822 - val_loss: 0.7472\n",
      "Epoch 264/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.5821 - val_loss: 0.7471\n",
      "Epoch 265/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.5820 - val_loss: 0.7471\n",
      "Epoch 266/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.5819 - val_loss: 0.7470\n",
      "Epoch 267/1000\n",
      "161/161 [==============================] - 0s 813us/step - loss: 0.5818 - val_loss: 0.7470\n",
      "Epoch 268/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.5817 - val_loss: 0.7470\n",
      "Epoch 269/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5816 - val_loss: 0.7469\n",
      "Epoch 270/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5815 - val_loss: 0.7469\n",
      "Epoch 271/1000\n",
      "161/161 [==============================] - 0s 741us/step - loss: 0.5814 - val_loss: 0.7468\n",
      "Epoch 272/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.5813 - val_loss: 0.7468\n",
      "Epoch 273/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5812 - val_loss: 0.7467\n",
      "Epoch 274/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.5811 - val_loss: 0.7467\n",
      "Epoch 275/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.5810 - val_loss: 0.7466\n",
      "Epoch 276/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.5809 - val_loss: 0.7466\n",
      "Epoch 277/1000\n",
      "161/161 [==============================] - 0s 749us/step - loss: 0.5808 - val_loss: 0.7466\n",
      "Epoch 278/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5807 - val_loss: 0.7465\n",
      "Epoch 279/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.5806 - val_loss: 0.7465\n",
      "Epoch 280/1000\n",
      "161/161 [==============================] - 0s 749us/step - loss: 0.5805 - val_loss: 0.7464\n",
      "Epoch 281/1000\n",
      "161/161 [==============================] - 0s 826us/step - loss: 0.5804 - val_loss: 0.7464\n",
      "Epoch 282/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.5803 - val_loss: 0.7464\n",
      "Epoch 283/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5802 - val_loss: 0.7463\n",
      "Epoch 284/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.5802 - val_loss: 0.7463\n",
      "Epoch 285/1000\n",
      "161/161 [==============================] - 0s 737us/step - loss: 0.5801 - val_loss: 0.7462\n",
      "Epoch 286/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5800 - val_loss: 0.7462\n",
      "Epoch 287/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5799 - val_loss: 0.7462\n",
      "Epoch 288/1000\n",
      "161/161 [==============================] - 0s 734us/step - loss: 0.5798 - val_loss: 0.7461\n",
      "Epoch 289/1000\n",
      "161/161 [==============================] - 0s 736us/step - loss: 0.5797 - val_loss: 0.7461\n",
      "Epoch 290/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5797 - val_loss: 0.7461\n",
      "Epoch 291/1000\n",
      "161/161 [==============================] - 0s 751us/step - loss: 0.5796 - val_loss: 0.7460\n",
      "Epoch 292/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5795 - val_loss: 0.7460\n",
      "Epoch 293/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.5794 - val_loss: 0.7460\n",
      "Epoch 294/1000\n",
      "161/161 [==============================] - 0s 809us/step - loss: 0.5793 - val_loss: 0.7459\n",
      "Epoch 295/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.5792 - val_loss: 0.7459\n",
      "Epoch 296/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.5792 - val_loss: 0.7459\n",
      "Epoch 297/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.5791 - val_loss: 0.7458\n",
      "Epoch 298/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5790 - val_loss: 0.7458\n",
      "Epoch 299/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.5789 - val_loss: 0.7458\n",
      "Epoch 300/1000\n",
      "161/161 [==============================] - 0s 751us/step - loss: 0.5789 - val_loss: 0.7457\n",
      "Epoch 301/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5788 - val_loss: 0.7457\n",
      "Epoch 302/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5787 - val_loss: 0.7457\n",
      "Epoch 303/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.5786 - val_loss: 0.7456\n",
      "Epoch 304/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.5786 - val_loss: 0.7456\n",
      "Epoch 305/1000\n",
      "161/161 [==============================] - 0s 739us/step - loss: 0.5785 - val_loss: 0.7456\n",
      "Epoch 306/1000\n",
      "161/161 [==============================] - 0s 737us/step - loss: 0.5784 - val_loss: 0.7455\n",
      "Epoch 307/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.5783 - val_loss: 0.7455\n",
      "Epoch 308/1000\n",
      "161/161 [==============================] - 0s 807us/step - loss: 0.5783 - val_loss: 0.7455\n",
      "Epoch 309/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.5782 - val_loss: 0.7454\n",
      "Epoch 310/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.5781 - val_loss: 0.7454\n",
      "Epoch 311/1000\n",
      "161/161 [==============================] - 0s 737us/step - loss: 0.5781 - val_loss: 0.7454\n",
      "Epoch 312/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.5780 - val_loss: 0.7454\n",
      "Epoch 313/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5779 - val_loss: 0.7453\n",
      "Epoch 314/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5778 - val_loss: 0.7453\n",
      "Epoch 315/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.5778 - val_loss: 0.7453\n",
      "Epoch 316/1000\n",
      "161/161 [==============================] - 0s 741us/step - loss: 0.5777 - val_loss: 0.7452\n",
      "Epoch 317/1000\n",
      "161/161 [==============================] - 0s 752us/step - loss: 0.5776 - val_loss: 0.7452\n",
      "Epoch 318/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.5776 - val_loss: 0.7452\n",
      "Epoch 319/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5775 - val_loss: 0.7452\n",
      "Epoch 320/1000\n",
      "161/161 [==============================] - 0s 741us/step - loss: 0.5774 - val_loss: 0.7451\n",
      "Epoch 321/1000\n",
      "161/161 [==============================] - 0s 752us/step - loss: 0.5774 - val_loss: 0.7451\n",
      "Epoch 322/1000\n",
      "161/161 [==============================] - 0s 825us/step - loss: 0.5773 - val_loss: 0.7451\n",
      "Epoch 323/1000\n",
      "161/161 [==============================] - 0s 741us/step - loss: 0.5772 - val_loss: 0.7450\n",
      "Epoch 324/1000\n",
      "161/161 [==============================] - 0s 752us/step - loss: 0.5772 - val_loss: 0.7450\n",
      "Epoch 325/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.5771 - val_loss: 0.7450\n",
      "Epoch 326/1000\n",
      "161/161 [==============================] - 0s 739us/step - loss: 0.5770 - val_loss: 0.7450\n",
      "Epoch 327/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.5770 - val_loss: 0.7449\n",
      "Epoch 328/1000\n",
      "161/161 [==============================] - 0s 751us/step - loss: 0.5769 - val_loss: 0.7449\n",
      "Epoch 329/1000\n",
      "161/161 [==============================] - 0s 735us/step - loss: 0.5768 - val_loss: 0.7449\n",
      "Epoch 330/1000\n",
      "161/161 [==============================] - 0s 733us/step - loss: 0.5768 - val_loss: 0.7449\n",
      "Epoch 331/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5767 - val_loss: 0.7448\n",
      "Epoch 332/1000\n",
      "161/161 [==============================] - 0s 746us/step - loss: 0.5766 - val_loss: 0.7448\n",
      "Epoch 333/1000\n",
      "161/161 [==============================] - 0s 749us/step - loss: 0.5766 - val_loss: 0.7448\n",
      "Epoch 334/1000\n",
      "161/161 [==============================] - 0s 821us/step - loss: 0.5765 - val_loss: 0.7447\n",
      "Epoch 335/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.5764 - val_loss: 0.7447\n",
      "Epoch 336/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.5764 - val_loss: 0.7447\n",
      "Epoch 337/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5763 - val_loss: 0.7447\n",
      "Epoch 338/1000\n",
      "161/161 [==============================] - 0s 740us/step - loss: 0.5762 - val_loss: 0.7446\n",
      "Epoch 339/1000\n",
      "161/161 [==============================] - 0s 739us/step - loss: 0.5762 - val_loss: 0.7446\n",
      "Epoch 340/1000\n",
      "161/161 [==============================] - 0s 746us/step - loss: 0.5761 - val_loss: 0.7446\n",
      "Epoch 341/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.5760 - val_loss: 0.7446\n",
      "Epoch 342/1000\n",
      "161/161 [==============================] - 0s 739us/step - loss: 0.5760 - val_loss: 0.7445\n",
      "Epoch 343/1000\n",
      "161/161 [==============================] - 0s 740us/step - loss: 0.5759 - val_loss: 0.7445\n",
      "Epoch 344/1000\n",
      "161/161 [==============================] - 0s 739us/step - loss: 0.5759 - val_loss: 0.7445\n",
      "Epoch 345/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.5758 - val_loss: 0.7445\n",
      "Epoch 346/1000\n",
      "161/161 [==============================] - 0s 818us/step - loss: 0.5757 - val_loss: 0.7444\n",
      "Epoch 347/1000\n",
      "161/161 [==============================] - 0s 736us/step - loss: 0.5757 - val_loss: 0.7444\n",
      "Epoch 348/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5756 - val_loss: 0.7444\n",
      "Epoch 349/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5755 - val_loss: 0.7444\n",
      "Epoch 350/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.5755 - val_loss: 0.7443\n",
      "Epoch 351/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.5754 - val_loss: 0.7443\n",
      "Epoch 352/1000\n",
      "161/161 [==============================] - 0s 740us/step - loss: 0.5753 - val_loss: 0.7443\n",
      "Epoch 353/1000\n",
      "161/161 [==============================] - 0s 749us/step - loss: 0.5753 - val_loss: 0.7443\n",
      "Epoch 354/1000\n",
      "161/161 [==============================] - 0s 741us/step - loss: 0.5752 - val_loss: 0.7442\n",
      "Epoch 355/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.5752 - val_loss: 0.7442\n",
      "Epoch 356/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5751 - val_loss: 0.7442\n",
      "Epoch 357/1000\n",
      "161/161 [==============================] - 0s 746us/step - loss: 0.5750 - val_loss: 0.7442\n",
      "Epoch 358/1000\n",
      "161/161 [==============================] - 0s 822us/step - loss: 0.5750 - val_loss: 0.7441\n",
      "Epoch 359/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.5749 - val_loss: 0.7441\n",
      "Epoch 360/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5748 - val_loss: 0.7441\n",
      "Epoch 361/1000\n",
      "161/161 [==============================] - 0s 741us/step - loss: 0.5748 - val_loss: 0.7441\n",
      "Epoch 362/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5747 - val_loss: 0.7441\n",
      "Epoch 363/1000\n",
      "161/161 [==============================] - 0s 736us/step - loss: 0.5747 - val_loss: 0.7440\n",
      "Epoch 364/1000\n",
      "161/161 [==============================] - 0s 740us/step - loss: 0.5746 - val_loss: 0.7440\n",
      "Epoch 365/1000\n",
      "161/161 [==============================] - 0s 740us/step - loss: 0.5745 - val_loss: 0.7440\n",
      "Epoch 366/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5745 - val_loss: 0.7440\n",
      "Epoch 367/1000\n",
      "161/161 [==============================] - 0s 739us/step - loss: 0.5744 - val_loss: 0.7439\n",
      "Epoch 368/1000\n",
      "161/161 [==============================] - 0s 732us/step - loss: 0.5743 - val_loss: 0.7439\n",
      "Epoch 369/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.5743 - val_loss: 0.7439\n",
      "Epoch 370/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5742 - val_loss: 0.7439\n",
      "Epoch 371/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.5742 - val_loss: 0.7438\n",
      "Epoch 372/1000\n",
      "161/161 [==============================] - 0s 826us/step - loss: 0.5741 - val_loss: 0.7438\n",
      "Epoch 373/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.5740 - val_loss: 0.7438\n",
      "Epoch 374/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5740 - val_loss: 0.7438\n",
      "Epoch 375/1000\n",
      "161/161 [==============================] - 0s 738us/step - loss: 0.5739 - val_loss: 0.7438\n",
      "Epoch 376/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5738 - val_loss: 0.7437\n",
      "Epoch 377/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.5738 - val_loss: 0.7437\n",
      "Epoch 378/1000\n",
      "161/161 [==============================] - 0s 727us/step - loss: 0.5737 - val_loss: 0.7437\n",
      "Epoch 379/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5737 - val_loss: 0.7437\n",
      "Epoch 380/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.5736 - val_loss: 0.7436\n",
      "Epoch 381/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.5735 - val_loss: 0.7436\n",
      "Epoch 382/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5735 - val_loss: 0.7436\n",
      "Epoch 383/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.5734 - val_loss: 0.7436\n",
      "Epoch 384/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.5734 - val_loss: 0.7436\n",
      "Epoch 385/1000\n",
      "161/161 [==============================] - 0s 823us/step - loss: 0.5733 - val_loss: 0.7435\n",
      "Epoch 386/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.5732 - val_loss: 0.7435\n",
      "Epoch 387/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5732 - val_loss: 0.7435\n",
      "Epoch 388/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5731 - val_loss: 0.7435\n",
      "Epoch 389/1000\n",
      "161/161 [==============================] - 0s 739us/step - loss: 0.5730 - val_loss: 0.7434\n",
      "Epoch 390/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.5730 - val_loss: 0.7434\n",
      "Epoch 391/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5729 - val_loss: 0.7434\n",
      "Epoch 392/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.5729 - val_loss: 0.7434\n",
      "Epoch 393/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.5728 - val_loss: 0.7434\n",
      "Epoch 394/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.5727 - val_loss: 0.7433\n",
      "Epoch 395/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.5727 - val_loss: 0.7433\n",
      "Epoch 396/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.5726 - val_loss: 0.7433\n",
      "Epoch 397/1000\n",
      "161/161 [==============================] - 0s 746us/step - loss: 0.5726 - val_loss: 0.7433\n",
      "Epoch 398/1000\n",
      "161/161 [==============================] - 0s 736us/step - loss: 0.5725 - val_loss: 0.7433\n",
      "Epoch 399/1000\n",
      "161/161 [==============================] - 0s 814us/step - loss: 0.5724 - val_loss: 0.7432\n",
      "Epoch 400/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.5724 - val_loss: 0.7432\n",
      "Epoch 401/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5723 - val_loss: 0.7432\n",
      "Epoch 402/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.5722 - val_loss: 0.7432\n",
      "Epoch 403/1000\n",
      "161/161 [==============================] - 0s 736us/step - loss: 0.5722 - val_loss: 0.7431\n",
      "Epoch 404/1000\n",
      "161/161 [==============================] - 0s 740us/step - loss: 0.5721 - val_loss: 0.7431\n",
      "Epoch 405/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5721 - val_loss: 0.7431\n",
      "Epoch 406/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.5720 - val_loss: 0.7431\n",
      "Epoch 407/1000\n",
      "161/161 [==============================] - 0s 741us/step - loss: 0.5719 - val_loss: 0.7431\n",
      "Epoch 408/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.5719 - val_loss: 0.7430\n",
      "Epoch 409/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.5718 - val_loss: 0.7430\n",
      "Epoch 410/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.5718 - val_loss: 0.7430\n",
      "Epoch 411/1000\n",
      "161/161 [==============================] - 0s 802us/step - loss: 0.5717 - val_loss: 0.7430\n",
      "Epoch 412/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.5716 - val_loss: 0.7430\n",
      "Epoch 413/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5716 - val_loss: 0.7429\n",
      "Epoch 414/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5715 - val_loss: 0.7429\n",
      "Epoch 415/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.5714 - val_loss: 0.7429\n",
      "Epoch 416/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.5714 - val_loss: 0.7429\n",
      "Epoch 417/1000\n",
      "161/161 [==============================] - 0s 739us/step - loss: 0.5713 - val_loss: 0.7429\n",
      "Epoch 418/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.5713 - val_loss: 0.7428\n",
      "Epoch 419/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.5712 - val_loss: 0.7428\n",
      "Epoch 420/1000\n",
      "161/161 [==============================] - 0s 746us/step - loss: 0.5711 - val_loss: 0.7428\n",
      "Epoch 421/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.5711 - val_loss: 0.7428\n",
      "Epoch 422/1000\n",
      "161/161 [==============================] - 0s 751us/step - loss: 0.5710 - val_loss: 0.7428\n",
      "Epoch 423/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.5710 - val_loss: 0.7427\n",
      "Epoch 424/1000\n",
      "161/161 [==============================] - 0s 724us/step - loss: 0.5709 - val_loss: 0.7427\n",
      "Epoch 425/1000\n",
      "161/161 [==============================] - 0s 811us/step - loss: 0.5708 - val_loss: 0.7427\n",
      "Epoch 426/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5708 - val_loss: 0.7427\n",
      "Epoch 427/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5707 - val_loss: 0.7427\n",
      "Epoch 428/1000\n",
      "161/161 [==============================] - 0s 738us/step - loss: 0.5707 - val_loss: 0.7426\n",
      "Epoch 429/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.5706 - val_loss: 0.7426\n",
      "Epoch 430/1000\n",
      "161/161 [==============================] - 0s 796us/step - loss: 0.5705 - val_loss: 0.7426\n",
      "Epoch 431/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5705 - val_loss: 0.7426\n",
      "Epoch 432/1000\n",
      "161/161 [==============================] - 0s 740us/step - loss: 0.5704 - val_loss: 0.7426\n",
      "Epoch 433/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5703 - val_loss: 0.7425\n",
      "Epoch 434/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.5703 - val_loss: 0.7425\n",
      "Epoch 435/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5702 - val_loss: 0.7425\n",
      "Epoch 436/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5702 - val_loss: 0.7425\n",
      "Epoch 437/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5701 - val_loss: 0.7425\n",
      "Epoch 438/1000\n",
      "161/161 [==============================] - 0s 806us/step - loss: 0.5700 - val_loss: 0.7424\n",
      "Epoch 439/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.5700 - val_loss: 0.7424\n",
      "Epoch 440/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5699 - val_loss: 0.7424\n",
      "Epoch 441/1000\n",
      "161/161 [==============================] - 0s 738us/step - loss: 0.5699 - val_loss: 0.7424\n",
      "Epoch 442/1000\n",
      "161/161 [==============================] - 0s 746us/step - loss: 0.5698 - val_loss: 0.7424\n",
      "Epoch 443/1000\n",
      "161/161 [==============================] - 0s 746us/step - loss: 0.5697 - val_loss: 0.7423\n",
      "Epoch 444/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.5697 - val_loss: 0.7423\n",
      "Epoch 445/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.5696 - val_loss: 0.7423\n",
      "Epoch 446/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.5696 - val_loss: 0.7423\n",
      "Epoch 447/1000\n",
      "161/161 [==============================] - 0s 749us/step - loss: 0.5695 - val_loss: 0.7423\n",
      "Epoch 448/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5694 - val_loss: 0.7422\n",
      "Epoch 449/1000\n",
      "161/161 [==============================] - 0s 746us/step - loss: 0.5694 - val_loss: 0.7422\n",
      "Epoch 450/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.5693 - val_loss: 0.7422\n",
      "Epoch 451/1000\n",
      "161/161 [==============================] - 0s 826us/step - loss: 0.5692 - val_loss: 0.7422\n",
      "Epoch 452/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.5692 - val_loss: 0.7422\n",
      "Epoch 453/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.5691 - val_loss: 0.7421\n",
      "Epoch 454/1000\n",
      "161/161 [==============================] - 0s 740us/step - loss: 0.5691 - val_loss: 0.7421\n",
      "Epoch 455/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5690 - val_loss: 0.7421\n",
      "Epoch 456/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.5689 - val_loss: 0.7421\n",
      "Epoch 457/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5689 - val_loss: 0.7421\n",
      "Epoch 458/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5688 - val_loss: 0.7420\n",
      "Epoch 459/1000\n",
      "161/161 [==============================] - 0s 746us/step - loss: 0.5688 - val_loss: 0.7420\n",
      "Epoch 460/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.5687 - val_loss: 0.7420\n",
      "Epoch 461/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.5686 - val_loss: 0.7420\n",
      "Epoch 462/1000\n",
      "161/161 [==============================] - 0s 746us/step - loss: 0.5686 - val_loss: 0.7420\n",
      "Epoch 463/1000\n",
      "161/161 [==============================] - 0s 815us/step - loss: 0.5685 - val_loss: 0.7419\n",
      "Epoch 464/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.5685 - val_loss: 0.7419\n",
      "Epoch 465/1000\n",
      "161/161 [==============================] - 0s 749us/step - loss: 0.5684 - val_loss: 0.7419\n",
      "Epoch 466/1000\n",
      "161/161 [==============================] - 0s 739us/step - loss: 0.5683 - val_loss: 0.7419\n",
      "Epoch 467/1000\n",
      "161/161 [==============================] - 0s 738us/step - loss: 0.5683 - val_loss: 0.7419\n",
      "Epoch 468/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5682 - val_loss: 0.7418\n",
      "Epoch 469/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.5682 - val_loss: 0.7418\n",
      "Epoch 470/1000\n",
      "161/161 [==============================] - 0s 740us/step - loss: 0.5681 - val_loss: 0.7418\n",
      "Epoch 471/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.5680 - val_loss: 0.7418\n",
      "Epoch 472/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.5680 - val_loss: 0.7418\n",
      "Epoch 473/1000\n",
      "161/161 [==============================] - 0s 842us/step - loss: 0.5679 - val_loss: 0.7417\n",
      "Epoch 474/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5679 - val_loss: 0.7417\n",
      "Epoch 475/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.5678 - val_loss: 0.7417\n",
      "Epoch 476/1000\n",
      "161/161 [==============================] - 0s 749us/step - loss: 0.5677 - val_loss: 0.7417\n",
      "Epoch 477/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.5677 - val_loss: 0.7417\n",
      "Epoch 478/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5676 - val_loss: 0.7416\n",
      "Epoch 479/1000\n",
      "161/161 [==============================] - 0s 751us/step - loss: 0.5676 - val_loss: 0.7416\n",
      "Epoch 480/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5675 - val_loss: 0.7416\n",
      "Epoch 481/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.5674 - val_loss: 0.7416\n",
      "Epoch 482/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.5674 - val_loss: 0.7416\n",
      "Epoch 483/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.5673 - val_loss: 0.7415\n",
      "Epoch 484/1000\n",
      "161/161 [==============================] - 0s 819us/step - loss: 0.5673 - val_loss: 0.7415\n",
      "Epoch 485/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.5672 - val_loss: 0.7415\n",
      "Epoch 486/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.5671 - val_loss: 0.7415\n",
      "Epoch 487/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5671 - val_loss: 0.7415\n",
      "Epoch 488/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.5670 - val_loss: 0.7414\n",
      "Epoch 489/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5670 - val_loss: 0.7414\n",
      "Epoch 490/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.5669 - val_loss: 0.7414\n",
      "Epoch 491/1000\n",
      "161/161 [==============================] - 0s 735us/step - loss: 0.5668 - val_loss: 0.7414\n",
      "Epoch 492/1000\n",
      "161/161 [==============================] - 0s 746us/step - loss: 0.5668 - val_loss: 0.7413\n",
      "Epoch 493/1000\n",
      "161/161 [==============================] - 0s 813us/step - loss: 0.5667 - val_loss: 0.7413\n",
      "Epoch 494/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.5667 - val_loss: 0.7413\n",
      "Epoch 495/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.5666 - val_loss: 0.7413\n",
      "Epoch 496/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5665 - val_loss: 0.7413\n",
      "Epoch 497/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.5665 - val_loss: 0.7412\n",
      "Epoch 498/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.5664 - val_loss: 0.7412\n",
      "Epoch 499/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5664 - val_loss: 0.7412\n",
      "Epoch 500/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5663 - val_loss: 0.7412\n",
      "Epoch 501/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.5662 - val_loss: 0.7412\n",
      "Epoch 502/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5662 - val_loss: 0.7411\n",
      "Epoch 503/1000\n",
      "161/161 [==============================] - 0s 878us/step - loss: 0.5661 - val_loss: 0.7411\n",
      "Epoch 504/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5661 - val_loss: 0.7411\n",
      "Epoch 505/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.5660 - val_loss: 0.7411\n",
      "Epoch 506/1000\n",
      "161/161 [==============================] - 0s 741us/step - loss: 0.5660 - val_loss: 0.7410\n",
      "Epoch 507/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.5659 - val_loss: 0.7410\n",
      "Epoch 508/1000\n",
      "161/161 [==============================] - 0s 746us/step - loss: 0.5658 - val_loss: 0.7410\n",
      "Epoch 509/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.5658 - val_loss: 0.7410\n",
      "Epoch 510/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.5657 - val_loss: 0.7410\n",
      "Epoch 511/1000\n",
      "161/161 [==============================] - 0s 746us/step - loss: 0.5657 - val_loss: 0.7409\n",
      "Epoch 512/1000\n",
      "161/161 [==============================] - 0s 732us/step - loss: 0.5656 - val_loss: 0.7409\n",
      "Epoch 513/1000\n",
      "161/161 [==============================] - 0s 746us/step - loss: 0.5655 - val_loss: 0.7409\n",
      "Epoch 514/1000\n",
      "161/161 [==============================] - 0s 829us/step - loss: 0.5655 - val_loss: 0.7409\n",
      "Epoch 515/1000\n",
      "161/161 [==============================] - 0s 741us/step - loss: 0.5654 - val_loss: 0.7408\n",
      "Epoch 516/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5654 - val_loss: 0.7408\n",
      "Epoch 517/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.5653 - val_loss: 0.7408\n",
      "Epoch 518/1000\n",
      "161/161 [==============================] - 0s 746us/step - loss: 0.5653 - val_loss: 0.7408\n",
      "Epoch 519/1000\n",
      "161/161 [==============================] - 0s 741us/step - loss: 0.5652 - val_loss: 0.7408\n",
      "Epoch 520/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.5651 - val_loss: 0.7407\n",
      "Epoch 521/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.5651 - val_loss: 0.7407\n",
      "Epoch 522/1000\n",
      "161/161 [==============================] - 0s 804us/step - loss: 0.5650 - val_loss: 0.7407\n",
      "Epoch 523/1000\n",
      "161/161 [==============================] - 0s 783us/step - loss: 0.5650 - val_loss: 0.7407\n",
      "Epoch 524/1000\n",
      "161/161 [==============================] - 0s 841us/step - loss: 0.5649 - val_loss: 0.7406\n",
      "Epoch 525/1000\n",
      "161/161 [==============================] - 0s 776us/step - loss: 0.5649 - val_loss: 0.7406\n",
      "Epoch 526/1000\n",
      "161/161 [==============================] - 0s 775us/step - loss: 0.5648 - val_loss: 0.7406\n",
      "Epoch 527/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5647 - val_loss: 0.7406\n",
      "Epoch 528/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.5647 - val_loss: 0.7405\n",
      "Epoch 529/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.5646 - val_loss: 0.7405\n",
      "Epoch 530/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5646 - val_loss: 0.7405\n",
      "Epoch 531/1000\n",
      "161/161 [==============================] - 0s 746us/step - loss: 0.5645 - val_loss: 0.7405\n",
      "Epoch 532/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.5645 - val_loss: 0.7405\n",
      "Epoch 533/1000\n",
      "161/161 [==============================] - 0s 825us/step - loss: 0.5644 - val_loss: 0.7404\n",
      "Epoch 534/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5643 - val_loss: 0.7404\n",
      "Epoch 535/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.5643 - val_loss: 0.7404\n",
      "Epoch 536/1000\n",
      "161/161 [==============================] - 0s 741us/step - loss: 0.5642 - val_loss: 0.7404\n",
      "Epoch 537/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.5642 - val_loss: 0.7403\n",
      "Epoch 538/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.5641 - val_loss: 0.7403\n",
      "Epoch 539/1000\n",
      "161/161 [==============================] - 0s 741us/step - loss: 0.5641 - val_loss: 0.7403\n",
      "Epoch 540/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.5640 - val_loss: 0.7403\n",
      "Epoch 541/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.5639 - val_loss: 0.7402\n",
      "Epoch 542/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.5639 - val_loss: 0.7402\n",
      "Epoch 543/1000\n",
      "161/161 [==============================] - 0s 822us/step - loss: 0.5638 - val_loss: 0.7402\n",
      "Epoch 544/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.5638 - val_loss: 0.7402\n",
      "Epoch 545/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.5637 - val_loss: 0.7401\n",
      "Epoch 546/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.5637 - val_loss: 0.7401\n",
      "Epoch 547/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.5636 - val_loss: 0.7401\n",
      "Epoch 548/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5635 - val_loss: 0.7401\n",
      "Epoch 549/1000\n",
      "161/161 [==============================] - 0s 740us/step - loss: 0.5635 - val_loss: 0.7400\n",
      "Epoch 550/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.5634 - val_loss: 0.7400\n",
      "Epoch 551/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.5634 - val_loss: 0.7400\n",
      "Epoch 552/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.5633 - val_loss: 0.7400\n",
      "Epoch 553/1000\n",
      "161/161 [==============================] - 0s 828us/step - loss: 0.5633 - val_loss: 0.7399\n",
      "Epoch 554/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.5632 - val_loss: 0.7399\n",
      "Epoch 555/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5632 - val_loss: 0.7399\n",
      "Epoch 556/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.5631 - val_loss: 0.7399\n",
      "Epoch 557/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.5630 - val_loss: 0.7398\n",
      "Epoch 558/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5630 - val_loss: 0.7398\n",
      "Epoch 559/1000\n",
      "161/161 [==============================] - 0s 746us/step - loss: 0.5629 - val_loss: 0.7398\n",
      "Epoch 560/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5629 - val_loss: 0.7398\n",
      "Epoch 561/1000\n",
      "161/161 [==============================] - 0s 752us/step - loss: 0.5628 - val_loss: 0.7397\n",
      "Epoch 562/1000\n",
      "161/161 [==============================] - 0s 824us/step - loss: 0.5628 - val_loss: 0.7397\n",
      "Epoch 563/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.5627 - val_loss: 0.7397\n",
      "Epoch 564/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5627 - val_loss: 0.7397\n",
      "Epoch 565/1000\n",
      "161/161 [==============================] - 0s 739us/step - loss: 0.5626 - val_loss: 0.7396\n",
      "Epoch 566/1000\n",
      "161/161 [==============================] - 0s 739us/step - loss: 0.5625 - val_loss: 0.7396\n",
      "Epoch 567/1000\n",
      "161/161 [==============================] - 0s 746us/step - loss: 0.5625 - val_loss: 0.7396\n",
      "Epoch 568/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.5624 - val_loss: 0.7396\n",
      "Epoch 569/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5624 - val_loss: 0.7395\n",
      "Epoch 570/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.5623 - val_loss: 0.7395\n",
      "Epoch 571/1000\n",
      "161/161 [==============================] - 0s 746us/step - loss: 0.5623 - val_loss: 0.7395\n",
      "Epoch 572/1000\n",
      "161/161 [==============================] - 0s 810us/step - loss: 0.5622 - val_loss: 0.7395\n",
      "Epoch 573/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.5622 - val_loss: 0.7394\n",
      "Epoch 574/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5621 - val_loss: 0.7394\n",
      "Epoch 575/1000\n",
      "161/161 [==============================] - 0s 739us/step - loss: 0.5621 - val_loss: 0.7394\n",
      "Epoch 576/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5620 - val_loss: 0.7394\n",
      "Epoch 577/1000\n",
      "161/161 [==============================] - 0s 744us/step - loss: 0.5620 - val_loss: 0.7393\n",
      "Epoch 578/1000\n",
      "161/161 [==============================] - 0s 746us/step - loss: 0.5619 - val_loss: 0.7393\n",
      "Epoch 579/1000\n",
      "161/161 [==============================] - 0s 734us/step - loss: 0.5618 - val_loss: 0.7393\n",
      "Epoch 580/1000\n",
      "161/161 [==============================] - 0s 742us/step - loss: 0.5618 - val_loss: 0.7393\n",
      "Epoch 581/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.5617 - val_loss: 0.7392\n",
      "Epoch 582/1000\n",
      "161/161 [==============================] - 0s 812us/step - loss: 0.5617 - val_loss: 0.7392\n",
      "Epoch 583/1000\n",
      "161/161 [==============================] - 0s 748us/step - loss: 0.5616 - val_loss: 0.7392\n",
      "Epoch 584/1000\n",
      "161/161 [==============================] - 0s 743us/step - loss: 0.5616 - val_loss: 0.7392\n",
      "Epoch 585/1000\n",
      "161/161 [==============================] - 0s 745us/step - loss: 0.5615 - val_loss: 0.7391\n",
      "Epoch 586/1000\n",
      "161/161 [==============================] - 0s 741us/step - loss: 0.5615 - val_loss: 0.7391\n",
      "Epoch 587/1000\n",
      "161/161 [==============================] - 0s 746us/step - loss: 0.5614 - val_loss: 0.7391\n",
      "Epoch 588/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5614 - val_loss: 0.7391\n",
      "Epoch 589/1000\n",
      "161/161 [==============================] - 0s 788us/step - loss: 0.5613 - val_loss: 0.7390\n",
      "Epoch 590/1000\n",
      "161/161 [==============================] - 0s 967us/step - loss: 0.5613 - val_loss: 0.7390\n",
      "Epoch 591/1000\n",
      "161/161 [==============================] - 0s 883us/step - loss: 0.5612 - val_loss: 0.7390\n",
      "Epoch 592/1000\n",
      "161/161 [==============================] - 0s 933us/step - loss: 0.5611 - val_loss: 0.7390\n",
      "Epoch 593/1000\n",
      "161/161 [==============================] - 0s 867us/step - loss: 0.5611 - val_loss: 0.7389\n",
      "Epoch 594/1000\n",
      "161/161 [==============================] - 0s 831us/step - loss: 0.5610 - val_loss: 0.7389\n",
      "Epoch 595/1000\n",
      "161/161 [==============================] - 0s 780us/step - loss: 0.5610 - val_loss: 0.7389\n",
      "Epoch 596/1000\n",
      "161/161 [==============================] - 0s 845us/step - loss: 0.5609 - val_loss: 0.7388\n",
      "Epoch 597/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5609 - val_loss: 0.7388\n",
      "Epoch 598/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5608 - val_loss: 0.7388\n",
      "Epoch 599/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.5608 - val_loss: 0.7388\n",
      "Epoch 600/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5607 - val_loss: 0.7387\n",
      "Epoch 601/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5607 - val_loss: 0.7387\n",
      "Epoch 602/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5606 - val_loss: 0.7387\n",
      "Epoch 603/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5606 - val_loss: 0.7387\n",
      "Epoch 604/1000\n",
      "161/161 [==============================] - 0s 808us/step - loss: 0.5605 - val_loss: 0.7386\n",
      "Epoch 605/1000\n",
      "161/161 [==============================] - 0s 834us/step - loss: 0.5605 - val_loss: 0.7386\n",
      "Epoch 606/1000\n",
      "161/161 [==============================] - 0s 777us/step - loss: 0.5604 - val_loss: 0.7386\n",
      "Epoch 607/1000\n",
      "161/161 [==============================] - 0s 812us/step - loss: 0.5604 - val_loss: 0.7386\n",
      "Epoch 608/1000\n",
      "161/161 [==============================] - 0s 870us/step - loss: 0.5603 - val_loss: 0.7385\n",
      "Epoch 609/1000\n",
      "161/161 [==============================] - 0s 863us/step - loss: 0.5603 - val_loss: 0.7385\n",
      "Epoch 610/1000\n",
      "161/161 [==============================] - 0s 770us/step - loss: 0.5602 - val_loss: 0.7385\n",
      "Epoch 611/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5602 - val_loss: 0.7385\n",
      "Epoch 612/1000\n",
      "161/161 [==============================] - 0s 841us/step - loss: 0.5601 - val_loss: 0.7384\n",
      "Epoch 613/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5601 - val_loss: 0.7384\n",
      "Epoch 614/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5600 - val_loss: 0.7384\n",
      "Epoch 615/1000\n",
      "161/161 [==============================] - 0s 862us/step - loss: 0.5600 - val_loss: 0.7384\n",
      "Epoch 616/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.5599 - val_loss: 0.7383\n",
      "Epoch 617/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5599 - val_loss: 0.7383\n",
      "Epoch 618/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.5598 - val_loss: 0.7383\n",
      "Epoch 619/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5598 - val_loss: 0.7382\n",
      "Epoch 620/1000\n",
      "161/161 [==============================] - 0s 847us/step - loss: 0.5597 - val_loss: 0.7382\n",
      "Epoch 621/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.5597 - val_loss: 0.7382\n",
      "Epoch 622/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5596 - val_loss: 0.7382\n",
      "Epoch 623/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5596 - val_loss: 0.7381\n",
      "Epoch 624/1000\n",
      "161/161 [==============================] - 0s 773us/step - loss: 0.5595 - val_loss: 0.7381\n",
      "Epoch 625/1000\n",
      "161/161 [==============================] - 0s 825us/step - loss: 0.5595 - val_loss: 0.7381\n",
      "Epoch 626/1000\n",
      "161/161 [==============================] - 0s 776us/step - loss: 0.5594 - val_loss: 0.7381\n",
      "Epoch 627/1000\n",
      "161/161 [==============================] - 0s 828us/step - loss: 0.5594 - val_loss: 0.7380\n",
      "Epoch 628/1000\n",
      "161/161 [==============================] - 0s 844us/step - loss: 0.5593 - val_loss: 0.7380\n",
      "Epoch 629/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5593 - val_loss: 0.7380\n",
      "Epoch 630/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5592 - val_loss: 0.7380\n",
      "Epoch 631/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5592 - val_loss: 0.7379\n",
      "Epoch 632/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5591 - val_loss: 0.7379\n",
      "Epoch 633/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5591 - val_loss: 0.7379\n",
      "Epoch 634/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5590 - val_loss: 0.7379\n",
      "Epoch 635/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5590 - val_loss: 0.7378\n",
      "Epoch 636/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5589 - val_loss: 0.7378\n",
      "Epoch 637/1000\n",
      "161/161 [==============================] - 0s 884us/step - loss: 0.5589 - val_loss: 0.7378\n",
      "Epoch 638/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5588 - val_loss: 0.7377\n",
      "Epoch 639/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5588 - val_loss: 0.7377\n",
      "Epoch 640/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.5587 - val_loss: 0.7377\n",
      "Epoch 641/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5587 - val_loss: 0.7377\n",
      "Epoch 642/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5586 - val_loss: 0.7376\n",
      "Epoch 643/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5586 - val_loss: 0.7376\n",
      "Epoch 644/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5585 - val_loss: 0.7376\n",
      "Epoch 645/1000\n",
      "161/161 [==============================] - 0s 835us/step - loss: 0.5585 - val_loss: 0.7376\n",
      "Epoch 646/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5585 - val_loss: 0.7375\n",
      "Epoch 647/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5584 - val_loss: 0.7375\n",
      "Epoch 648/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5584 - val_loss: 0.7375\n",
      "Epoch 649/1000\n",
      "161/161 [==============================] - 0s 804us/step - loss: 0.5583 - val_loss: 0.7375\n",
      "Epoch 650/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5583 - val_loss: 0.7374\n",
      "Epoch 651/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5582 - val_loss: 0.7374\n",
      "Epoch 652/1000\n",
      "161/161 [==============================] - 0s 829us/step - loss: 0.5582 - val_loss: 0.7374\n",
      "Epoch 653/1000\n",
      "161/161 [==============================] - 0s 751us/step - loss: 0.5581 - val_loss: 0.7374\n",
      "Epoch 654/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5581 - val_loss: 0.7373\n",
      "Epoch 655/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5580 - val_loss: 0.7373\n",
      "Epoch 656/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5580 - val_loss: 0.7373\n",
      "Epoch 657/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5579 - val_loss: 0.7372\n",
      "Epoch 658/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5579 - val_loss: 0.7372\n",
      "Epoch 659/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5579 - val_loss: 0.7372\n",
      "Epoch 660/1000\n",
      "161/161 [==============================] - 0s 887us/step - loss: 0.5578 - val_loss: 0.7372\n",
      "Epoch 661/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5578 - val_loss: 0.7371\n",
      "Epoch 662/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.5577 - val_loss: 0.7371\n",
      "Epoch 663/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5577 - val_loss: 0.7371\n",
      "Epoch 664/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5576 - val_loss: 0.7371\n",
      "Epoch 665/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5576 - val_loss: 0.7370\n",
      "Epoch 666/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5575 - val_loss: 0.7370\n",
      "Epoch 667/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.5575 - val_loss: 0.7370\n",
      "Epoch 668/1000\n",
      "161/161 [==============================] - 0s 827us/step - loss: 0.5575 - val_loss: 0.7370\n",
      "Epoch 669/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5574 - val_loss: 0.7369\n",
      "Epoch 670/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5574 - val_loss: 0.7369\n",
      "Epoch 671/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5573 - val_loss: 0.7369\n",
      "Epoch 672/1000\n",
      "161/161 [==============================] - 0s 801us/step - loss: 0.5573 - val_loss: 0.7369\n",
      "Epoch 673/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5572 - val_loss: 0.7368\n",
      "Epoch 674/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5572 - val_loss: 0.7368\n",
      "Epoch 675/1000\n",
      "161/161 [==============================] - 0s 849us/step - loss: 0.5572 - val_loss: 0.7368\n",
      "Epoch 676/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5571 - val_loss: 0.7368\n",
      "Epoch 677/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.5571 - val_loss: 0.7367\n",
      "Epoch 678/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.5570 - val_loss: 0.7367\n",
      "Epoch 679/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5570 - val_loss: 0.7367\n",
      "Epoch 680/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5570 - val_loss: 0.7366\n",
      "Epoch 681/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.5569 - val_loss: 0.7366\n",
      "Epoch 682/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5569 - val_loss: 0.7366\n",
      "Epoch 683/1000\n",
      "161/161 [==============================] - 0s 885us/step - loss: 0.5568 - val_loss: 0.7366\n",
      "Epoch 684/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5568 - val_loss: 0.7365\n",
      "Epoch 685/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5567 - val_loss: 0.7365\n",
      "Epoch 686/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5567 - val_loss: 0.7365\n",
      "Epoch 687/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5567 - val_loss: 0.7365\n",
      "Epoch 688/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5566 - val_loss: 0.7364\n",
      "Epoch 689/1000\n",
      "161/161 [==============================] - 0s 769us/step - loss: 0.5566 - val_loss: 0.7364\n",
      "Epoch 690/1000\n",
      "161/161 [==============================] - 0s 749us/step - loss: 0.5565 - val_loss: 0.7364\n",
      "Epoch 691/1000\n",
      "161/161 [==============================] - 0s 830us/step - loss: 0.5565 - val_loss: 0.7364\n",
      "Epoch 692/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5565 - val_loss: 0.7363\n",
      "Epoch 693/1000\n",
      "161/161 [==============================] - 0s 751us/step - loss: 0.5564 - val_loss: 0.7363\n",
      "Epoch 694/1000\n",
      "161/161 [==============================] - 0s 804us/step - loss: 0.5564 - val_loss: 0.7363\n",
      "Epoch 695/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5563 - val_loss: 0.7363\n",
      "Epoch 696/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5563 - val_loss: 0.7362\n",
      "Epoch 697/1000\n",
      "161/161 [==============================] - 0s 751us/step - loss: 0.5563 - val_loss: 0.7362\n",
      "Epoch 698/1000\n",
      "161/161 [==============================] - 0s 835us/step - loss: 0.5562 - val_loss: 0.7362\n",
      "Epoch 699/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.5562 - val_loss: 0.7362\n",
      "Epoch 700/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5562 - val_loss: 0.7361\n",
      "Epoch 701/1000\n",
      "161/161 [==============================] - 0s 752us/step - loss: 0.5561 - val_loss: 0.7361\n",
      "Epoch 702/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.5561 - val_loss: 0.7361\n",
      "Epoch 703/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5560 - val_loss: 0.7361\n",
      "Epoch 704/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5560 - val_loss: 0.7360\n",
      "Epoch 705/1000\n",
      "161/161 [==============================] - 0s 800us/step - loss: 0.5560 - val_loss: 0.7360\n",
      "Epoch 706/1000\n",
      "161/161 [==============================] - 0s 825us/step - loss: 0.5559 - val_loss: 0.7360\n",
      "Epoch 707/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5559 - val_loss: 0.7360\n",
      "Epoch 708/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5558 - val_loss: 0.7359\n",
      "Epoch 709/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5558 - val_loss: 0.7359\n",
      "Epoch 710/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5558 - val_loss: 0.7359\n",
      "Epoch 711/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5557 - val_loss: 0.7359\n",
      "Epoch 712/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5557 - val_loss: 0.7358\n",
      "Epoch 713/1000\n",
      "161/161 [==============================] - 0s 837us/step - loss: 0.5557 - val_loss: 0.7358\n",
      "Epoch 714/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5556 - val_loss: 0.7358\n",
      "Epoch 715/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5556 - val_loss: 0.7358\n",
      "Epoch 716/1000\n",
      "161/161 [==============================] - 0s 815us/step - loss: 0.5556 - val_loss: 0.7357\n",
      "Epoch 717/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5555 - val_loss: 0.7357\n",
      "Epoch 718/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5555 - val_loss: 0.7357\n",
      "Epoch 719/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5555 - val_loss: 0.7357\n",
      "Epoch 720/1000\n",
      "161/161 [==============================] - 0s 840us/step - loss: 0.5554 - val_loss: 0.7356\n",
      "Epoch 721/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5554 - val_loss: 0.7356\n",
      "Epoch 722/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5553 - val_loss: 0.7356\n",
      "Epoch 723/1000\n",
      "161/161 [==============================] - 0s 751us/step - loss: 0.5553 - val_loss: 0.7356\n",
      "Epoch 724/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5553 - val_loss: 0.7355\n",
      "Epoch 725/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5552 - val_loss: 0.7355\n",
      "Epoch 726/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5552 - val_loss: 0.7355\n",
      "Epoch 727/1000\n",
      "161/161 [==============================] - 0s 892us/step - loss: 0.5552 - val_loss: 0.7355\n",
      "Epoch 728/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5551 - val_loss: 0.7354\n",
      "Epoch 729/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5551 - val_loss: 0.7354\n",
      "Epoch 730/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5551 - val_loss: 0.7354\n",
      "Epoch 731/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5550 - val_loss: 0.7354\n",
      "Epoch 732/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5550 - val_loss: 0.7353\n",
      "Epoch 733/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5550 - val_loss: 0.7353\n",
      "Epoch 734/1000\n",
      "161/161 [==============================] - 0s 831us/step - loss: 0.5549 - val_loss: 0.7353\n",
      "Epoch 735/1000\n",
      "161/161 [==============================] - 0s 751us/step - loss: 0.5549 - val_loss: 0.7353\n",
      "Epoch 736/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.5549 - val_loss: 0.7352\n",
      "Epoch 737/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5548 - val_loss: 0.7352\n",
      "Epoch 738/1000\n",
      "161/161 [==============================] - 0s 809us/step - loss: 0.5548 - val_loss: 0.7352\n",
      "Epoch 739/1000\n",
      "161/161 [==============================] - 0s 752us/step - loss: 0.5548 - val_loss: 0.7352\n",
      "Epoch 740/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5548 - val_loss: 0.7351\n",
      "Epoch 741/1000\n",
      "161/161 [==============================] - 0s 839us/step - loss: 0.5547 - val_loss: 0.7351\n",
      "Epoch 742/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5547 - val_loss: 0.7351\n",
      "Epoch 743/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5547 - val_loss: 0.7351\n",
      "Epoch 744/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.5546 - val_loss: 0.7350\n",
      "Epoch 745/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.5546 - val_loss: 0.7350\n",
      "Epoch 746/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5546 - val_loss: 0.7350\n",
      "Epoch 747/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.5545 - val_loss: 0.7350\n",
      "Epoch 748/1000\n",
      "161/161 [==============================] - 0s 892us/step - loss: 0.5545 - val_loss: 0.7349\n",
      "Epoch 749/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5545 - val_loss: 0.7349\n",
      "Epoch 750/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5544 - val_loss: 0.7349\n",
      "Epoch 751/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5544 - val_loss: 0.7349\n",
      "Epoch 752/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5544 - val_loss: 0.7348\n",
      "Epoch 753/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.5544 - val_loss: 0.7348\n",
      "Epoch 754/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5543 - val_loss: 0.7348\n",
      "Epoch 755/1000\n",
      "161/161 [==============================] - 0s 838us/step - loss: 0.5543 - val_loss: 0.7348\n",
      "Epoch 756/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5543 - val_loss: 0.7347\n",
      "Epoch 757/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5542 - val_loss: 0.7347\n",
      "Epoch 758/1000\n",
      "161/161 [==============================] - 0s 811us/step - loss: 0.5542 - val_loss: 0.7347\n",
      "Epoch 759/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5542 - val_loss: 0.7347\n",
      "Epoch 760/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5542 - val_loss: 0.7346\n",
      "Epoch 761/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5541 - val_loss: 0.7346\n",
      "Epoch 762/1000\n",
      "161/161 [==============================] - 0s 842us/step - loss: 0.5541 - val_loss: 0.7346\n",
      "Epoch 763/1000\n",
      "161/161 [==============================] - 0s 752us/step - loss: 0.5541 - val_loss: 0.7346\n",
      "Epoch 764/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5540 - val_loss: 0.7345\n",
      "Epoch 765/1000\n",
      "161/161 [==============================] - 0s 752us/step - loss: 0.5540 - val_loss: 0.7345\n",
      "Epoch 766/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5540 - val_loss: 0.7345\n",
      "Epoch 767/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5540 - val_loss: 0.7345\n",
      "Epoch 768/1000\n",
      "161/161 [==============================] - 0s 790us/step - loss: 0.5539 - val_loss: 0.7344\n",
      "Epoch 769/1000\n",
      "161/161 [==============================] - 0s 879us/step - loss: 0.5539 - val_loss: 0.7344\n",
      "Epoch 770/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5539 - val_loss: 0.7344\n",
      "Epoch 771/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5539 - val_loss: 0.7344\n",
      "Epoch 772/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5538 - val_loss: 0.7344\n",
      "Epoch 773/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5538 - val_loss: 0.7343\n",
      "Epoch 774/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5538 - val_loss: 0.7343\n",
      "Epoch 775/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5538 - val_loss: 0.7343\n",
      "Epoch 776/1000\n",
      "161/161 [==============================] - 0s 842us/step - loss: 0.5537 - val_loss: 0.7343\n",
      "Epoch 777/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5537 - val_loss: 0.7342\n",
      "Epoch 778/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5537 - val_loss: 0.7342\n",
      "Epoch 779/1000\n",
      "161/161 [==============================] - 0s 805us/step - loss: 0.5537 - val_loss: 0.7342\n",
      "Epoch 780/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5536 - val_loss: 0.7342\n",
      "Epoch 781/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.5536 - val_loss: 0.7341\n",
      "Epoch 782/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5536 - val_loss: 0.7341\n",
      "Epoch 783/1000\n",
      "161/161 [==============================] - 0s 847us/step - loss: 0.5536 - val_loss: 0.7341\n",
      "Epoch 784/1000\n",
      "161/161 [==============================] - 0s 749us/step - loss: 0.5535 - val_loss: 0.7341\n",
      "Epoch 785/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5535 - val_loss: 0.7340\n",
      "Epoch 786/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5535 - val_loss: 0.7340\n",
      "Epoch 787/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5535 - val_loss: 0.7340\n",
      "Epoch 788/1000\n",
      "161/161 [==============================] - 0s 805us/step - loss: 0.5534 - val_loss: 0.7340\n",
      "Epoch 789/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5534 - val_loss: 0.7339\n",
      "Epoch 790/1000\n",
      "161/161 [==============================] - 0s 843us/step - loss: 0.5534 - val_loss: 0.7339\n",
      "Epoch 791/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5534 - val_loss: 0.7339\n",
      "Epoch 792/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.5533 - val_loss: 0.7339\n",
      "Epoch 793/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5533 - val_loss: 0.7338\n",
      "Epoch 794/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5533 - val_loss: 0.7338\n",
      "Epoch 795/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5533 - val_loss: 0.7338\n",
      "Epoch 796/1000\n",
      "161/161 [==============================] - 0s 829us/step - loss: 0.5533 - val_loss: 0.7338\n",
      "Epoch 797/1000\n",
      "161/161 [==============================] - 0s 774us/step - loss: 0.5532 - val_loss: 0.7338\n",
      "Epoch 798/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.5532 - val_loss: 0.7337\n",
      "Epoch 799/1000\n",
      "161/161 [==============================] - 0s 792us/step - loss: 0.5532 - val_loss: 0.7337\n",
      "Epoch 800/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5532 - val_loss: 0.7337\n",
      "Epoch 801/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5531 - val_loss: 0.7337\n",
      "Epoch 802/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5531 - val_loss: 0.7336\n",
      "Epoch 803/1000\n",
      "161/161 [==============================] - 0s 746us/step - loss: 0.5531 - val_loss: 0.7336\n",
      "Epoch 804/1000\n",
      "161/161 [==============================] - 0s 834us/step - loss: 0.5531 - val_loss: 0.7336\n",
      "Epoch 805/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5531 - val_loss: 0.7336\n",
      "Epoch 806/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5530 - val_loss: 0.7335\n",
      "Epoch 807/1000\n",
      "161/161 [==============================] - 0s 751us/step - loss: 0.5530 - val_loss: 0.7335\n",
      "Epoch 808/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5530 - val_loss: 0.7335\n",
      "Epoch 809/1000\n",
      "161/161 [==============================] - 0s 801us/step - loss: 0.5530 - val_loss: 0.7335\n",
      "Epoch 810/1000\n",
      "161/161 [==============================] - 0s 825us/step - loss: 0.5530 - val_loss: 0.7334\n",
      "Epoch 811/1000\n",
      "161/161 [==============================] - 0s 752us/step - loss: 0.5529 - val_loss: 0.7334\n",
      "Epoch 812/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5529 - val_loss: 0.7334\n",
      "Epoch 813/1000\n",
      "161/161 [==============================] - 0s 749us/step - loss: 0.5529 - val_loss: 0.7334\n",
      "Epoch 814/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5529 - val_loss: 0.7333\n",
      "Epoch 815/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5529 - val_loss: 0.7333\n",
      "Epoch 816/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5528 - val_loss: 0.7333\n",
      "Epoch 817/1000\n",
      "161/161 [==============================] - 0s 768us/step - loss: 0.5528 - val_loss: 0.7333\n",
      "Epoch 818/1000\n",
      "161/161 [==============================] - 0s 804us/step - loss: 0.5528 - val_loss: 0.7333\n",
      "Epoch 819/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5528 - val_loss: 0.7332\n",
      "Epoch 820/1000\n",
      "161/161 [==============================] - 0s 826us/step - loss: 0.5528 - val_loss: 0.7332\n",
      "Epoch 821/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5528 - val_loss: 0.7332\n",
      "Epoch 822/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5527 - val_loss: 0.7332\n",
      "Epoch 823/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5527 - val_loss: 0.7331\n",
      "Epoch 824/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5527 - val_loss: 0.7331\n",
      "Epoch 825/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5527 - val_loss: 0.7331\n",
      "Epoch 826/1000\n",
      "161/161 [==============================] - 0s 834us/step - loss: 0.5527 - val_loss: 0.7331\n",
      "Epoch 827/1000\n",
      "161/161 [==============================] - 0s 803us/step - loss: 0.5526 - val_loss: 0.7330\n",
      "Epoch 828/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5526 - val_loss: 0.7330\n",
      "Epoch 829/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5526 - val_loss: 0.7330\n",
      "Epoch 830/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.5526 - val_loss: 0.7330\n",
      "Epoch 831/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5526 - val_loss: 0.7329\n",
      "Epoch 832/1000\n",
      "161/161 [==============================] - 0s 834us/step - loss: 0.5526 - val_loss: 0.7329\n",
      "Epoch 833/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5525 - val_loss: 0.7329\n",
      "Epoch 834/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5525 - val_loss: 0.7329\n",
      "Epoch 835/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5525 - val_loss: 0.7328\n",
      "Epoch 836/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.5525 - val_loss: 0.7328\n",
      "Epoch 837/1000\n",
      "161/161 [==============================] - 0s 800us/step - loss: 0.5525 - val_loss: 0.7328\n",
      "Epoch 838/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5525 - val_loss: 0.7328\n",
      "Epoch 839/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5524 - val_loss: 0.7327\n",
      "Epoch 840/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.5524 - val_loss: 0.7327\n",
      "Epoch 841/1000\n",
      "161/161 [==============================] - 0s 786us/step - loss: 0.5524 - val_loss: 0.7327\n",
      "Epoch 842/1000\n",
      "161/161 [==============================] - 0s 788us/step - loss: 0.5524 - val_loss: 0.7327\n",
      "Epoch 843/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5524 - val_loss: 0.7327\n",
      "Epoch 844/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5524 - val_loss: 0.7326\n",
      "Epoch 845/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.5524 - val_loss: 0.7326\n",
      "Epoch 846/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5523 - val_loss: 0.7326\n",
      "Epoch 847/1000\n",
      "161/161 [==============================] - 0s 804us/step - loss: 0.5523 - val_loss: 0.7326\n",
      "Epoch 848/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.5523 - val_loss: 0.7325\n",
      "Epoch 849/1000\n",
      "161/161 [==============================] - 0s 829us/step - loss: 0.5523 - val_loss: 0.7325\n",
      "Epoch 850/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5523 - val_loss: 0.7325\n",
      "Epoch 851/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5523 - val_loss: 0.7325\n",
      "Epoch 852/1000\n",
      "161/161 [==============================] - 0s 752us/step - loss: 0.5523 - val_loss: 0.7324\n",
      "Epoch 853/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5522 - val_loss: 0.7324\n",
      "Epoch 854/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5522 - val_loss: 0.7324\n",
      "Epoch 855/1000\n",
      "161/161 [==============================] - 0s 831us/step - loss: 0.5522 - val_loss: 0.7324\n",
      "Epoch 856/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5522 - val_loss: 0.7323\n",
      "Epoch 857/1000\n",
      "161/161 [==============================] - 0s 799us/step - loss: 0.5522 - val_loss: 0.7323\n",
      "Epoch 858/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5522 - val_loss: 0.7323\n",
      "Epoch 859/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.5522 - val_loss: 0.7323\n",
      "Epoch 860/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5522 - val_loss: 0.7322\n",
      "Epoch 861/1000\n",
      "161/161 [==============================] - 0s 844us/step - loss: 0.5521 - val_loss: 0.7322\n",
      "Epoch 862/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5521 - val_loss: 0.7322\n",
      "Epoch 863/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5521 - val_loss: 0.7322\n",
      "Epoch 864/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5521 - val_loss: 0.7321\n",
      "Epoch 865/1000\n",
      "161/161 [==============================] - 0s 749us/step - loss: 0.5521 - val_loss: 0.7321\n",
      "Epoch 866/1000\n",
      "161/161 [==============================] - 0s 771us/step - loss: 0.5521 - val_loss: 0.7321\n",
      "Epoch 867/1000\n",
      "161/161 [==============================] - 0s 889us/step - loss: 0.5521 - val_loss: 0.7321\n",
      "Epoch 868/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5521 - val_loss: 0.7320\n",
      "Epoch 869/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.5520 - val_loss: 0.7320\n",
      "Epoch 870/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.5520 - val_loss: 0.7320\n",
      "Epoch 871/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.5520 - val_loss: 0.7320\n",
      "Epoch 872/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.5520 - val_loss: 0.7319\n",
      "Epoch 873/1000\n",
      "161/161 [==============================] - 0s 831us/step - loss: 0.5520 - val_loss: 0.7319\n",
      "Epoch 874/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5520 - val_loss: 0.7319\n",
      "Epoch 875/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.5520 - val_loss: 0.7319\n",
      "Epoch 876/1000\n",
      "161/161 [==============================] - 0s 820us/step - loss: 0.5520 - val_loss: 0.7318\n",
      "Epoch 877/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5520 - val_loss: 0.7318\n",
      "Epoch 878/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5519 - val_loss: 0.7318\n",
      "Epoch 879/1000\n",
      "161/161 [==============================] - 0s 841us/step - loss: 0.5519 - val_loss: 0.7318\n",
      "Epoch 880/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5519 - val_loss: 0.7317\n",
      "Epoch 881/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5519 - val_loss: 0.7317\n",
      "Epoch 882/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5519 - val_loss: 0.7317\n",
      "Epoch 883/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5519 - val_loss: 0.7317\n",
      "Epoch 884/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5519 - val_loss: 0.7316\n",
      "Epoch 885/1000\n",
      "161/161 [==============================] - 0s 880us/step - loss: 0.5519 - val_loss: 0.7316\n",
      "Epoch 886/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.5519 - val_loss: 0.7316\n",
      "Epoch 887/1000\n",
      "161/161 [==============================] - 0s 747us/step - loss: 0.5518 - val_loss: 0.7316\n",
      "Epoch 888/1000\n",
      "161/161 [==============================] - 0s 749us/step - loss: 0.5518 - val_loss: 0.7315\n",
      "Epoch 889/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5518 - val_loss: 0.7315\n",
      "Epoch 890/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.5518 - val_loss: 0.7315\n",
      "Epoch 891/1000\n",
      "161/161 [==============================] - 0s 841us/step - loss: 0.5518 - val_loss: 0.7315\n",
      "Epoch 892/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5518 - val_loss: 0.7314\n",
      "Epoch 893/1000\n",
      "161/161 [==============================] - 0s 795us/step - loss: 0.5518 - val_loss: 0.7314\n",
      "Epoch 894/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5518 - val_loss: 0.7314\n",
      "Epoch 895/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.5518 - val_loss: 0.7314\n",
      "Epoch 896/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5518 - val_loss: 0.7313\n",
      "Epoch 897/1000\n",
      "161/161 [==============================] - 0s 831us/step - loss: 0.5518 - val_loss: 0.7313\n",
      "Epoch 898/1000\n",
      "161/161 [==============================] - 0s 749us/step - loss: 0.5517 - val_loss: 0.7313\n",
      "Epoch 899/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5517 - val_loss: 0.7313\n",
      "Epoch 900/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5517 - val_loss: 0.7312\n",
      "Epoch 901/1000\n",
      "161/161 [==============================] - 0s 752us/step - loss: 0.5517 - val_loss: 0.7312\n",
      "Epoch 902/1000\n",
      "161/161 [==============================] - 0s 800us/step - loss: 0.5517 - val_loss: 0.7312\n",
      "Epoch 903/1000\n",
      "161/161 [==============================] - 0s 836us/step - loss: 0.5517 - val_loss: 0.7312\n",
      "Epoch 904/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5517 - val_loss: 0.7311\n",
      "Epoch 905/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5517 - val_loss: 0.7311\n",
      "Epoch 906/1000\n",
      "161/161 [==============================] - 0s 752us/step - loss: 0.5517 - val_loss: 0.7311\n",
      "Epoch 907/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5517 - val_loss: 0.7311\n",
      "Epoch 908/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5517 - val_loss: 0.7310\n",
      "Epoch 909/1000\n",
      "161/161 [==============================] - 0s 831us/step - loss: 0.5517 - val_loss: 0.7310\n",
      "Epoch 910/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5516 - val_loss: 0.7310\n",
      "Epoch 911/1000\n",
      "161/161 [==============================] - 0s 811us/step - loss: 0.5516 - val_loss: 0.7309\n",
      "Epoch 912/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5516 - val_loss: 0.7309\n",
      "Epoch 913/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5516 - val_loss: 0.7309\n",
      "Epoch 914/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5516 - val_loss: 0.7309\n",
      "Epoch 915/1000\n",
      "161/161 [==============================] - 0s 849us/step - loss: 0.5516 - val_loss: 0.7308\n",
      "Epoch 916/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5516 - val_loss: 0.7308\n",
      "Epoch 917/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5516 - val_loss: 0.7308\n",
      "Epoch 918/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5516 - val_loss: 0.7308\n",
      "Epoch 919/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5516 - val_loss: 0.7307\n",
      "Epoch 920/1000\n",
      "161/161 [==============================] - 0s 789us/step - loss: 0.5516 - val_loss: 0.7307\n",
      "Epoch 921/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5516 - val_loss: 0.7307\n",
      "Epoch 922/1000\n",
      "161/161 [==============================] - 0s 827us/step - loss: 0.5516 - val_loss: 0.7307\n",
      "Epoch 923/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5516 - val_loss: 0.7306\n",
      "Epoch 924/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5515 - val_loss: 0.7306\n",
      "Epoch 925/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5515 - val_loss: 0.7306\n",
      "Epoch 926/1000\n",
      "161/161 [==============================] - 0s 749us/step - loss: 0.5515 - val_loss: 0.7306\n",
      "Epoch 927/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5515 - val_loss: 0.7305\n",
      "Epoch 928/1000\n",
      "161/161 [==============================] - 0s 796us/step - loss: 0.5515 - val_loss: 0.7305\n",
      "Epoch 929/1000\n",
      "161/161 [==============================] - 0s 804us/step - loss: 0.5515 - val_loss: 0.7305\n",
      "Epoch 930/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5515 - val_loss: 0.7304\n",
      "Epoch 931/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5515 - val_loss: 0.7304\n",
      "Epoch 932/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5515 - val_loss: 0.7304\n",
      "Epoch 933/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5515 - val_loss: 0.7304\n",
      "Epoch 934/1000\n",
      "161/161 [==============================] - 0s 827us/step - loss: 0.5515 - val_loss: 0.7303\n",
      "Epoch 935/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5515 - val_loss: 0.7303\n",
      "Epoch 936/1000\n",
      "161/161 [==============================] - 0s 750us/step - loss: 0.5515 - val_loss: 0.7303\n",
      "Epoch 937/1000\n",
      "161/161 [==============================] - 0s 761us/step - loss: 0.5515 - val_loss: 0.7303\n",
      "Epoch 938/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5515 - val_loss: 0.7302\n",
      "Epoch 939/1000\n",
      "161/161 [==============================] - 0s 808us/step - loss: 0.5514 - val_loss: 0.7302\n",
      "Epoch 940/1000\n",
      "161/161 [==============================] - 0s 765us/step - loss: 0.5514 - val_loss: 0.7302\n",
      "Epoch 941/1000\n",
      "161/161 [==============================] - 0s 848us/step - loss: 0.5514 - val_loss: 0.7301\n",
      "Epoch 942/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5514 - val_loss: 0.7301\n",
      "Epoch 943/1000\n",
      "161/161 [==============================] - 0s 752us/step - loss: 0.5514 - val_loss: 0.7301\n",
      "Epoch 944/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5514 - val_loss: 0.7301\n",
      "Epoch 945/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5514 - val_loss: 0.7300\n",
      "Epoch 946/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5514 - val_loss: 0.7300\n",
      "Epoch 947/1000\n",
      "161/161 [==============================] - 0s 827us/step - loss: 0.5514 - val_loss: 0.7300\n",
      "Epoch 948/1000\n",
      "161/161 [==============================] - 0s 803us/step - loss: 0.5514 - val_loss: 0.7300\n",
      "Epoch 949/1000\n",
      "161/161 [==============================] - 0s 752us/step - loss: 0.5514 - val_loss: 0.7299\n",
      "Epoch 950/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5514 - val_loss: 0.7299\n",
      "Epoch 951/1000\n",
      "161/161 [==============================] - 0s 749us/step - loss: 0.5514 - val_loss: 0.7299\n",
      "Epoch 952/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5514 - val_loss: 0.7298\n",
      "Epoch 953/1000\n",
      "161/161 [==============================] - 0s 819us/step - loss: 0.5514 - val_loss: 0.7298\n",
      "Epoch 954/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5514 - val_loss: 0.7298\n",
      "Epoch 955/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5514 - val_loss: 0.7298\n",
      "Epoch 956/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5514 - val_loss: 0.7297\n",
      "Epoch 957/1000\n",
      "161/161 [==============================] - 0s 798us/step - loss: 0.5513 - val_loss: 0.7297\n",
      "Epoch 958/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5513 - val_loss: 0.7297\n",
      "Epoch 959/1000\n",
      "161/161 [==============================] - 0s 838us/step - loss: 0.5513 - val_loss: 0.7297\n",
      "Epoch 960/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5513 - val_loss: 0.7296\n",
      "Epoch 961/1000\n",
      "161/161 [==============================] - 0s 760us/step - loss: 0.5513 - val_loss: 0.7296\n",
      "Epoch 962/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5513 - val_loss: 0.7296\n",
      "Epoch 963/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5513 - val_loss: 0.7295\n",
      "Epoch 964/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5513 - val_loss: 0.7295\n",
      "Epoch 965/1000\n",
      "161/161 [==============================] - 0s 839us/step - loss: 0.5513 - val_loss: 0.7295\n",
      "Epoch 966/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5513 - val_loss: 0.7295\n",
      "Epoch 967/1000\n",
      "161/161 [==============================] - 0s 813us/step - loss: 0.5513 - val_loss: 0.7294\n",
      "Epoch 968/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.5513 - val_loss: 0.7294\n",
      "Epoch 969/1000\n",
      "161/161 [==============================] - 0s 766us/step - loss: 0.5513 - val_loss: 0.7294\n",
      "Epoch 970/1000\n",
      "161/161 [==============================] - 0s 752us/step - loss: 0.5513 - val_loss: 0.7293\n",
      "Epoch 971/1000\n",
      "161/161 [==============================] - 0s 838us/step - loss: 0.5513 - val_loss: 0.7293\n",
      "Epoch 972/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5513 - val_loss: 0.7293\n",
      "Epoch 973/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5513 - val_loss: 0.7293\n",
      "Epoch 974/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5512 - val_loss: 0.7292\n",
      "Epoch 975/1000\n",
      "161/161 [==============================] - 0s 805us/step - loss: 0.5512 - val_loss: 0.7292\n",
      "Epoch 976/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.5512 - val_loss: 0.7292\n",
      "Epoch 977/1000\n",
      "161/161 [==============================] - 0s 844us/step - loss: 0.5512 - val_loss: 0.7291\n",
      "Epoch 978/1000\n",
      "161/161 [==============================] - 0s 767us/step - loss: 0.5512 - val_loss: 0.7291\n",
      "Epoch 979/1000\n",
      "161/161 [==============================] - 0s 758us/step - loss: 0.5512 - val_loss: 0.7291\n",
      "Epoch 980/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5512 - val_loss: 0.7291\n",
      "Epoch 981/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.5512 - val_loss: 0.7290\n",
      "Epoch 982/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.5512 - val_loss: 0.7290\n",
      "Epoch 983/1000\n",
      "161/161 [==============================] - 0s 806us/step - loss: 0.5512 - val_loss: 0.7290\n",
      "Epoch 984/1000\n",
      "161/161 [==============================] - 0s 826us/step - loss: 0.5512 - val_loss: 0.7290\n",
      "Epoch 985/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5512 - val_loss: 0.7289\n",
      "Epoch 986/1000\n",
      "161/161 [==============================] - 0s 759us/step - loss: 0.5512 - val_loss: 0.7289\n",
      "Epoch 987/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5512 - val_loss: 0.7289\n",
      "Epoch 988/1000\n",
      "161/161 [==============================] - 0s 764us/step - loss: 0.5512 - val_loss: 0.7288\n",
      "Epoch 989/1000\n",
      "161/161 [==============================] - 0s 753us/step - loss: 0.5512 - val_loss: 0.7288\n",
      "Epoch 990/1000\n",
      "161/161 [==============================] - 0s 846us/step - loss: 0.5512 - val_loss: 0.7288\n",
      "Epoch 991/1000\n",
      "161/161 [==============================] - 0s 809us/step - loss: 0.5511 - val_loss: 0.7288\n",
      "Epoch 992/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5511 - val_loss: 0.7287\n",
      "Epoch 993/1000\n",
      "161/161 [==============================] - 0s 755us/step - loss: 0.5511 - val_loss: 0.7287\n",
      "Epoch 994/1000\n",
      "161/161 [==============================] - 0s 762us/step - loss: 0.5511 - val_loss: 0.7287\n",
      "Epoch 995/1000\n",
      "161/161 [==============================] - 0s 756us/step - loss: 0.5511 - val_loss: 0.7287\n",
      "Epoch 996/1000\n",
      "161/161 [==============================] - 0s 754us/step - loss: 0.5511 - val_loss: 0.7286\n",
      "Epoch 997/1000\n",
      "161/161 [==============================] - 0s 870us/step - loss: 0.5511 - val_loss: 0.7286\n",
      "Epoch 998/1000\n",
      "161/161 [==============================] - 0s 757us/step - loss: 0.5511 - val_loss: 0.7286\n",
      "Epoch 999/1000\n",
      "161/161 [==============================] - 0s 810us/step - loss: 0.5511 - val_loss: 0.7285\n",
      "Epoch 1000/1000\n",
      "161/161 [==============================] - 0s 763us/step - loss: 0.5511 - val_loss: 0.7285\n"
     ]
    }
   ],
   "source": [
    "epocas = 1000\n",
    "history= modelo1.fit(\n",
    "    x = x_train,\n",
    "    y = y_train,\n",
    "    batch_size = lote,\n",
    "    epochs = epocas,\n",
    "    shuffle = False,\n",
    "    validation_data = (x_val,y_val),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2bab04650>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABTs0lEQVR4nO3deXwT1f4//tdkb5u2oXsLLWUTQbGURSxwFa/1IiAu16uoeAU3fiqoyPUquIDgR+F7vSAuCFw39LqACyIKoggCwkU2QWXfaYUubG26Jk1yfn9MM23oQluSTJK+no/HPGYyc5K+M2r78pwzM5IQQoCIiIgoRGjULoCIiIjImxhuiIiIKKQw3BAREVFIYbghIiKikMJwQ0RERCGF4YaIiIhCCsMNERERhRSGGyIiIgopDDdEREQUUhhuiCjgHT16FJIkYcGCBc1+75o1ayBJEtasWdNouwULFkCSJBw9erRFNRJR4GC4ISIiopDCcENEREQhheGGiIiIQgrDDRGd1/PPPw9JkrB//37cddddiI6ORnx8PJ577jkIIZCbm4sbb7wRUVFRSEpKwsyZM+t8RmFhIe677z4kJibCZDIhIyMD77//fp12RUVFGD16NKKjo2GxWDBq1CgUFRXVW9fevXvxt7/9DTExMTCZTOjTpw+WLl3q1e/+5ptv4pJLLoHRaERKSgrGjh1bp54DBw7glltuQVJSEkwmE9q1a4fbb78dxcXFSpuVK1di4MCBsFgsMJvN6Nq1K55++mmv1kpEMp3aBRBR8BgxYgS6deuGGTNmYNmyZfi///s/xMTEYP78+fjzn/+M//f//h8++ugjPPHEE+jbty+uvPJKAEBFRQUGDRqEgwcPYty4cejQoQM+++wzjB49GkVFRXjssccAAEII3HjjjVi/fj0efPBBdOvWDV9++SVGjRpVp5Zdu3ZhwIABaNu2LSZOnIiIiAh8+umnuOmmm/DFF1/g5ptvvuDv+/zzz2Pq1KnIzs7GQw89hH379mHu3LnYsmULNmzYAL1eD7vdjsGDB8Nms+GRRx5BUlISjh8/jm+++QZFRUWIjo7Grl27cP311+Oyyy7DtGnTYDQacfDgQWzYsOGCaySieggiovOYMmWKACDGjBmj7HM4HKJdu3ZCkiQxY8YMZf/Zs2dFWFiYGDVqlLJv9uzZAoD48MMPlX12u11kZWUJs9ksrFarEEKIJUuWCADiX//6l8fP+dOf/iQAiPfee0/Zf80114gePXqIyspKZZ/L5RL9+/cXXbp0Ufb9+OOPAoD48ccfG/2O7733ngAgjhw5IoQQorCwUBgMBvGXv/xFOJ1Opd0bb7whAIh3331XCCHE9u3bBQDx2WefNfjZr7zyigAgTp482WgNROQdHJYioia7//77lW2tVos+ffpACIH77rtP2W+xWNC1a1ccPnxY2bd8+XIkJSXhjjvuUPbp9Xo8+uijKC0txdq1a5V2Op0ODz30kMfPeeSRRzzqOHPmDFavXo3bbrsNJSUlOHXqFE6dOoXTp09j8ODBOHDgAI4fP35B3/WHH36A3W7H+PHjodHU/Kp84IEHEBUVhWXLlgEAoqOjAQDfffcdysvL6/0si8UCAPjqq6/gcrkuqC4iOj+GGyJqsrS0NI/X0dHRMJlMiIuLq7P/7Nmzyutjx46hS5cuHiEBALp166Ycd6+Tk5NhNps92nXt2tXj9cGDByGEwHPPPYf4+HiPZcqUKQDkOT4Xwl3TuT/bYDCgY8eOyvEOHTpgwoQJePvttxEXF4fBgwdjzpw5HvNtRowYgQEDBuD+++9HYmIibr/9dnz66acMOkQ+wjk3RNRkWq22SfsAef6Mr7hDwRNPPIHBgwfX26Zz584++/nnmjlzJkaPHo2vvvoK33//PR599FFMnz4dP//8M9q1a4ewsDCsW7cOP/74I5YtW4YVK1Zg0aJF+POf/4zvv/++wXNIRC3Dnhsi8rn27dvjwIEDdXoq9u7dqxx3r/Py8lBaWurRbt++fR6vO3bsCEAe2srOzq53iYyMvOCa6/vZdrsdR44cUY679ejRA88++yzWrVuHn376CcePH8e8efOU4xqNBtdccw1mzZqF3bt348UXX8Tq1avx448/XlCdRFQXww0R+dzQoUORn5+PRYsWKfscDgdef/11mM1mXHXVVUo7h8OBuXPnKu2cTidef/11j89LSEjAoEGDMH/+fOTl5dX5eSdPnrzgmrOzs2EwGPDaa6959EK98847KC4uxrBhwwAAVqsVDofD4709evSARqOBzWYDIM8ROlfPnj0BQGlDRN7DYSki8rkxY8Zg/vz5GD16NLZt24b09HR8/vnn2LBhA2bPnq30sgwfPhwDBgzAxIkTcfToUXTv3h2LFy/2mL/iNmfOHAwcOBA9evTAAw88gI4dO6KgoAAbN27EH3/8gV9//fWCao6Pj8ekSZMwdepUXHfddbjhhhuwb98+vPnmm+jbty/uuusuAMDq1asxbtw43HrrrbjooovgcDjw3//+F1qtFrfccgsAYNq0aVi3bh2GDRuG9u3bo7CwEG+++SbatWuHgQMHXlCdRFQXww0R+VxYWBjWrFmDiRMn4v3334fVakXXrl3x3nvvYfTo0Uo7jUaDpUuXYvz48fjwww8hSRJuuOEGzJw5E5mZmR6f2b17d2zduhVTp07FggULcPr0aSQkJCAzMxOTJ0/2St3PP/884uPj8cYbb+Dxxx9HTEwMxowZg5deegl6vR4AkJGRgcGDB+Prr7/G8ePHER4ejoyMDHz77be44oorAAA33HADjh49infffRenTp1CXFwcrrrqKkydOlW52oqIvEcSvpz1R0RERORnnHNDREREIYXhhoiIiEIKww0RERGFFIYbIiIiCikMN0RERBRSGG6IiIgopLS6+9y4XC6cOHECkZGRkCRJ7XKIiIioCYQQKCkpQUpKSp2H8J6r1YWbEydOIDU1Ve0yiIiIqAVyc3PRrl27Rtu0unDjvs17bm4uoqKiVK6GiIiImsJqtSI1NbVJD8VtdeHGPRQVFRXFcENERBRkmjKlhBOKiYiIKKQw3BAREVFIYbghIiKikMJwQ0RERCGF4YaIiIhCCsMNERERhRSGGyIiIgopDDdEREQUUhhuiIiIKKQw3BAREVFIYbghIiKikMJwQ0RERCGF4cbXqioAp0PtKoiIiFoNhhtf+mMbMLMr8M61gBBqV0NERNQqMNz40sbXgcpi4MQvQN6valdDRETUKjDc+FLtQHP0J/XqICIiakUYbnzFXgacOVzz+tR+9WohIiJqRRhufKX4D8/XJxluiIiI/IHhxleKcz1fnz2qShlEREStDcONrxQfl9dJl8nrskLA5VSvHiIiolaC4cZX3D03KZmApAGECyg7qW5NRERErQDDja8UVYebNumAOVHeLslTrRwiIqLWguHGV4py5LUlDYhMkrdL8tWrh4iIqJVguPGWymJg+0fApvnya/ewlCUNiEyWt9lzQ0RE5HM6tQsIGZVW4KuHAY0e6HMvYD0h749OrTUsxZ4bIiIiX2PPjbe4h55cVUD+74BwAlqDHGzYc0NEROQ3qoabdevWYfjw4UhJSYEkSViyZEmj7RcvXoxrr70W8fHxiIqKQlZWFr777jv/FHs+Wj0QES9v526W19HtAI2mJvhYGW6IiIh8TdVwU1ZWhoyMDMyZM6dJ7detW4drr70Wy5cvx7Zt23D11Vdj+PDh2L59u48rbSJ3iMn9WV5Hp8pr97AULwUnIiLyOVXn3AwZMgRDhgxpcvvZs2d7vH7ppZfw1Vdf4euvv0ZmZqaXq2uByGR5SGrfCvl14iXyOjxGXlecUacuIiKiViSoJxS7XC6UlJQgJiamwTY2mw02m015bbVafVdQTEd57aiQ1ym95HV4rLwuZ7ghIiLytaCeUPzvf/8bpaWluO222xpsM336dERHRytLamqq7wpq37/WCwlIHyhvhrWR1/ZSwGH33c8nIiKi4A03H3/8MaZOnYpPP/0UCQkJDbabNGkSiouLlSU3N7fBthes49U182suuRmIqr5KymSRH8EAcGiKiIjIx4JyWGrhwoW4//778dlnnyE7O7vRtkajEUaj0T+FmaKA+74Hjq4HLv1bzX6NRu69KT8tL+6Jx0REROR1QRduPvnkE9x7771YuHAhhg0bpnY5dbVJl5dzhcVUhxv23BAREfmSquGmtLQUBw8eVF4fOXIEO3bsQExMDNLS0jBp0iQcP34cH3zwAQB5KGrUqFF49dVX0a9fP+Tny3f8DQsLQ3R0tCrfocnCY4HTBzgsRURE5GOqzrnZunUrMjMzlcu4J0yYgMzMTEyePBkAkJeXh5ycHKX9f/7zHzgcDowdOxbJycnK8thjj6lSf7O4LwcvP61uHURERCFO1Z6bQYMGQQjR4PEFCxZ4vF6zZo1vC/KlMHe4Yc8NERGRLwXt1VJBR7mR31l16yAiIgpxDDf+wmEpIiIiv2C48Rf3jfwqilQtg4iIKNQx3PiLqfpqrspidesgIiIKcQw3/sJwQ0RE5BcMN/7CcENEROQXDDf+YrLIa4YbIiIin2K48Rd3z429BHA61K2FiIgohDHc+IsxqmbbZlWvDiIiohDHcOMvOgOgD5e3OTRFRETkMww3/sRJxURERD7HcONPDDdEREQ+x3DjT+5wwzk3REREPsNw40/suSEiIvI5hht/YrghIiLyOYYbf2K4ISIi8jmGG39iuCEiIvI5hht/YrghIiLyOYYbf2K4ISIi8jmGG39yP4KB4YaIiMhnGG78iT03REREPsdw408mi7xmuCEiIvIZhht/Ys8NERGRzzHc+FPtxy+4nOrWQkREFKIYbvzJGFmzbS9Vrw4iIqIQxnDjTzojoNHL27YSdWshIiIKUQw3/iRJgNEsb9vYc0NEROQLDDf+5h6aYs8NERGRTzDc+JuhOtzYGW6IiIh8geHG39hzQ0RE5FMMN/6mhBvOuSEiIvIFhht/UyYUs+eGiIjIFxhu/M3IOTdERES+xHDjbwbOuSEiIvIlhht/44RiIiIin2K48TfexI+IiMinGG78jT03REREPsVw42+G6p4bPjiTiIjIJ1QNN+vWrcPw4cORkpICSZKwZMmSRtvn5eXhzjvvxEUXXQSNRoPx48f7pU6vMkbJa5tV3TqIiIhClKrhpqysDBkZGZgzZ06T2ttsNsTHx+PZZ59FRkaGj6vzEd7Ej4iIyKd0av7wIUOGYMiQIU1un56ejldffRUA8O677/qqLN/iTfyIiIh8StVw4w82mw02m015bbWqPByk3MSPPTdERES+EPITiqdPn47o6GhlSU1NVbcg9038qsoBp0PdWoiIiEJQyIebSZMmobi4WFlyc3PVLcg9LAWw94aIiMgHQn5Yymg0wmg0ql1GDZ0R0BoAp12edxNmUbsiIiKikBLyPTcBifNuiIiIfEbVnpvS0lIcPHhQeX3kyBHs2LEDMTExSEtLw6RJk3D8+HF88MEHSpsdO3Yo7z158iR27NgBg8GA7t27+7v8ljOYgfLTvGKKiIjIB1QNN1u3bsXVV1+tvJ4wYQIAYNSoUViwYAHy8vKQk5Pj8Z7MzExle9u2bfj444/Rvn17HD161C81e4VyIz+GGyIiIm9TNdwMGjQIQogGjy9YsKDOvsbaBw0+X4qIiMhnOOdGDUY+X4qIiMhXGG7UwJ4bIiIin2G4UYP7yeB8vhQREZHXMdyoQem54ZPBiYiIvI3hRg28zw0REZHPMNyogXNuiIiIfIbhRg3KnBuGGyIiIm9juFGD0nPDYSkiIiJvY7hRg3KHYk4oJiIi8jaGGzXwJn5EREQ+w3CjBkOEvLaXqVsHERFRCGK4UQNv4kdEROQzDDd+YHM4PXe4JxRXlQEul/8LIiIiCmEMNz72/NJd6D75O/x8+HTNTvewFMB5N0RERF7GcONDf5wtx4L/HYXTJTB/7aGaAzoTIGnlbc67ISIi8iqGGx/al19zk77tuUUQQsgvJIlXTBEREfkIw40PHSysCS5F5VU4VWqvOWjgIxiIiIh8geHGh46dKfd4faCwVpDh5eBEREQ+wXDjQ2fL7B6vj5+tqHnBYSkiIiKfYLjxobPlcrgx6eXTXGCtrDnIe90QERH5BMOND50tqwIAXJwkP0sqv75wY+ecGyIiIm9iuPEhd89Nt2R58nCB1VZzUBmW4pwbIiIib2K48REhBIrK5Z6bbslyzw2HpYiIiHyP4cZHyu1O2J3yoxW6Jrp7bmqHG/fVUgw3RERE3sRw4yNnqq+UMuo06BAnB5mTJTY4qgOP8nwp3ueGiIjIqxhufMQ9JNUm3IBYsxFajQSXQM2N/Aycc0NEROQLDDc+4p5MbAnXQ6uRkBBpBFBraIr3uSEiIvIJhhsfcYebmAiDx/pMubvnpnrODScUExEReRXDjY+4707cJtwz3Ch3LXY/W4o9N0RERF7FcOMjZ6vn3FjC9QBqQo57ojGHpYiIiHyD4cZHGhqWOsthKSIiIp9iuPGRmp6bc+bclJ17tRTDDRERkTcx3PhIUbl7zk31sNS54cZ9nxt7GeBy+b0+IiKiUMVw4yNnzp1QHO6eUCz36Cg9NxBAVbm/yyMiIgpZDDc+otzEr7rHpk2E3IOjXAquDwOk6tPPoSkiIiKvYbjxkZqeGznU1LkUXJL48EwiIiIfYLjxgTKbAxVVTgBAnFm+M7EyLFVuh8sl5IacVExEROR1DDc+cKrUBgAI02sRYdQBqLlqyiUAa6V73g2fDE5ERORtqoabdevWYfjw4UhJSYEkSViyZMl537NmzRr06tULRqMRnTt3xoIFC3xeZ1MUV1ThvQ1H8Oaag0q4iYs0KMcNOg3M1UHHfZm4ciM/DksRERF5jarhpqysDBkZGZgzZ06T2h85cgTDhg3D1VdfjR07dmD8+PG4//778d133/m40vOrsDsx9evdmPX9fpwsqQ431UNSblEmOdxYK865Yoo9N0RERF6jU/OHDxkyBEOGDGly+3nz5qFDhw6YOXMmAKBbt25Yv349XnnlFQwePNhXZTaJ+2ooh0vg0MkyAPWEmzA9ThRXotgdbox8vhQREZG3BdWcm40bNyI7O9tj3+DBg7Fx40aVKqph1GmVYadfc4sAAO3ahHm0iQ6TA5ASbvgIBiIiIq9TteemufLz85GYmOixLzExEVarFRUVFQgLC6vzHpvNBpvNpry2Wq0+q69NhB6lNgd++6MYAJDaJtzjeFR1uKmZUMxhKSIiIm8Lqp6blpg+fTqio6OVJTU11Wc/y325d761EkATem74ZHAiIiKvC6pwk5SUhIKCAo99BQUFiIqKqrfXBgAmTZqE4uJiZcnNzfVZfe4b9bl1jDd7vI4yVffcVDjkHbyJHxERkdcF1bBUVlYWli9f7rFv5cqVyMrKavA9RqMRRqOxwePelBRtUrbNRh06xkV4HK8754Y9N0RERN6mas9NaWkpduzYgR07dgCQL/XesWMHcnJyAMi9LnfffbfS/sEHH8Thw4fx5JNPYu/evXjzzTfx6aef4vHHH1ej/Dp6t49Rtnu1bwONRvI4Hh1WfSl45bnDUmV+qY+IiKg1UDXcbN26FZmZmcjMzAQATJgwAZmZmZg8eTIAIC8vTwk6ANChQwcsW7YMK1euREZGBmbOnIm3335b9cvA3a7sEocIgxYAMLp/+zrHlQnF5/bc2Er8Uh8REVFroOqw1KBBgyCEaPB4fXcfHjRoELZv3+7DqlouIcqEdU9ejVOldnRNiqxznMNSREREvhdUc26CQazZiFhz/XN86vTc8PELREREXhdUV0sFu4Z7bjjnhoiIyFsYbvxIuRS80iEPx3FYioiIyOsYbvzI3XPjdAmU2Z2eN/FrZO4RERERNR3DjR+Z9BoYtPIpt1ZU1fTcCBdQVa5iZURERKGD4caPJElCVPW9boorqgB9rWdPcd4NERGRVzDc+FlU7UnFGg3vdUNERORlDDd+VvN8Kd7rhoiIyBcYbvys4SeDc1iKiIjIGxhu/KzuvW6qH67JG/kRERF5BcONn0Wa5AnFpTaHvMNQ/ZgGO+fcEBEReQPDjZ+Z3eGmsjrc8BEMREREXsVw42fuCcUl7nDjHpbinBsiIiKvYLjxM7Px3GEpXi1FRETkTQw3fuYONyXucGOsnnPD+9wQERF5BcONn9XMueF9boiIiHyB4cbPIusMS3HODRERkTcx3PgZr5YiIiLyLYYbP1Pm3FTyPjdERES+wHDjZ5HVl4KX2h1wuQSHpYiIiLyM4cbP3HcoFgIor3JyWIqIiMjLGG78zKjTQKeRAFTPu+HVUkRERF7FcONnkiTVTCq2VdW6zw3DDRERkTfo1C6gNTIbdSgqr5InFVvcc25K5bEqSVK3OKImcDqdqKqqUruMoKTX66HVatUugyikMdyowOOKKfewlHACjkpAH6ZiZUSNE0IgPz8fRUVFapcS1CwWC5KSkiDxf2aIfILhRgWRplo38jPE1BywlTLcUEBzB5uEhASEh4fzj3MzCSFQXl6OwsJCAEBycrLKFRGFJoYbFSiXg1c6AI0W0IcDVeXV97qJV7c4ogY4nU4l2MTGxqpdTtAKC5P/B6awsBAJCQkcoiLyAU4oVkGdh2cqV0zxXjcUuNxzbMLDw1WuJPi5zyHnLRH5BsONCvgIBgpmHIq6cDyHRL7FcKOCmodn8sngRERE3sZwo4K6z5diuCEKFunp6Zg9e7baZRBRIzihWAXuYSllzg2HpYh8atCgQejZs6dXQsmWLVsQERFx4UURkc8w3KjA3XNTyp4booAghIDT6YROd/5fifHxvKKRKNBxWEoFyqXg7p4bU7S8rrSqVBFR6Bo9ejTWrl2LV199FZIkQZIkLFiwAJIk4dtvv0Xv3r1hNBqxfv16HDp0CDfeeCMSExNhNpvRt29f/PDDDx6fd+6wlCRJePvtt3HzzTcjPDwcXbp0wdKlS/38LYmoNoYbFUSee7WUKUpeVxarVBFRywghUG53+H0RQjS5xldffRVZWVl44IEHkJeXh7y8PKSmpgIAJk6ciBkzZmDPnj247LLLUFpaiqFDh2LVqlXYvn07rrvuOgwfPhw5OTmN/oypU6fitttuw2+//YahQ4di5MiROHPmzAWdWyJqOQ5LqUAZlqrTc8NwQ8GlosqJ7pO/8/vP3T1tMMINTfv1FR0dDYPBgPDwcCQlJQEA9u7dCwCYNm0arr32WqVtTEwMMjIylNcvvPACvvzySyxduhTjxo1r8GeMHj0ad9xxBwDgpZdewmuvvYbNmzfjuuuua/Z3I6ILx54bFbgnFFsrqy8FV8JNkToFEbVSffr08XhdWlqKJ554At26dYPFYoHZbMaePXvO23Nz2WWXKdsRERGIiopSHrFARP7Xop6b999/H3FxcRg2bBgA4Mknn8R//vMfdO/eHZ988gnat2/v1SJDTWStnhshBCSTRT7AnhsKMmF6LXZPG6zKz/WGc696euKJJ7By5Ur8+9//RufOnREWFoa//e1vsNvtjX6OXq/3eC1JElwul1dqJKLma1G4eemllzB37lwAwMaNGzFnzhy88sor+Oabb/D4449j8eLFXi0y1Lh7boQAyu1ORHBYioKUJElNHh5Sk8FggNPpPG+7DRs2YPTo0bj55psByD05R48e9XF1RORtLRqWys3NRefOnQEAS5YswS233IIxY8Zg+vTp+Omnn5r9eXPmzEF6ejpMJhP69euHzZs3N9i2qqoK06ZNQ6dOnWAymZCRkYEVK1a05GuoJkyvhVYj33691ObgnBsiH0tPT8emTZtw9OhRnDp1qsFelS5dumDx4sXYsWMHfv31V9x5553sgSEKQi0KN2azGadPnwYAfP/998qEPJPJhIqKimZ91qJFizBhwgRMmTIFv/zyCzIyMjB48OAGx6ufffZZzJ8/H6+//jp2796NBx98EDfffDO2b9/ekq+iCkmSPO9SzHBD5FNPPPEEtFotunfvjvj4+Abn0MyaNQtt2rRB//79MXz4cAwePBi9evXyc7VEdKEk0ZxrKquNHDkSe/fuRWZmJj755BPk5OQgNjYWS5cuxdNPP42dO3c2+bP69euHvn374o033gAAuFwupKam4pFHHsHEiRPrtE9JScEzzzyDsWPHKvtuueUWhIWF4cMPPzzvz7NarYiOjkZxcTGioqKaXKe3DZixGseLKvDlw/2R2cYGzOwKSBpg8hmAD9WjAFRZWYkjR46gQ4cOMJlMapcT1HguiZqvOX+/W9RzM2fOHGRlZeHkyZP44osvEBsbCwDYtm2bcjlkU9jtdmzbtg3Z2dk1BWk0yM7OxsaNG+t9j81mq/PLICwsDOvXr2+wvdVq9VgCgfteNx49N8LFuxQTERFdoBbNBLRYLEpPS21Tp05t1uecOnUKTqcTiYmJHvsTExOV+1Cca/DgwZg1axauvPJKdOrUCatWrcLixYsbnCw4ffr0ZtflD+5hqTKbA9CZAK0BcNrloSljpMrVERERBa8W9dysWLHCo6dkzpw56NmzJ+68806cPXvWa8XV59VXX0WXLl1w8cUXw2AwYNy4cbjnnnug0dT/VSZNmoTi4mJlyc3N9Wl9TRVR+0Z+ksR5N0RERF7SonDzz3/+Uxne+f333/GPf/wDQ4cOxZEjRzBhwoQmf05cXBy0Wi0KCgo89hcUFCh3Ej1XfHw8lixZgrKyMhw7dgx79+6F2WxGx44d621vNBoRFRXlsQQC9+Xgyl2KjXwEAxERkTe0KNwcOXIE3bt3BwB88cUXuP766/HSSy9hzpw5+Pbbb5v8OQaDAb1798aqVauUfS6XC6tWrUJWVlaj7zWZTGjbti0cDge++OIL3HjjjS35KqqJPPfJ4O6em4oidQoiIiIKES0KNwaDAeXl5QCAH374AX/5y18AyM9lae6E3QkTJuCtt97C+++/jz179uChhx5CWVkZ7rnnHgDA3XffjUmTJintN23ahMWLF+Pw4cP46aefcN1118HlcuHJJ59syVdRjfJ8KXt1uAlrI6/5CAYiIqIL0qIJxQMHDsSECRMwYMAAbN68GYsWLQIA7N+/H+3atWvWZ40YMQInT57E5MmTkZ+fj549e2LFihXKJOOcnByP+TSVlZV49tlncfjwYZjNZgwdOhT//e9/YbFYWvJVVBNxbs9NeIy8LueThImIiC5Ei8LNG2+8gYcffhiff/455s6di7Zt2wIAvv322xY9BXfcuHENPnF3zZo1Hq+vuuoq7N69u9k/I9BEnjvnJly+nB7lp1WqiIiIKDS0KNykpaXhm2++qbP/lVdeueCCWgvzuT03YdU9NxXsuSEiIroQLX7indPpxJIlS7Bnzx4AwCWXXIIbbrgBWq13ntYb6upcLaUMS7HnhijQpKenY/z48Rg/frzapRBRE7Qo3Bw8eBBDhw7F8ePH0bVrVwDyzfJSU1OxbNkydOrUyatFhiKP+9wAtYal2HNDRER0IVp0tdSjjz6KTp06ITc3F7/88gt++eUX5OTkoEOHDnj00Ue9XWNIiqwTbjihmIiIyBtaFG7Wrl2Lf/3rX4iJiVH2xcbGYsaMGVi7dq3Xigtl7mGpMk4oJvKp//znP0hJSYHL5fLYf+ONN+Lee+/FoUOHcOONNyIxMRFmsxl9+/bFDz/8oFK1ROQNLQo3RqMRJSUldfaXlpbCYDBccFGtQYSh1oMzgZpwU3EGaP6D2onUIQRgL/P/0oz/Rm699VacPn0aP/74o7LvzJkzWLFiBUaOHInS0lIMHToUq1atwvbt23Hddddh+PDhyMnJ8cUZIyI/aNGcm+uvvx5jxozBO++8g8svvxyAfHO9Bx98EDfccINXCwxV7kvBbQ4X7A4XDO6rpVwOwGatuWMxUSCrKgdeSvH/z336BGCIaFLTNm3aYMiQIfj4449xzTXXAAA+//xzxMXF4eqrr4ZGo0FGRobS/oUXXsCXX36JpUuXNniLCiIKbC3quXnttdfQqVMnZGVlwWQywWQyoX///ujcuTNmz57t5RJDk3tCMVA9NKU3AfrqX9YcmiLyqpEjR+KLL76AzWYDAHz00Ue4/fbbodFoUFpaiieeeALdunWDxWKB2WzGnj172HNDFMRa1HNjsVjw1Vdf4eDBg8ql4N26dUPnzp29Wlwo02s1MOk1qKxyodTmQJsIgzypuLhMnlQcU/+DQIkCij5c7kVR4+c2w/DhwyGEwLJly9C3b1/89NNPyn25nnjiCaxcuRL//ve/0blzZ4SFheFvf/sb7Ha7LyonIj9ocrg539O+a49nz5o1q+UVtSJmow6VVfaaK6Yi4oHiXKC0UN3CiJpKkpo8PKQmk8mEv/71r/joo49w8OBBdO3aFb169QIAbNiwAaNHj8bNN98MQJ47ePToURWrJaIL1eRws3379ia1kySpxcW0NmajDqdKa4WbyCR5XZqvXlFEIWrkyJG4/vrrsWvXLtx1113K/i5dumDx4sUYPnw4JEnCc889V+fKKiIKLk0ON7V7Zsg76tyl2Cw/LBQlBSpVRBS6/vznPyMmJgb79u3DnXfeqeyfNWsW7r33XvTv3x9xcXF46qmnYLVaVayUiC5Uix+/QBeuzvOl3D03JXkqVUQUujQaDU6cqDs/KD09HatXr/bYN3bsWI/XHKYiCi4tulqKvMN87l2K3T03pey5ISIiaimGGxU13HPDOTdEREQtxXCjogbn3LDnhoiIqMUYblRU58ngytVShYDLqVJVREREwY3hRkWR5w5LRSQAkADh5F2KKWAJPvvsgvEcEvkWw42KlDk39upwo9XVDE0V/6FSVUT10+v1AIDy8nKVKwl+7nPoPqdE5F28FFxFZpP8i03puQGANu3lm/gVHQPa9lKpMqK6tFotLBYLCgvlO2iHh4fzpp3NJIRAeXk5CgsLYbFYoNVq1S6JKCQx3KjIbJR/sSlzbgDAkgbkbgLOHlOpKqKGJSXJ88LcAYdaxmKxKOeSiLyP4UZFZmM9PTeW9vK6iOGGAo8kSUhOTkZCQgKqqqrULico6fV69tgQ+RjDjYrqXAoOyMNSAFCUo0JFRE2j1Wr5B5qIAhYnFKuozh2KgZqeGw5LERERtQjDjYpqhxvl0tDaPTd8MjEREVGzMdyoyD0s5XQJ2BzVQSaqHaA1AE4bYOXl4ERERM3FcKOicL0W7itpSypr3esmtou8XbhHncKIiIiCGMONijQaCWZDPfNuErrJ68LdKlRFREQU3BhuVBZx7iMYgFrhhj03REREzcVwo7J6LwdP6C6v2XNDRETUbAw3Kqv3cvBEd7jZC1RVqlAVERFR8GK4UVlNuKl1t1dLe/kJ4a4q4MR2lSojIiIKTgw3KqsJN86anZIEpPWTt3M2qlAVERFR8GK4UZky56b2hGIASMuS1zk/+7kiIiKi4MZwo7J6h6UAoH1/eX1sA+fdEBERNQPDjcrM9V0KDgBJGUBkCmAvBY6sVaEyIiKi4MRwo7KaS8Gdngc0GuDiYfL2nqV+roqIiCh4MdyorMFhKQC45CZ5vWsJUFnst5qIiIiCWUCEmzlz5iA9PR0mkwn9+vXD5s2bG20/e/ZsdO3aFWFhYUhNTcXjjz+OysrgnJcSWd9N/NzaDwDiL5aHprZ/5OfKiIiIgpPq4WbRokWYMGECpkyZgl9++QUZGRkYPHgwCgsL623/8ccfY+LEiZgyZQr27NmDd955B4sWLcLTTz/t58q9I8LQwLAUIF8S3u9Befunmey9ISIiagLVw82sWbPwwAMP4J577kH37t0xb948hIeH49133623/f/+9z8MGDAAd955J9LT0/GXv/wFd9xxx3l7ewJVzaXg9QxLAUDmXfJTwstPASun+LEyIiKi4KRquLHb7di2bRuys7OVfRqNBtnZ2di4sf6b1/Xv3x/btm1Twszhw4exfPlyDB06tN72NpsNVqvVYwkk9T5+oTatHhj6sry97T3glw/8VBkREVFwUjXcnDp1Ck6nE4mJiR77ExMTkZ+fX+977rzzTkybNg0DBw6EXq9Hp06dMGjQoAaHpaZPn47o6GhlSU1N9fr3uBANXgpeW6ergSv/KW8vfRTYtsD3hREREQUp1YelmmvNmjV46aWX8Oabb+KXX37B4sWLsWzZMrzwwgv1tp80aRKKi4uVJTc3188VN849LFVmd8LlEg03vPoZoO/9AATw9WNyyLGV+KdIIiKiIKJT84fHxcVBq9WioKDAY39BQQGSkpLqfc9zzz2Hv//977j//vsBAD169EBZWRnGjBmDZ555BhqNZ14zGo0wGo2++QJe4O65AYAyuwORJn39DSUJGPIyEBEPrJkB/PI+cHCVPGR1cf1DckRERK2Rqj03BoMBvXv3xqpVq5R9LpcLq1atQlZWVr3vKS8vrxNgtFotAECIRno+ApRRp4FeKwFoZN6Nm0YDDJoI/P1L+cnh1j+AhXcAH9wE5P3q+2KJiIiCgOrDUhMmTMBbb72F999/H3v27MFDDz2EsrIy3HPPPQCAu+++G5MmTVLaDx8+HHPnzsXChQtx5MgRrFy5Es899xyGDx+uhJxgIkkSIqp7b8rOF27cOl0NPPwzMGA8oNEDh38E5l8JLB4DnD3mu2KJiIiCgKrDUgAwYsQInDx5EpMnT0Z+fj569uyJFStWKJOMc3JyPHpqnn32WUiShGeffRbHjx9HfHw8hg8fjhdffFGtr3DBzEYdisqrUNLYpOJzGcKBa6cCvUcDq/8P2Pk58NsiYNeX8r6BjwNRKb4qmYiIKGBJIhjHci6A1WpFdHQ0iouLERUVpXY5AIDrZq/D3vwS/Pe+y/GnLvEt+5AT2+X74Lgfsqk1Voec8Qw5REQU9Jrz91v1YSmq9QiG5vTcnCslE7j7K+DupUBaFuC0AZvnA6/2BJY/CVjzvFMsERFRgGO4CQAR57uRX1NJEtDxKuCeb+sJORnAsn8AZw57oWIiIqLAxXATANyXgzdrzk1jPELOV0DqFXLI2fI28Hpv4NO7gT+2eednERERBRiGmwAQHSbf26a4ooHnS7WUJAEdBwH3rgBGfQ10vhYQLmD3V8DbfwbeGwrsXQ646nloJxERUZBS/WopAizhPgo3bpIEdLhSXgp2AxvfAH77FDi2QV6i04Deo4DMvwORief/PCIiogDGnpsA4LOem/okdgduehMY/xsw4DEgrA1QnAOsfgF4pTvw2Wjg8FrA5fJ9LURERD7AcBMALGEGAEBRud1/PzQqBbh2GjBhD3DzfKDd5YDLId8n54MbgNk9gB+eBwr3+q8mIiIiL+CwVACI8mfPzbn0YUDG7fKS/zuw5R1g52L50Q7rX5GX5J7y8e43AVHJ/q+RiIioGRhuAoB7zk2RGuGmtqQewPDZwHUzgP3fAr8uAg6uBPJ2yMuKiUC7vkC34cDF1wOxndStl4iIqB4MNwHAPefGqna4cdObgEtulpeyU/JQ1e+fAbmbgD+2yMvKyUDCJfITyTtdI4ceLf91IiIi9fGvUQBQem7KqyCEgCRJKldUS0QccPkD8mLNA/YtA/Z8DRz5CSjcJS/rXgaMUfLVWJ2zgc7XAJY0tSsnIqJWiuEmALgnFDtcAmV2p3JTv4ATlQz0vV9eys8A+7+Th60O/QhUnAH2fiMvANCmA9B+ANA+S75TckxH+ZJ0IiIiHwvQv6Kti0mvgUGrgd3pQnFFVeCGm9rCY4Ced8iLywmc2AEcWgUcXCUPW509Ii87PpTbm5PkoNPucvk5WMmXAYYIVb8CERGFpiD4Kxr6JElCdLgeJ0tsKCq3o60lTO2SmkejBdr1lperngQqi4HczdU3CdwIHN8GlObLc3d2fSm/R9IA8RcDKb2AlJ7yFVkJFwPGSDW/CRERhQCGmwARHSaHG1UuB/c2UzTQ5Vp5AYCqCjngHNsInPgFOLEdKMkDCnfLi7t3B5Dn6iR0BxK61axjO8uXrBMRETUBw02AsLjvdVMeAuHmXPowIH2gvLhZ8+SQc2K7HHjyd8q9O0U58rJ/hednRLWV5+3EdKheVy+W9oApyr/fh4iIAhrDTYDw6yMYAkFUsrxcPLRmX/kZoHBPdY/OnprtyiLAelxejv5U97OMUXL4iUoBotsCUe2q1ylAZApgTgBMFkDDG3ITEbUGDDcBIjpQbuSnpvAYIH2AvLgJAVScBc4crrucPiRfpWWzAietwMk9DX+2pJUva4+Ir7Wu3g5rIw+lmaLlEGSy1LzWGXz9rYmIyMsYbgKEu+emKBSHpS6EJMmhJzwGaNen7nFbKWA9IT8uwnoCKD7uuV2SJ/f8CCdQWiAvzaELk0NOmEWe7KwPBwxmwBDexO0w+TN0RkBnql6qt3nTQyIin+Bv1wDhvtdNqxmW8hajGYi/SF4a4rAD5aeBspPVy6la2yflq7sqi4GKopptW3H1eyuA0gp5PpC3SVrPsHO+tb46HGkNNYuu1rZWD2iNNds6Y/U+Q/X+6u2G9msNHLojopDAcBMg3HcpLq7w45PBWwudoWaOT1O5nICtpDrsFFUHnhLAXg7YS4Gq8nO2y+SlznY54LQBDhvgqASctf75CidQVSYvgUKj8wxP5wtKSqiqL2g1ZdvYSJtG3scbQhJRIxhuAkSrm1Ac6DRaeSgqzAKgvfc+1+WSQ46jsibwnHd9zrbTLvdGOc9dquR27m2n7Zz9VdWvbTX7hPOc+hzyUlXuve/sC40GJy8EqJYGNV11+NMZ5X+HiEgVDDcBwt1zc6aM4SakaTTyXBxDuNqVyFzOWkGoOvzUF4QaDFT2Wm2bsl3fz6jnuMe2rW7d7jaBTBl2NNQNPlpDw8d0xnP2uddNbe/+/FrHOORIrQzDTYCIMxsBAGfK6vlFTuQrGq286E1qV9IwIapDWEuCVT37GgxWTf0MW93j7h4ziFp1B9iwo0bv/WDVrM+q1Z5Di+RjDDcBItYsTyg+XWoPvCeDE6lJkuQry7Q6AAHS41UfIeQhPXfQcQ8nKoHK7jn3yuOYTe4Za/CYrRmfVetzanNVAfYA6hluqJepsR4obwSr+j5fo2PYCjEMNwEiJqLmyeDFFVWwhPP+KkRBRZKq597o1a5EJkQTg1Jzg9X52jdwzHVOsHLa6h9yVIXUcPDxWrBqRujifK0LxnATIIw6LSJNOpRUOnCq1M5wQ0QXRpJq/qAGAperJiw1uTergWDljZDmMZleyLd9cFSodno8SFr1e7OUIcTgnK/FcBNA4s1GlFQ6cLrUhs4JZrXLISLyHo0G0IQFzkNwXU4vDhO2cFix9rE687XKA+eqRY/5Wk0MSuGxwJAZqpXMcBNAYs0GHD5VhtNlAX4VCBFRsNNoA+fKxTrztVrQm+XNuVznDhe2ZL5WZDLDDcliI+Tu49OlgTIOTUREPhfs87XqO6bycCjDTQBxXzF1qpQ9N0REpJJAm6/VAsE3SyiEue91c5r3uiEiImoxhpsAElfdc3OyhOGGiIiopRhuAkhClHyX2AIrww0REVFLMdwEkCQl3FSqXAkREVHwYrgJIEnRcrgpLLHB6RLnaU1ERET1YbgJIHFmI7QaCU6XwCleDk5ERNQiDDcBRKuRkBApXzGVV8yhKSIiopYIiHAzZ84cpKenw2QyoV+/fti8eXODbQcNGgRJkuosw4YN82PFvuMemsovDpBnnBAREQUZ1cPNokWLMGHCBEyZMgW//PILMjIyMHjwYBQWFtbbfvHixcjLy1OWnTt3QqvV4tZbb/Vz5b7hnlScz54bIiKiFlE93MyaNQsPPPAA7rnnHnTv3h3z5s1DeHg43n333Xrbx8TEICkpSVlWrlyJ8PDw0Ak31T03ebxiioiIqEVUDTd2ux3btm1Ddna2sk+j0SA7OxsbN25s0me88847uP322xEREVHvcZvNBqvV6rEEspRo+Ym5x89yWIqIiKglVA03p06dgtPpRGJiosf+xMRE5Ofnn/f9mzdvxs6dO3H//fc32Gb69OmIjo5WltTU1Auu25fSYuUn1OacCZBH3RMREQUZ1YelLsQ777yDHj164PLLL2+wzaRJk1BcXKwsubm5fqyw+dJi5HBz7DTDDRERUUuo+lTwuLg4aLVaFBQUeOwvKChAUlJSo+8tKyvDwoULMW3atEbbGY1GGI3B82RTd7gprqhCcXkVosP1KldEREQUXFTtuTEYDOjduzdWrVql7HO5XFi1ahWysrIafe9nn30Gm82Gu+66y9dl+lWEUac8HZxDU0RERM2n+rDUhAkT8NZbb+H999/Hnj178NBDD6GsrAz33HMPAODuu+/GpEmT6rzvnXfewU033YTY2Fh/l+xz7avn3Rw7U6ZyJURERMFH1WEpABgxYgROnjyJyZMnIz8/Hz179sSKFSuUScY5OTnQaDwz2L59+7B+/Xp8//33apTsc+1jwrHt2FnOuyEiImoB1cMNAIwbNw7jxo2r99iaNWvq7OvatSuECN0HS3ZKMAMADhaWqlwJERFR8FF9WIrq6poYCQDYm1+iciVERETBh+EmAHVNksPNocJSOJwulashIiIKLgw3AaitJQzhBi3sTheOct4NERFRszDcBCCNRkKX6qGpfRyaIiIiahaGmwDVPTkKAPDbH0XqFkJERBRkGG4CVK80CwBge06RqnUQEREFG4abAJWZ1gYA8NvxIlRxUjEREVGTMdwEqI5xEYgO06OyyoU9eVa1yyEiIgoaDDcBSqOR0Ke93Huz4eBplashIiIKHgw3AeyqrvEAgDX7ClWuhIiIKHgw3ASwQRclAAC2HTsLa2WVytUQEREFB4abAJYWG45O8RFwuARW7ipQuxwiIqKgwHAT4G7q2RYA8OX24ypXQkREFBwYbgLcTZlyuNlw6BRy+CgGIiKi82K4CXCpMeG46qJ4CAG89dNhtcshIiIKeAw3QeD/u6ojAODTrbn44yx7b4iIiBrDcBMEsjrG4oqOMbA5XHjhm90QQqhdEhERUcBiuAkCkiRhyvBLoNVI+G5XAT7ZnKt2SURERAGL4SZIdEuOwpODuwIAnv96FzYfOaNyRURERIGJ4SaIPPCnjsjulgi7w4XR723G/w6dUrskIiKigMNwE0Q0Gglv3JmJgZ3jUG534u/vbMa7649wDg4REVEtDDdBxqTX4u1RfXBTzxQ4XQLTvtmNUe9tQe4ZXkVFREQEMNwEJZNei1dG9MSU4d1h0Gmwbv9JXPvKWry+6gDK7Q61yyMiIlKVJFrZmIbVakV0dDSKi4sRFRWldjkX7PDJUjz95e/4+bA8wTg+0ojx2V0wok8qdFpmVyIiCg3N+fvNcBMChBBY+usJzPx+P3Kqh6fax4Zj7KDOuLlXW+gZcoiIKMgx3DQiFMONm93hwsebjuH11QdxuswOAGhrCcNDgzrh1j7tYNRpVa6QiIioZRhuGhHK4cat3O7Ax5tyMG/tYZwqtQEAkqJMePCqjrj98jSY9Aw5REQUXBhuGtEawo1bZZUTn2zOwby1h1BglUNOnNmI+wZ2wMgr0hBl0qtcIRERUdMw3DSiNYUbt8oqJz7b9gfmrTmE40UVAIBIow53XpGG+wZ0QEKUSeUKiYiIGsdw04jWGG7cqpwuLN1xAvPWHsKBwlIAgEGrwS2922HMlR3RIS5C5QqJiIjqx3DTiNYcbtxcLoHVewsxd+0hbDt2FgAgScDQS5Px4FWd0KNdtMoVEhEReWK4aQTDjactR89g7ppDWL23UNk3sHMcHryqEwZ0joUkSSpWR0REJGO4aQTDTf325lsxf+1hLP31BJwu+V+Ji5Mice/ADrghI4VXWBERkaoYbhrBcNO43DPleGf9ESzakouKKicAIDbCgJFXtMddV6QhIZKTj4mIyP8YbhrBcNM0ReV2LNySiw/+dxQniisBAHqthOEZKbh3QAdc2pbzcoiIyH8YbhrBcNM8DqcL3+0qwLsbjiiTjwGgb3objOzXHtddmsQhKyIi8jmGm0Yw3LTcjtwivLfhCJb9lgdH9bwcS7get/RqhzsuT0PnBLPKFRIRUahiuGkEw82Fyy+uxKItuVi0JUcZsgKAyzvEYGS/NAy+hL05RETkXQw3jWC48R6nS2Dt/kJ8vCkHq/cWorozB1EmHa7PSMFfM9uid/s2vJyciIguWHP+fmv8VFOj5syZg/T0dJhMJvTr1w+bN29utH1RURHGjh2L5ORkGI1GXHTRRVi+fLmfqiU3rUbCny9OxNuj+mL9U3/G+OwuSI42wVopP7jzb/M24qqX12DWyv04eqpM7XKJiKiVUL3nZtGiRbj77rsxb9489OvXD7Nnz8Znn32Gffv2ISEhoU57u92OAQMGICEhAU8//TTatm2LY8eOwWKxICMj47w/jz03vuV0CWw8dBqLt/+BFTvzUW53Kscy0ywY1iMZ112ahHZtwlWskoiIgk1QDUv169cPffv2xRtvvAEAcLlcSE1NxSOPPIKJEyfWaT9v3jy8/PLL2Lt3L/T65j/VmuHGf8rtDny/qwCLtx/H+gMnlWErAMhoF43rLk3GkEuTkM5nWhER0XkETbix2+0IDw/H559/jptuuknZP2rUKBQVFeGrr76q856hQ4ciJiYG4eHh+OqrrxAfH48777wTTz31FLTaupNYbTYbbDab8tpqtSI1NZXhxs8KrZX4dmc+lv+eh81Hz6D2v3Xdk6OQ3T0RV3eNx2XtLNBqOEeHiIg8NSfc6PxUU71OnToFp9OJxMREj/2JiYnYu3dvve85fPgwVq9ejZEjR2L58uU4ePAgHn74YVRVVWHKlCl12k+fPh1Tp071Sf3UdAlRJozqn45R/dNxssSG73fn49vf87Hx8GnszrNid54Vr606gJgIAwZdFI9BFyfgyi5xsIQb1C6diIiCjKo9NydOnEDbtm3xv//9D1lZWcr+J598EmvXrsWmTZvqvOeiiy5CZWUljhw5ovTUzJo1Cy+//DLy8vLqtGfPTWA7U2bHD3sKsGZfIX7afwolNodyTCMBGakWZHWMRVanWPRpH4MwAy8xJyJqjYKm5yYuLg5arRYFBQUe+wsKCpCUlFTve5KTk6HX6z2GoLp164b8/HzY7XYYDJ7/p280GmE0Gr1fPHlFTIQBt/VJxW19UlHldGHbsbP4cW8hftxXiP0FpdieU4TtOUV4c80h6LUSMlPb4IpOsbiiYwwy2lkQYVT1X2EiIgpAqv5lMBgM6N27N1atWqXMuXG5XFi1ahXGjRtX73sGDBiAjz/+GC6XCxqNfCX7/v37kZycXCfYUHDRazW4omMsrugYi0lDu+GPs+X436HT+PnQaWw8fBp5xZXYfPQMNh89g9dWyT07XZOikJlmQa+0NshMs6BDbAQ0nLNDRNSqqX611KJFizBq1CjMnz8fl19+OWbPno1PP/0Ue/fuRWJiIu6++260bdsW06dPBwDk5ubikksuwahRo/DII4/gwIEDuPfee/Hoo4/imWeeOe/P49VSwUkIgWOny7Hx8GlsPHQaW4+e8bg7slt0mB6XtYtG95QodE+Wlw5xEdBpA+KWTkRE1EJBMywFACNGjMDJkycxefJk5Ofno2fPnlixYoUyyTgnJ0fpoQGA1NRUfPfdd3j88cdx2WWXoW3btnjsscfw1FNPqfUVyA8kSUJ6XATS4yJwx+VpAOTHQGzPOYvtuUXYnnMWv/1RjOKKKvx04BR+OnBKea9Rp8HFSZHolhyFixIj0SnBjE7xEUiJDmMvDxFRCFK958bf2HMTuqqcLuzJs2LncSt25xVjT14J9uRZPW4kWJtJr0HHOLMSdjrERaBdm3CkxoQh3mzkYyOIiAJI0NznRg0MN62LyyVw7Ew5dp+QA8+hwjIcOlmKo6fLUOVs+F99k14jB502YUiNCUdqm3C0bROGxCgTkqJNiDcbYdBxqIuIyF8YbhrBcEMA4HC6kHu2AocKS3HoZCkOFpbi2Jly/HGmHHnWSjTlv4o4swGJUSZlSYoyITHKiDizETFmA+Ii5HWEQcteICKiC8Rw0wiGGzofu8OFE0UVyD1bjtwz7nU58oorkV9cicKSykZ7fc5l1GnkwBNhQKzZgJgIQ83rCAPahBvQJkKP6DADLOF6RIfpoecEaCIiD0E1oZgo0Bh0GmXycn1cLoGz5XbkWytRYK1EgdWmhJ784kqcLrPjdKkdp8tsqKxyweZw4XhRBY4XVTS5hkijDtHheljC9bBUhx7PbQMsYfqabYYiIiIFww1RM2k0EmLNRsSajbgkJbrRtuV2R3XQseN0qa0m+JTacKbMjlNldhSX23G2vApF5XZYK+U7NJfYHCixOfDH2aYHIgAwG3WIDtPXXcLr2VdriQrT85leRBQyGG6IfCjcoEN4jA6pMeFNau90CVgrqnC23I6iiioUl1dvl1dVv64OQtXbRRVVOFtWE4pKbQ6U2hzN6iVyc/cWNRR+ar+21GoXaWIwIqLAwnBDFEC0GgltIgxoE9G8u227Q1FRhdwDVFxRVbOUV3m+rl6s1euy6kvlW9pbBACRpro9RpbwuqHo3IXBiIh8geGGKAR4hqL65wo1xO5wwVpZf/CpHYyKzj1WUaXcQ6ik0oGSyuYHI0lquMeodjCyhBnqCUY63oSRiOrFcEPUyhmqr+aKMzf/AbP1BaOGeorqC0ZCANZKB6yVDuSi5cEoyiSHnSiTHIrkde3XOkSesy/SyHBEFKoYboioxS40GDUUfGovReV1j1VUeQYjNDMYAXI4MhtrhR0lHHmGIiUs1d4O03FIjSiAMdwQkSoMOg3iI42Ij2x+MLI5nLBWOORQVCmHH2ulA9aKKpRUOursq/26pLIKlVUuCFEznNaSCdiAOxzpzglHdYOQ2aiH2aRDpEmHSKMOZpMOZqMOEQb2HhH5AsMNEQUdo06L+Ehti4IRIIejEiX41ASgEo8wVDckuYOTe66R++q0+p5Q31Rmoxx03IEnsjoEyfurQ1Gt4+7XkSZ9zT6jjr1IRLUw3BBRq2PUaWE0a1s0nAbID2mtLwiVNBSKbA6UVjqUMFRSWaXc5dq9D9YL+07hBq1H+DGbdAg36BBh0CLcWL026BB+zusIY83+CIMO4UZ5bdJr+NgQCloMN0REzaTXahATIT9KoyWEELA5XHKwqXQHHnf4qUJppXxZfknlucer6rzH5nABAMrtTpTbnSgssXnlO0oSEGHQIcygrROEIoxahOnday1MyqKp89qkr2kTVr3PWL2t10oMUOQTDDdERH4mSZISAFrae+Rmd7hQVh10SqqDkbs3qMzmRLndgXK7E2V2B8pt56ztTpTZHKiocnq0BQAhanqVTnrjS9dDI6HeMOQOQgadBgatRl67F60GxlrbtY/p3ce0nu3d2/Ix+XN1Wgl6jQZarQSdRoJeq+HQXghhuCEiCmLyH+7m3/ixIS6XkMNOdQiSe4QcKLM7UW6rXttrglOF3YlKhxOVVS5UVDlhq3Kiokp+XVm9bas+Vlm9uKqfO+sSQJndqdxIUm2SBOg0EnQaOfzoNBJ0Wg30Ggna6jCk00rQajTQu4+722o11a9rgpJWI0GSAK3k3pag1civpep9Gkl+pItWkqCRpFrb1fvdbZT29bSRJGg0chv3IklyeJQkCRKqj2kACe5jnmsJNZ8nQX6f+/3un49zPkf5fMn9s2s+X6eVkBwdpto/S4YbIiJSaDQSIow6RBh1QKT3P18IAbvTJT9UtlYQqh1+Kqv32RxO2B3yw2ftThfsjlqL04UqZ/WxWvvq267zfqcLTnfC8qgNqHIKVDmdQJX3v3trkhBpxOZnslX7+Qw3RETkN5IkyRO6dVogTK9aHS6XgMMl4HQJVLlccDgFHNVrp0ugyumCwyWU/VXV+x3u/bX2VTlrvU/5LAEh5H1OISCE/JgUp3u/EHC6UG8bl5AXeVuu1SlqbdfXRrg/W94WAhCoOSaE/LNqv66vHarX9b3P3U7Uen997xMQMOo1qv2zBRhuiIioFdJoJBiq59iEQatyNeRt6kYrIiIiIi9juCEiIqKQwnBDREREIYXhhoiIiEIKww0RERGFFIYbIiIiCikMN0RERBRSGG6IiIgopDDcEBERUUhhuCEiIqKQwnBDREREIYXhhoiIiEIKww0RERGFFIYbIiIiCik6tQvwNyEEAMBqtapcCRERETWV+++2++94Y1pduCkpKQEApKamqlwJERERNVdJSQmio6MbbSOJpkSgEOJyuXDixAlERkZCkiSvfrbVakVqaipyc3MRFRXl1c+mGjzP/sHz7D881/7B8+wfvjrPQgiUlJQgJSUFGk3js2paXc+NRqNBu3btfPozoqKi+B+OH/A8+wfPs//wXPsHz7N/+OI8n6/Hxo0TiomIiCikMNwQERFRSGG48SKj0YgpU6bAaDSqXUpI43n2D55n/+G59g+eZ/8IhPPc6iYUExERUWhjzw0RERGFFIYbIiIiCikMN0RERBRSGG6IiIgopDDceMmcOXOQnp4Ok8mEfv36YfPmzWqXFFSmT5+Ovn37IjIyEgkJCbjpppuwb98+jzaVlZUYO3YsYmNjYTabccstt6CgoMCjTU5ODoYNG4bw8HAkJCTgn//8JxwOhz+/SlCZMWMGJEnC+PHjlX08z95z/Phx3HXXXYiNjUVYWBh69OiBrVu3KseFEJg8eTKSk5MRFhaG7OxsHDhwwOMzzpw5g5EjRyIqKgoWiwX33XcfSktL/f1VApbT6cRzzz2HDh06ICwsDJ06dcILL7zg8fwhnufmW7duHYYPH46UlBRIkoQlS5Z4HPfWOf3tt9/wpz/9CSaTCampqfjXv/7lnS8g6IItXLhQGAwG8e6774pdu3aJBx54QFgsFlFQUKB2aUFj8ODB4r333hM7d+4UO3bsEEOHDhVpaWmitLRUafPggw+K1NRUsWrVKrF161ZxxRVXiP79+yvHHQ6HuPTSS0V2drbYvn27WL58uYiLixOTJk1S4ysFvM2bN4v09HRx2WWXiccee0zZz/PsHWfOnBHt27cXo0ePFps2bRKHDx8W3333nTh48KDSZsaMGSI6OlosWbJE/Prrr+KGG24QHTp0EBUVFUqb6667TmRkZIiff/5Z/PTTT6Jz587ijjvuUOMrBaQXX3xRxMbGim+++UYcOXJEfPbZZ8JsNotXX31VacPz3HzLly8XzzzzjFi8eLEAIL788kuP4944p8XFxSIxMVGMHDlS7Ny5U3zyySciLCxMzJ8//4LrZ7jxgssvv1yMHTtWee10OkVKSoqYPn26ilUFt8LCQgFArF27VgghRFFRkdDr9eKzzz5T2uzZs0cAEBs3bhRCyP8xajQakZ+fr7SZO3euiIqKEjabzb9fIMCVlJSILl26iJUrV4qrrrpKCTc8z97z1FNPiYEDBzZ43OVyiaSkJPHyyy8r+4qKioTRaBSffPKJEEKI3bt3CwBiy5YtSptvv/1WSJIkjh8/7rvig8iwYcPEvffe67Hvr3/9qxg5cqQQgufZG84NN946p2+++aZo06aNx++Np556SnTt2vWCa+aw1AWy2+3Ytm0bsrOzlX0ajQbZ2dnYuHGjipUFt+LiYgBATEwMAGDbtm2oqqryOM8XX3wx0tLSlPO8ceNG9OjRA4mJiUqbwYMHw2q1YteuXX6sPvCNHTsWw4YN8zifAM+zNy1duhR9+vTBrbfeioSEBGRmZuKtt95Sjh85cgT5+fke5zo6Ohr9+vXzONcWiwV9+vRR2mRnZ0Oj0WDTpk3++zIBrH///li1ahX2798PAPj111+xfv16DBkyBADPsy9465xu3LgRV155JQwGg9Jm8ODB2LdvH86ePXtBNba6B2d626lTp+B0Oj1+0QNAYmIi9u7dq1JVwc3lcmH8+PEYMGAALr30UgBAfn4+DAYDLBaLR9vExETk5+crber75+A+RrKFCxfil19+wZYtW+oc43n2nsOHD2Pu3LmYMGECnn76aWzZsgWPPvooDAYDRo0apZyr+s5l7XOdkJDgcVyn0yEmJobnutrEiRNhtVpx8cUXQ6vVwul04sUXX8TIkSMBgOfZB7x1TvPz89GhQ4c6n+E+1qZNmxbXyHBDAWfs2LHYuXMn1q9fr3YpISc3NxePPfYYVq5cCZPJpHY5Ic3lcqFPnz546aWXAACZmZnYuXMn5s2bh1GjRqlcXej49NNP8dFHH+Hjjz/GJZdcgh07dmD8+PFISUnheW7FOCx1geLi4qDVautcTVJQUICkpCSVqgpe48aNwzfffIMff/wR7dq1U/YnJSXBbrejqKjIo33t85yUlFTvPwf3MZKHnQoLC9GrVy/odDrodDqsXbsWr732GnQ6HRITE3mevSQ5ORndu3f32NetWzfk5OQAqDlXjf3uSEpKQmFhocdxh8OBM2fO8FxX++c//4mJEyfi9ttvR48ePfD3v/8djz/+OKZPnw6A59kXvHVOffm7hOHmAhkMBvTu3RurVq1S9rlcLqxatQpZWVkqVhZchBAYN24cvvzyS6xevbpOV2Xv3r2h1+s9zvO+ffuQk5OjnOesrCz8/vvvHv9BrVy5ElFRUXX+yLRW11xzDX7//Xfs2LFDWfr06YORI0cq2zzP3jFgwIA6tzPYv38/2rdvDwDo0KEDkpKSPM611WrFpk2bPM51UVERtm3bprRZvXo1XC4X+vXr54dvEfjKy8uh0Xj+KdNqtXC5XAB4nn3BW+c0KysL69atQ1VVldJm5cqV6Nq16wUNSQHgpeDesHDhQmE0GsWCBQvE7t27xZgxY4TFYvG4moQa99BDD4no6GixZs0akZeXpyzl5eVKmwcffFCkpaWJ1atXi61bt4qsrCyRlZWlHHdfovyXv/xF7NixQ6xYsULEx8fzEuXzqH21lBA8z96yefNmodPpxIsvvigOHDggPvroIxEeHi4+/PBDpc2MGTOExWIRX331lfjtt9/EjTfeWO/ltJmZmWLTpk1i/fr1okuXLq36EuVzjRo1SrRt21a5FHzx4sUiLi5OPPnkk0obnufmKykpEdu3bxfbt28XAMSsWbPE9u3bxbFjx4QQ3jmnRUVFIjExUfz9738XO3fuFAsXLhTh4eG8FDyQvP766yItLU0YDAZx+eWXi59//lntkoIKgHqX9957T2lTUVEhHn74YdGmTRsRHh4ubr75ZpGXl+fxOUePHhVDhgwRYWFhIi4uTvzjH/8QVVVVfv42weXccMPz7D1ff/21uPTSS4XRaBQXX3yx+M9//uNx3OVyieeee04kJiYKo9EorrnmGrFv3z6PNqdPnxZ33HGHMJvNIioqStxzzz2ipKTEn18joFmtVvHYY4+JtLQ0YTKZRMeOHcUzzzzjcXkxz3Pz/fjjj/X+Th41apQQwnvn9NdffxUDBw4URqNRtG3bVsyYMcMr9UtC1LqNIxEREVGQ45wbIiIiCikMN0RERBRSGG6IiIgopDDcEBERUUhhuCEiIqKQwnBDREREIYXhhoiIiEIKww0RtXpr1qyBJEl1nqlFRMGJ4YaIiIhCCsMNERERhRSGGyJSncvlwvTp09GhQweEhYUhIyMDn3/+OYCaIaNly5bhsssug8lkwhVXXIGdO3d6fMYXX3yBSy65BEajEenp6Zg5c6bHcZvNhqeeegqpqakwGo3o3Lkz3nnnHY8227ZtQ58+fRAeHo7+/fvXeao3EQUHhhsiUt306dPxwQcfYN68edi1axcef/xx3HXXXVi7dq3S5p///CdmzpyJLVu2ID4+HsOHD0dVVRUAOZTcdtttuP322/H777/j+eefx3PPPYcFCxYo77/77rvxySef4LXXXsOePXswf/58mM1mjzqeeeYZzJw5E1u3boVOp8O9997rl+9PRN7FB2cSkapsNhtiYmLwww8/ICsrS9l///33o7y8HGPGjMHVV1+NhQsXYsSIEQCAM2fOoF27dliwYAFuu+02jBw5EidPnsT333+vvP/JJ5/EsmXLsGvXLuzfvx9du3bFypUrkZ2dXaeGNWvW4Oqrr8YPP/yAa665BgCwfPlyDBs2DBUVFTCZTD4+C0TkTey5ISJVHTx4EOXl5bj22mthNpuV5YMPPsChQ4eUdrWDT0xMDLp27Yo9e/YAAPbs2YMBAwZ4fO6AAQNw4MABOJ1O7NixA1qtFldddVWjtVx22WXKdnJyMgCgsLDwgr8jEfmXTu0CiKh1Ky0tBQAsW7YMbdu29ThmNBo9Ak5LhYWFNamdXq9XtiVJAiDPByKi4MKeGyJSVffu3WE0GpGTk4POnTt7LKmpqUq7n3/+Wdk+e/Ys9u/fj27dugEAunXrhg0bNnh87oYNG3DRRRdBq9WiR48ecLlcHnN4iCh0seeGiFQVGRmJJ554Ao8//jhcLhcGDhyI4uJibNiwAVFRUWjfvj0AYNq0aYiNjUViYiKeeeYZxMXF4aabbgIA/OMf/0Dfvn3xwgsvYMSIEdi4cSPeeOMNvPnmmwCA9PR0jBo1Cvfeey9ee+01ZGRk4NixYygsLMRtt92m1lcnIh9huCEi1b3wwguIj4/H9OnTcfjwYVgsFvTq1QtPP/20Miw0Y8YMPPbYYzhw4AB69uyJr7/+GgaDAQDQq1cvfPrpp5g8eTJeeOEFJCcnY9q0aRg9erTyM+bOnYunn34aDz/8ME6fPo20tDQ8/fTTanxdIvIxXi1FRAHNfSXT2bNnYbFY1C6HiIIA59wQERFRSGG4ISIiopDCYSkiIiIKKey5ISIiopDCcENEREQhheGGiIiIQgrDDREREYUUhhsiIiIKKQw3REREFFIYboiIiCikMNwQERFRSGG4ISIiopDy/wPjYMOEP1UOnAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prdida en Entrenamiento\n",
      "161/161 [==============================] - 0s 439us/step - loss: 0.5497\n",
      "Prdida en Validacin\n",
      "53/53 [==============================] - 0s 405us/step - loss: 0.7283\n",
      "Prdida en Prueba\n",
      "52/52 [==============================] - 0s 413us/step - loss: 0.4975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.49752333760261536"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Prdida en Entrenamiento\")\n",
    "modelo1.evaluate(\n",
    "    x = x_train,\n",
    "    y = y_train,\n",
    "    batch_size=1\n",
    ")\n",
    "print(\"Prdida en Validacin\")\n",
    "modelo1.evaluate(\n",
    "    x = x_val,\n",
    "    y = y_val,\n",
    "    batch_size=1\n",
    ")\n",
    "print(\"Prdida en Prueba\")\n",
    "modelo1.evaluate(\n",
    "    x = x_test,\n",
    "    y = y_test,\n",
    "    batch_size=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
